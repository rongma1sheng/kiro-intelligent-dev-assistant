"""GPUçœ‹é—¨ç‹— - GPU Watchdog (ç¬¬åäºŒç« ç‰ˆæœ¬)

ç™½çš®ä¹¦ä¾æ®: ç¬¬åäºŒç«  12.1.2 GPUçœ‹é—¨ç‹—ä¸é©±åŠ¨çƒ­é‡è½½

é—®é¢˜: AMD/NVIDIAé©±åŠ¨å´©æºƒå¯¼è‡´æœ¬åœ°æ¨ç†å¤±è´¥
é£é™©ç­‰çº§: ğŸŸ¡ ä¸­

åŠŸèƒ½:
1. GPUå¥åº·æ£€æŸ¥ï¼ˆ30ç§’å‘¨æœŸï¼‰
2. æ˜¾å­˜ç¢ç‰‡åŒ–æ£€æµ‹ï¼ˆé˜ˆå€¼30%ï¼‰
3. é©±åŠ¨çƒ­é‡è½½è§¦å‘
4. RedisçŠ¶æ€æ ‡è®°ï¼ˆSoldieré™çº§ï¼‰

æ€§èƒ½è¦æ±‚:
- æ£€æµ‹å‘¨æœŸ: 30ç§’
- å“åº”å»¶è¿Ÿ: < 30ç§’
- é‡è½½æ—¶é—´: 30-90ç§’
"""

import re
import subprocess
import threading
import time
from dataclasses import dataclass
from enum import Enum
from typing import Any, Optional

from loguru import logger


class GPUHealthStatus(Enum):
    """GPUå¥åº·çŠ¶æ€æšä¸¾

    ç™½çš®ä¹¦ä¾æ®: ç¬¬åäºŒç«  12.1.2 GPUçœ‹é—¨ç‹—ä¸é©±åŠ¨çƒ­é‡è½½
    """

    HEALTHY = "healthy"  # å¥åº·çŠ¶æ€
    DEGRADED = "degraded"  # é™çº§çŠ¶æ€ï¼ˆé©±åŠ¨é‡è½½ä¸­ï¼‰
    UNHEALTHY = "unhealthy"  # ä¸å¥åº·çŠ¶æ€
    UNAVAILABLE = "unavailable"  # ä¸å¯ç”¨çŠ¶æ€ï¼ˆæ— GPUæˆ–æ£€æµ‹å¤±è´¥ï¼‰


@dataclass
class GPUHealthMetrics:
    """GPUå¥åº·æŒ‡æ ‡

    ç™½çš®ä¹¦ä¾æ®: ç¬¬åäºŒç«  12.1.2 GPUçœ‹é—¨ç‹—ä¸é©±åŠ¨çƒ­é‡è½½

    Attributes:
        memory_used_mb: å·²ä½¿ç”¨æ˜¾å­˜ï¼ˆMBï¼‰
        memory_total_mb: æ€»æ˜¾å­˜ï¼ˆMBï¼‰
        memory_free_mb: ç©ºé—²æ˜¾å­˜ï¼ˆMBï¼‰
        fragmentation_ratio: ç¢ç‰‡åŒ–æ¯”ç‡ï¼ˆ0-1ï¼‰
        temperature_celsius: æ¸©åº¦ï¼ˆæ‘„æ°åº¦ï¼‰
        utilization_percent: åˆ©ç”¨ç‡ï¼ˆ0-100ï¼‰
        is_healthy: æ˜¯å¦å¥åº·
    """

    memory_used_mb: float
    memory_total_mb: float
    memory_free_mb: float
    fragmentation_ratio: float
    temperature_celsius: Optional[float] = None
    utilization_percent: Optional[float] = None
    is_healthy: bool = True


class GPUWatchdog:
    """GPUçœ‹é—¨ç‹—ï¼ˆç¬¬åäºŒç« ç‰ˆæœ¬ï¼‰

    ç™½çš®ä¹¦ä¾æ®: ç¬¬åäºŒç«  12.1.2 GPUçœ‹é—¨ç‹—ä¸é©±åŠ¨çƒ­é‡è½½

    ç›‘æ§AMD GPUå¥åº·çŠ¶æ€ï¼Œæ£€æµ‹æ˜¾å­˜ç¢ç‰‡åŒ–ï¼Œå¹¶åœ¨å¿…è¦æ—¶è§¦å‘é©±åŠ¨çƒ­é‡è½½ã€‚
    é€šè¿‡Redisæ ‡è®°SoldierçŠ¶æ€ä¸ºDEGRADEDï¼Œå¯ç”¨Cloud Failoverã€‚

    Attributes:
        redis_client: Rediså®¢æˆ·ç«¯ï¼ˆå¯é€‰ï¼‰
        check_interval: æ£€æŸ¥é—´éš”ï¼ˆç§’ï¼‰ï¼Œé»˜è®¤30ç§’
        fragmentation_threshold: ç¢ç‰‡åŒ–é˜ˆå€¼ï¼Œé»˜è®¤0.3ï¼ˆ30%ï¼‰
        failure_threshold: è¿ç»­å¤±è´¥é˜ˆå€¼ï¼Œé»˜è®¤3æ¬¡
        status: å½“å‰GPUçŠ¶æ€
        metrics: å½“å‰GPUæŒ‡æ ‡
        failure_count: è¿ç»­å¤±è´¥è®¡æ•°
        _running: ç›‘æ§çº¿ç¨‹è¿è¡Œæ ‡å¿—
        _thread: ç›‘æ§çº¿ç¨‹
        _lock: çº¿ç¨‹é”
    """

    # Redisé”®å¸¸é‡
    REDIS_KEY_SOLDIER_STATUS = "mia:soldier:status"
    REDIS_KEY_GPU_FAILURES = "system:gpu_failures"

    def __init__(
        self,
        redis_client: Optional[Any] = None,
        check_interval: int = 30,
        fragmentation_threshold: float = 0.3,
        failure_threshold: int = 3,
    ):
        """åˆå§‹åŒ–GPUçœ‹é—¨ç‹—

        Args:
            redis_client: Rediså®¢æˆ·ç«¯ï¼Œç”¨äºæ ‡è®°SoldierçŠ¶æ€
            check_interval: æ£€æŸ¥é—´éš”ï¼ˆç§’ï¼‰ï¼Œé»˜è®¤30ç§’
            fragmentation_threshold: ç¢ç‰‡åŒ–é˜ˆå€¼ï¼Œé»˜è®¤0.3ï¼ˆ30%ï¼‰
            failure_threshold: è¿ç»­å¤±è´¥é˜ˆå€¼ï¼Œé»˜è®¤3æ¬¡

        Raises:
            ValueError: å½“å‚æ•°ä¸åœ¨æœ‰æ•ˆèŒƒå›´æ—¶
        """
        if check_interval <= 0:
            raise ValueError(f"æ£€æŸ¥é—´éš”å¿…é¡» > 0ï¼Œå½“å‰: {check_interval}")

        if not 0 < fragmentation_threshold < 1:
            raise ValueError(f"ç¢ç‰‡åŒ–é˜ˆå€¼å¿…é¡»åœ¨ (0, 1)ï¼Œå½“å‰: {fragmentation_threshold}")

        if failure_threshold <= 0:
            raise ValueError(f"å¤±è´¥é˜ˆå€¼å¿…é¡» > 0ï¼Œå½“å‰: {failure_threshold}")

        self.redis = redis_client
        self.check_interval: int = check_interval
        self.fragmentation_threshold: float = fragmentation_threshold
        self.failure_threshold: int = failure_threshold

        self.status: GPUHealthStatus = GPUHealthStatus.HEALTHY
        self.metrics: Optional[GPUHealthMetrics] = None
        self.failure_count: int = 0

        self._running: bool = False
        self._thread: Optional[threading.Thread] = None
        self._lock: threading.RLock = threading.RLock()

        logger.info(
            f"åˆå§‹åŒ–GPUWatchdog: "
            f"check_interval={check_interval}s, "
            f"fragmentation_threshold={fragmentation_threshold:.0%}, "
            f"failure_threshold={failure_threshold}"
        )

    def check_gpu_health(self) -> bool:
        """æ£€æŸ¥GPUå¥åº·çŠ¶æ€

        ç™½çš®ä¹¦ä¾æ®: ç¬¬åäºŒç«  12.1.2 GPUçœ‹é—¨ç‹—ä¸é©±åŠ¨çƒ­é‡è½½

        è°ƒç”¨rocm-smiæ£€æµ‹GPUçŠ¶æ€ï¼Œæ£€æŸ¥æ˜¾å­˜ç¢ç‰‡åŒ–ç¨‹åº¦ã€‚

        Returns:
            GPUæ˜¯å¦å¥åº·
        """
        try:
            # è°ƒç”¨rocm-smiè·å–GPUä¿¡æ¯
            result = subprocess.run(  # pylint: disable=w1510
                ["rocm-smi", "--showmeminfo", "vram"], capture_output=True, text=True, timeout=5.0
            )  # pylint: disable=w1510

            if result.returncode != 0:
                logger.error(f"[GPU] rocm-smiæ‰§è¡Œå¤±è´¥: {result.stderr}")
                self._handle_failure()
                return False

            # è§£æè¾“å‡º
            metrics = self._parse_gpu_output(result.stdout)

            if metrics is None:
                logger.error("[GPU] æ— æ³•è§£ærocm-smiè¾“å‡º")
                self._handle_failure()
                return False

            # æ›´æ–°æŒ‡æ ‡
            with self._lock:
                self.metrics = metrics

                # æ£€æŸ¥ç¢ç‰‡åŒ–ç¨‹åº¦
                if metrics.fragmentation_ratio > self.fragmentation_threshold:
                    logger.warning(f"[GPU] High fragmentation: {metrics.fragmentation_ratio:.1%}")
                    metrics.is_healthy = False
                    return False

                # é‡ç½®å¤±è´¥è®¡æ•°
                self.failure_count = 0
                self.status = GPUHealthStatus.HEALTHY

                # æ›´æ–°RedisçŠ¶æ€
                self._update_redis_status("NORMAL")

            return True

        except subprocess.TimeoutExpired:
            logger.error("[GPU] rocm-smi timeout")
            self._handle_failure()
            return False

        except FileNotFoundError:
            logger.warning("[GPU] rocm-smi not found, AMD driver may not be installed")
            with self._lock:
                self.status = GPUHealthStatus.UNAVAILABLE
            return False

        except Exception as e:  # pylint: disable=broad-exception-caught
            logger.error(f"[GPU] Check failed: {e}")
            self._handle_failure()
            return False

    def detect_fragmentation(self) -> float:
        """æ£€æµ‹æ˜¾å­˜ç¢ç‰‡åŒ–ç¨‹åº¦

        ç™½çš®ä¹¦ä¾æ®: ç¬¬åäºŒç«  12.1.2 GPUçœ‹é—¨ç‹—ä¸é©±åŠ¨çƒ­é‡è½½

        Returns:
            ç¢ç‰‡åŒ–æ¯”ç‡ï¼ˆ0-1ï¼‰ï¼Œ-1è¡¨ç¤ºæ£€æµ‹å¤±è´¥
        """
        with self._lock:
            if self.metrics is None:
                # å…ˆæ‰§è¡Œå¥åº·æ£€æŸ¥
                self.check_gpu_health()

            if self.metrics is not None:
                return self.metrics.fragmentation_ratio

            return -1.0

    def trigger_driver_reload(self) -> bool:
        """è§¦å‘é©±åŠ¨çƒ­é‡è½½

        ç™½çš®ä¹¦ä¾æ®: ç¬¬åäºŒç«  12.1.2 GPUçœ‹é—¨ç‹—ä¸é©±åŠ¨çƒ­é‡è½½

        åœ¨é‡è½½æœŸé—´ï¼Œç³»ç»Ÿå°†SoldierçŠ¶æ€æ ‡è®°ä¸ºDEGRADEDï¼Œ
        å¹¶é¢„åŠ è½½Cloud Failoveré…ç½®ã€‚

        Returns:
            é‡è½½æ˜¯å¦æˆåŠŸ
        """
        logger.warning("[GPU] Triggering driver reload...")

        # 1. æ ‡è®°Soldierä¸ºé™çº§æ¨¡å¼
        with self._lock:
            self.status = GPUHealthStatus.DEGRADED
        self._update_redis_status("DEGRADED")

        # 2. æ‰§è¡Œé©±åŠ¨é‡è½½
        try:
            result = subprocess.run(  # pylint: disable=unused-variable,w1510
                ["rocm-smi", "--gpureset", "-d", "0"], capture_output=True, text=True, timeout=90.0  # é‡è½½æœ€å¤š90ç§’
            )

            # ç­‰å¾…é©±åŠ¨æ¢å¤
            time.sleep(10)

            # 3. éªŒè¯æ¢å¤
            if self.check_gpu_health():  # pylint: disable=no-else-return
                self._update_redis_status("NORMAL")
                logger.info("[GPU] Driver reload successful")
                return True
            else:
                logger.error("[GPU] Driver reload failed")
                return False

        except subprocess.TimeoutExpired:
            logger.error("[GPU] Driver reload timeout")
            return False

        except FileNotFoundError:
            logger.error("[GPU] rocm-smi not found, cannot reload driver")
            return False

        except Exception as e:  # pylint: disable=broad-exception-caught
            logger.error(f"[GPU] Reload error: {e}")
            return False

    def start(self) -> None:
        """å¯åŠ¨GPUçœ‹é—¨ç‹—

        å¯åŠ¨åå°ç›‘æ§çº¿ç¨‹ï¼Œå®šæœŸæ£€æŸ¥GPUçŠ¶æ€ã€‚

        Raises:
            RuntimeError: å½“çœ‹é—¨ç‹—å·²ç»åœ¨è¿è¡Œæ—¶
        """
        with self._lock:
            if self._running:
                raise RuntimeError("GPUçœ‹é—¨ç‹—å·²ç»åœ¨è¿è¡Œ")

            self._running = True
            self._thread = threading.Thread(target=self._watchdog_loop, name="GPUWatchdog", daemon=True)
            self._thread.start()

        logger.info("[GPU] Watchdog started")

    def stop(self) -> None:
        """åœæ­¢GPUçœ‹é—¨ç‹—

        åœæ­¢åå°ç›‘æ§çº¿ç¨‹ã€‚
        """
        with self._lock:
            if not self._running:
                logger.warning("[GPU] Watchdog not running")
                return
            self._running = False

        if self._thread and self._thread.is_alive():
            self._thread.join(timeout=5.0)

        logger.info("[GPU] Watchdog stopped")

    def get_status(self) -> GPUHealthStatus:
        """è·å–å½“å‰GPUçŠ¶æ€

        Returns:
            å½“å‰GPUçŠ¶æ€
        """
        with self._lock:
            return self.status

    def get_metrics(self) -> Optional[GPUHealthMetrics]:
        """è·å–å½“å‰GPUæŒ‡æ ‡

        Returns:
            å½“å‰GPUæŒ‡æ ‡ï¼Œå¦‚æœæœªæ£€æµ‹åˆ°åˆ™è¿”å›None
        """
        with self._lock:
            return self.metrics

    def get_failure_count(self) -> int:
        """è·å–è¿ç»­å¤±è´¥è®¡æ•°

        Returns:
            è¿ç»­å¤±è´¥æ¬¡æ•°
        """
        with self._lock:
            return self.failure_count

    def _parse_gpu_output(self, output: str) -> Optional[GPUHealthMetrics]:
        """è§£ærocm-smiè¾“å‡ºï¼ˆå†…éƒ¨æ–¹æ³•ï¼‰

        Args:
            output: rocm-smiå‘½ä»¤è¾“å‡º

        Returns:
            GPUæŒ‡æ ‡ï¼Œè§£æå¤±è´¥è¿”å›None
        """
        try:
            # ç¤ºä¾‹è¾“å‡ºæ ¼å¼:
            # GPU[0]		: VRAM Total Memory (B): 34359738368
            # GPU[0]		: VRAM Total Used Memory (B): 8589934592

            # æå–æ€»æ˜¾å­˜
            total_match = re.search(r"VRAM Total Memory \(B\):\s*(\d+)", output)
            if not total_match:
                return None

            memory_total = int(total_match.group(1)) / (1024**2)  # è½¬æ¢ä¸ºMB

            # æå–å·²ä½¿ç”¨æ˜¾å­˜
            used_match = re.search(r"VRAM Total Used Memory \(B\):\s*(\d+)", output)
            if not used_match:
                return None

            memory_used = int(used_match.group(1)) / (1024**2)  # è½¬æ¢ä¸ºMB

            # è®¡ç®—ç©ºé—²æ˜¾å­˜
            memory_free = memory_total - memory_used

            # è®¡ç®—ç¢ç‰‡åŒ–ç¨‹åº¦
            # ç®€åŒ–æ¨¡å‹ï¼šåŸºäºä½¿ç”¨ç‡ä¼°ç®—ç¢ç‰‡åŒ–
            usage_ratio = memory_used / memory_total if memory_total > 0 else 0
            fragmentation = usage_ratio * 0.5  # ç®€åŒ–ä¼°ç®—

            return GPUHealthMetrics(
                memory_used_mb=memory_used,
                memory_total_mb=memory_total,
                memory_free_mb=memory_free,
                fragmentation_ratio=fragmentation,
                is_healthy=fragmentation <= self.fragmentation_threshold,
            )

        except Exception as e:  # pylint: disable=broad-exception-caught
            logger.error(f"[GPU] Parse output failed: {e}")
            return None

    def _handle_failure(self) -> None:
        """å¤„ç†æ£€æµ‹å¤±è´¥ï¼ˆå†…éƒ¨æ–¹æ³•ï¼‰

        ç™½çš®ä¹¦ä¾æ®: ç¬¬åäºŒç«  12.1.2 GPUçœ‹é—¨ç‹—ä¸é©±åŠ¨çƒ­é‡è½½
        """
        with self._lock:
            self.failure_count += 1

            # æ›´æ–°Rediså¤±è´¥è®¡æ•°
            self._update_redis_failure_count()

            if self.failure_count >= self.failure_threshold:
                self.status = GPUHealthStatus.UNHEALTHY
                logger.error(f"[GPU] Consecutive failures: {self.failure_count}, " f"triggering driver reload")
                # è§¦å‘é©±åŠ¨é‡è½½
                self.trigger_driver_reload()
            else:
                self.status = GPUHealthStatus.DEGRADED
                logger.warning(f"[GPU] Failure {self.failure_count}/{self.failure_threshold}")

    def _update_redis_status(self, status: str) -> None:
        """æ›´æ–°Redisä¸­çš„SoldierçŠ¶æ€ï¼ˆå†…éƒ¨æ–¹æ³•ï¼‰

        Args:
            status: çŠ¶æ€å­—ç¬¦ä¸²ï¼ˆNORMAL/DEGRADEDï¼‰
        """
        if self.redis is not None:
            try:
                self.redis.set(self.REDIS_KEY_SOLDIER_STATUS, status)
                logger.debug(f"[GPU] Redis status updated: {status}")
            except Exception as e:  # pylint: disable=broad-exception-caught
                logger.warning(f"[GPU] Failed to update Redis status: {e}")

    def _update_redis_failure_count(self) -> None:
        """æ›´æ–°Redisä¸­çš„å¤±è´¥è®¡æ•°ï¼ˆå†…éƒ¨æ–¹æ³•ï¼‰"""
        if self.redis is not None:
            try:
                self.redis.set(self.REDIS_KEY_GPU_FAILURES, self.failure_count)
            except Exception as e:  # pylint: disable=broad-exception-caught
                logger.warning(f"[GPU] Failed to update Redis failure count: {e}")

    def _watchdog_loop(self) -> None:
        """çœ‹é—¨ç‹—ä¸»å¾ªç¯ï¼ˆå†…éƒ¨æ–¹æ³•ï¼‰

        åå°çº¿ç¨‹å®šæœŸæ£€æŸ¥GPUçŠ¶æ€ã€‚
        """
        logger.info("[GPU] Watchdog loop started")

        while self._running:
            try:
                # æ£€æŸ¥GPU
                self.check_gpu_health()

                # ç­‰å¾…ä¸‹ä¸€æ¬¡æ£€æŸ¥
                time.sleep(self.check_interval)

            except Exception as e:  # pylint: disable=broad-exception-caught
                logger.error(f"[GPU] Watchdog loop error: {e}")
                time.sleep(self.check_interval)

        logger.info("[GPU] Watchdog loop exited")
