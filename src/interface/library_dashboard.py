"""è—ç»é˜ä»ªè¡¨ç›˜ (Library Dashboard)

ç™½çš®ä¹¦ä¾æ®: é™„å½•A å…¨æ¯æŒ‡æŒ¥å° - 9. è—ç»é˜ (Library)
ä¼˜å…ˆçº§: P2 - é«˜çº§åŠŸèƒ½

æ ¸å¿ƒåŠŸèƒ½:
- ç ”æŠ¥ç®¡ç†ä¸é˜…è¯»
- è®ºæ–‡åº“æµè§ˆ
- çŸ¥è¯†å›¾è°±å±•ç¤º
- Scholarå¼•æ“é›†æˆ
"""

from dataclasses import dataclass
from datetime import datetime
from enum import Enum
from typing import Any, Dict, List, Optional

from loguru import logger

try:
    import streamlit as st

    HAS_STREAMLIT = True
except ImportError:
    HAS_STREAMLIT = False


class DocumentType(Enum):
    """æ–‡æ¡£ç±»å‹æšä¸¾"""

    RESEARCH_REPORT = "ç ”æŠ¥"
    ACADEMIC_PAPER = "è®ºæ–‡"
    NEWS_ARTICLE = "æ–°é—»"
    STRATEGY_DOC = "ç­–ç•¥æ–‡æ¡£"
    FACTOR_DOC = "å› å­æ–‡æ¡£"


class DocumentSource(Enum):
    """æ–‡æ¡£æ¥æºæšä¸¾"""

    BROKER = "åˆ¸å•†ç ”æŠ¥"
    ARXIV = "arXiv"
    SSRN = "SSRN"
    NEWS = "è´¢ç»æ–°é—»"
    INTERNAL = "å†…éƒ¨æ–‡æ¡£"


@dataclass
class Document:
    """æ–‡æ¡£æ•°æ®æ¨¡å‹

    Attributes:
        doc_id: æ–‡æ¡£ID
        title: æ ‡é¢˜
        doc_type: æ–‡æ¡£ç±»å‹
        source: æ¥æº
        author: ä½œè€…
        publish_date: å‘å¸ƒæ—¥æœŸ
        summary: æ‘˜è¦
        keywords: å…³é”®è¯
        relevance_score: ç›¸å…³æ€§è¯„åˆ†
        read_count: é˜…è¯»æ¬¡æ•°
        extracted_factors: æå–çš„å› å­æ•°
    """

    doc_id: str
    title: str
    doc_type: DocumentType
    source: DocumentSource
    author: str
    publish_date: datetime
    summary: str
    keywords: List[str]
    relevance_score: float
    read_count: int = 0
    extracted_factors: int = 0

    def to_dict(self) -> Dict[str, Any]:
        return {
            "doc_id": self.doc_id,
            "title": self.title,
            "doc_type": self.doc_type.value,
            "source": self.source.value,
            "author": self.author,
            "publish_date": self.publish_date.isoformat(),
            "summary": self.summary,
            "keywords": self.keywords,
            "relevance_score": self.relevance_score,
            "read_count": self.read_count,
            "extracted_factors": self.extracted_factors,
        }


@dataclass
class KnowledgeNode:
    """çŸ¥è¯†å›¾è°±èŠ‚ç‚¹

    Attributes:
        node_id: èŠ‚ç‚¹ID
        name: åç§°
        node_type: èŠ‚ç‚¹ç±»å‹
        connections: è¿æ¥æ•°
        importance: é‡è¦æ€§è¯„åˆ†
    """

    node_id: str
    name: str
    node_type: str
    connections: int
    importance: float


class LibraryDashboard:
    """è—ç»é˜ä»ªè¡¨ç›˜

    ç™½çš®ä¹¦ä¾æ®: é™„å½•A å…¨æ¯æŒ‡æŒ¥å° - 9. è—ç»é˜ (Library)

    æä¾›ç ”æŠ¥å’Œè®ºæ–‡çš„ç®¡ç†ä¸é˜…è¯»åŠŸèƒ½:
    - ç ”æŠ¥åº“æµè§ˆ
    - è®ºæ–‡åº“æµè§ˆ
    - çŸ¥è¯†å›¾è°±
    - Scholarå¼•æ“é›†æˆ
    """

    COLOR_SCHEME = {
        "rise": "#FF4D4F",
        "fall": "#52C41A",
        "neutral": "#8C8C8C",
        "primary": "#1890FF",
        "warning": "#FA8C16",
    }

    def __init__(self, redis_client: Optional[Any] = None):
        """åˆå§‹åŒ–è—ç»é˜ä»ªè¡¨ç›˜

        Args:
            redis_client: Rediså®¢æˆ·ç«¯
        """
        self.redis_client = redis_client
        logger.info("LibraryDashboard initialized")

    def get_documents(
        self,
        doc_type: Optional[DocumentType] = None,
        source: Optional[DocumentSource] = None,
        keyword: Optional[str] = None,
        limit: int = 20,
    ) -> List[Document]:
        """è·å–æ–‡æ¡£åˆ—è¡¨

        Args:
            doc_type: æ–‡æ¡£ç±»å‹ç­›é€‰
            source: æ¥æºç­›é€‰
            keyword: å…³é”®è¯æœç´¢
            limit: è¿”å›æ•°é‡é™åˆ¶

        Returns:
            æ–‡æ¡£åˆ—è¡¨
        """
        if self.redis_client is None:
            return self._get_mock_documents(limit)

        try:
            doc_ids = self.redis_client.lrange("mia:library:documents", 0, limit * 2)
            documents = []

            for doc_id in doc_ids:
                data = self.redis_client.hgetall(f"mia:library:doc:{doc_id}")
                if not data:
                    continue

                doc = Document(
                    doc_id=doc_id,
                    title=data.get("title", ""),
                    doc_type=DocumentType[data.get("doc_type", "RESEARCH_REPORT")],
                    source=DocumentSource[data.get("source", "BROKER")],
                    author=data.get("author", ""),
                    publish_date=datetime.fromisoformat(data.get("publish_date", datetime.now().isoformat())),
                    summary=data.get("summary", ""),
                    keywords=data.get("keywords", "").split(","),
                    relevance_score=float(data.get("relevance_score", 0)),
                    read_count=int(data.get("read_count", 0)),
                    extracted_factors=int(data.get("extracted_factors", 0)),
                )

                # åº”ç”¨ç­›é€‰
                if doc_type and doc.doc_type != doc_type:
                    continue
                if source and doc.source != source:
                    continue
                if keyword and keyword.lower() not in doc.title.lower():
                    continue

                documents.append(doc)

                if len(documents) >= limit:
                    break

            return documents if documents else self._get_mock_documents(limit)

        except Exception as e:  # pylint: disable=broad-exception-caught
            logger.error(f"Failed to get documents: {e}")
            return self._get_mock_documents(limit)

    def get_knowledge_graph(self) -> List[KnowledgeNode]:
        """è·å–çŸ¥è¯†å›¾è°±èŠ‚ç‚¹

        Returns:
            çŸ¥è¯†å›¾è°±èŠ‚ç‚¹åˆ—è¡¨
        """
        if self.redis_client is None:
            return self._get_mock_knowledge_graph()

        try:
            nodes = []
            node_ids = self.redis_client.smembers("mia:library:knowledge:nodes")

            for node_id in node_ids:
                data = self.redis_client.hgetall(f"mia:library:knowledge:{node_id}")
                if data:
                    nodes.append(
                        KnowledgeNode(
                            node_id=node_id,
                            name=data.get("name", ""),
                            node_type=data.get("type", ""),
                            connections=int(data.get("connections", 0)),
                            importance=float(data.get("importance", 0)),
                        )
                    )

            return nodes if nodes else self._get_mock_knowledge_graph()

        except Exception as e:  # pylint: disable=broad-exception-caught
            logger.error(f"Failed to get knowledge graph: {e}")
            return self._get_mock_knowledge_graph()

    def trigger_scholar_scan(self, sources: List[str]) -> Dict[str, Any]:
        """è§¦å‘Scholarå¼•æ“æ‰«æ

        Args:
            sources: è¦æ‰«æçš„æ¥æºåˆ—è¡¨

        Returns:
            æ‰«æä»»åŠ¡ä¿¡æ¯
        """
        task_id = f"scan_{datetime.now().strftime('%Y%m%d%H%M%S')}"  # pylint: disable=implicit-str-concat

        if self.redis_client:
            self.redis_client.hset(
                f"mia:library:scan:{task_id}",
                mapping={"status": "running", "sources": ",".join(sources), "start_time": datetime.now().isoformat()},
            )

        logger.info(f"Scholar scan triggered: {task_id}, sources: {sources}")

        return {"task_id": task_id, "status": "running", "sources": sources}

    def render_streamlit(self) -> None:
        """æ¸²æŸ“Streamlitç•Œé¢"""
        if not HAS_STREAMLIT:
            logger.warning("Streamlit not available")
            return

        st.title("ğŸ“š è—ç»é˜ (Library)")
        st.caption("ç ”æŠ¥ç®¡ç† Â· è®ºæ–‡åº“ Â· çŸ¥è¯†å›¾è°± Â· Scholarå¼•æ“")

        tab1, tab2, tab3, tab4 = st.tabs(["ğŸ“„ ç ”æŠ¥åº“", "ğŸ“‘ è®ºæ–‡åº“", "ğŸ•¸ï¸ çŸ¥è¯†å›¾è°±", "ğŸ” Scholaræ‰«æ"])

        with tab1:
            self._render_research_reports()

        with tab2:
            self._render_academic_papers()

        with tab3:
            self._render_knowledge_graph()

        with tab4:
            self._render_scholar_scan()

    def _render_research_reports(self) -> None:
        """æ¸²æŸ“ç ”æŠ¥åº“"""
        st.subheader("ğŸ“„ åˆ¸å•†ç ”æŠ¥åº“")

        # æœç´¢å’Œç­›é€‰
        col1, col2 = st.columns([3, 1])
        with col1:
            keyword = st.text_input("æœç´¢ç ”æŠ¥", placeholder="è¾“å…¥å…³é”®è¯...")
        with col2:
            source_filter = st.selectbox("æ¥æº", ["å…¨éƒ¨", "åˆ¸å•†ç ”æŠ¥", "è´¢ç»æ–°é—»"])

        source = None
        if source_filter == "åˆ¸å•†ç ”æŠ¥":
            source = DocumentSource.BROKER
        elif source_filter == "è´¢ç»æ–°é—»":
            source = DocumentSource.NEWS

        documents = self.get_documents(
            doc_type=DocumentType.RESEARCH_REPORT, source=source, keyword=keyword if keyword else None
        )

        st.caption(f"å…± {len(documents)} ç¯‡ç ”æŠ¥")
        st.divider()

        for doc in documents:
            with st.expander(f"ğŸ“„ {doc.title}"):
                col1, col2, col3 = st.columns([2, 1, 1])

                with col1:
                    st.markdown(f"**ä½œè€…**: {doc.author}")
                    st.markdown(f"**æ¥æº**: {doc.source.value}")
                    st.caption(f"å‘å¸ƒæ—¥æœŸ: {doc.publish_date.strftime('%Y-%m-%d')}")

                with col2:
                    st.metric("ç›¸å…³æ€§", f"{doc.relevance_score:.0%}")

                with col3:
                    st.metric("æå–å› å­", doc.extracted_factors)

                st.markdown("**æ‘˜è¦**")
                st.info(doc.summary)

                st.markdown(f"**å…³é”®è¯**: {', '.join(doc.keywords)}")

                if st.button("ğŸ“– é˜…è¯»å…¨æ–‡", key=f"read_{doc.doc_id}"):
                    st.success("æ­£åœ¨åŠ è½½å…¨æ–‡...")

    def _render_academic_papers(self) -> None:
        """æ¸²æŸ“è®ºæ–‡åº“"""
        st.subheader("ğŸ“‘ å­¦æœ¯è®ºæ–‡åº“")

        # æ¥æºç­›é€‰
        source_filter = st.selectbox("è®ºæ–‡æ¥æº", ["å…¨éƒ¨", "arXiv", "SSRN"])

        source = None
        if source_filter == "arXiv":
            source = DocumentSource.ARXIV
        elif source_filter == "SSRN":
            source = DocumentSource.SSRN

        documents = self.get_documents(doc_type=DocumentType.ACADEMIC_PAPER, source=source)

        st.caption(f"å…± {len(documents)} ç¯‡è®ºæ–‡")
        st.divider()

        for doc in documents:
            with st.container():
                st.markdown(f"### {doc.title}")
                st.caption(f"{doc.author} | {doc.source.value} | {doc.publish_date.strftime('%Y-%m-%d')}")
                st.markdown(doc.summary[:200] + "..." if len(doc.summary) > 200 else doc.summary)

                col1, col2, col3 = st.columns(3)
                with col1:
                    st.metric("ç›¸å…³æ€§", f"{doc.relevance_score:.0%}")
                with col2:
                    st.metric("é˜…è¯»æ¬¡æ•°", doc.read_count)
                with col3:
                    st.metric("æå–å› å­", doc.extracted_factors)

                st.divider()

    def _render_knowledge_graph(self) -> None:
        """æ¸²æŸ“çŸ¥è¯†å›¾è°±"""
        st.subheader("ğŸ•¸ï¸ çŸ¥è¯†å›¾è°±")
        st.info("å±•ç¤ºå› å­ã€ç­–ç•¥ã€å¸‚åœºæ¦‚å¿µä¹‹é—´çš„å…³è”å…³ç³»")

        nodes = self.get_knowledge_graph()

        # æŒ‰ç±»å‹åˆ†ç»„
        node_types = {}
        for node in nodes:
            if node.node_type not in node_types:
                node_types[node.node_type] = []
            node_types[node.node_type].append(node)

        for node_type, type_nodes in node_types.items():
            st.markdown(f"### {node_type}")

            cols = st.columns(4)
            for i, node in enumerate(type_nodes[:8]):
                with cols[i % 4]:
                    st.markdown(f"**{node.name}**")
                    st.caption(f"è¿æ¥: {node.connections} | é‡è¦æ€§: {node.importance:.2f}")

            st.divider()

    def _render_scholar_scan(self) -> None:
        """æ¸²æŸ“Scholaræ‰«æ"""
        st.subheader("ğŸ” Scholarå¼•æ“æ‰«æ")
        st.info("è§¦å‘Scholarå¼•æ“æ‰«ææœ€æ–°ç ”æŠ¥å’Œè®ºæ–‡")

        with st.form("scholar_scan_form"):
            st.markdown("**é€‰æ‹©æ‰«ææ¥æº**")

            col1, col2 = st.columns(2)
            with col1:
                scan_arxiv = st.checkbox("arXiv (q-fin)", value=True)
                scan_ssrn = st.checkbox("SSRN", value=True)
            with col2:
                scan_broker = st.checkbox("åˆ¸å•†ç ”æŠ¥", value=True)
                scan_news = st.checkbox("è´¢ç»æ–°é—»", value=False)

            submitted = st.form_submit_button("ğŸš€ å¼€å§‹æ‰«æ", use_container_width=True)

        if submitted:
            sources = []
            if scan_arxiv:
                sources.append("arxiv")
            if scan_ssrn:
                sources.append("ssrn")
            if scan_broker:
                sources.append("broker")
            if scan_news:
                sources.append("news")

            if sources:
                result = self.trigger_scholar_scan(sources)
                st.success(f"æ‰«æä»»åŠ¡å·²å¯åŠ¨: {result['task_id']}")
                st.info(f"æ‰«ææ¥æº: {', '.join(sources)}")
            else:
                st.warning("è¯·è‡³å°‘é€‰æ‹©ä¸€ä¸ªæ‰«ææ¥æº")

    def _get_mock_documents(self, limit: int) -> List[Document]:
        """è·å–æ¨¡æ‹Ÿæ–‡æ¡£æ•°æ®"""
        mock_docs = [
            Document(
                "DOC001",
                "2026å¹´Aè‚¡å¸‚åœºå±•æœ›ï¼šç»“æ„æ€§æœºä¼šå‡¸æ˜¾",
                DocumentType.RESEARCH_REPORT,
                DocumentSource.BROKER,
                "ä¸­ä¿¡è¯åˆ¸ç ”ç©¶éƒ¨",
                datetime(2026, 1, 15),
                "æœ¬æŠ¥å‘Šåˆ†æäº†2026å¹´Aè‚¡å¸‚åœºçš„ä¸»è¦æŠ•èµ„æœºä¼šï¼Œé‡ç‚¹å…³æ³¨ç§‘æŠ€ã€æ¶ˆè´¹ã€æ–°èƒ½æºä¸‰å¤§æ¿å—...",
                ["Aè‚¡", "æŠ•èµ„ç­–ç•¥", "2026å±•æœ›"],
                0.92,
                156,
                3,
            ),
            Document(
                "DOC002",
                "é‡åŒ–å› å­æœ‰æ•ˆæ€§ç ”ç©¶ï¼šåŠ¨é‡å› å­çš„è¡°å‡ä¸é‡ç”Ÿ",
                DocumentType.ACADEMIC_PAPER,
                DocumentSource.ARXIV,
                "Zhang et al.",
                datetime(2026, 1, 10),
                "æœ¬æ–‡ç ”ç©¶äº†åŠ¨é‡å› å­åœ¨Aè‚¡å¸‚åœºçš„æœ‰æ•ˆæ€§å˜åŒ–ï¼Œå‘ç°ä¼ ç»ŸåŠ¨é‡å› å­å­˜åœ¨æ˜¾è‘—è¡°å‡...",
                ["åŠ¨é‡å› å­", "å› å­è¡°å‡", "é‡åŒ–æŠ•èµ„"],
                0.88,
                89,
                5,
            ),
            Document(
                "DOC003",
                "æœºå™¨å­¦ä¹ åœ¨å› å­æŒ–æ˜ä¸­çš„åº”ç”¨",
                DocumentType.ACADEMIC_PAPER,
                DocumentSource.SSRN,
                "Li & Wang",
                datetime(2026, 1, 8),
                "æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºæ·±åº¦å­¦ä¹ çš„å› å­æŒ–æ˜æ¡†æ¶ï¼Œèƒ½å¤Ÿè‡ªåŠ¨å‘ç°æœ‰æ•ˆçš„Alphaå› å­...",
                ["æœºå™¨å­¦ä¹ ", "å› å­æŒ–æ˜", "æ·±åº¦å­¦ä¹ "],
                0.85,
                67,
                8,
            ),
            Document(
                "DOC004",
                "æ–°èƒ½æºæ±½è½¦äº§ä¸šé“¾æ·±åº¦æŠ¥å‘Š",
                DocumentType.RESEARCH_REPORT,
                DocumentSource.BROKER,
                "å›½æ³°å›å®‰",
                datetime(2026, 1, 5),
                "æ–°èƒ½æºæ±½è½¦æ¸—é€ç‡æŒç»­æå‡ï¼Œäº§ä¸šé“¾ä¸Šä¸‹æ¸¸è¿æ¥æ–°ä¸€è½®å¢é•¿æœºé‡...",
                ["æ–°èƒ½æº", "æ±½è½¦", "äº§ä¸šé“¾"],
                0.78,
                234,
                2,
            ),
            Document(
                "DOC005",
                "ESGæŠ•èµ„ç­–ç•¥ç ”ç©¶ï¼šä¸­å›½å¸‚åœºçš„å®è¯åˆ†æ",
                DocumentType.ACADEMIC_PAPER,
                DocumentSource.ARXIV,
                "Chen et al.",
                datetime(2026, 1, 3),
                "æœ¬æ–‡é¦–æ¬¡ç³»ç»Ÿæ€§åœ°ç ”ç©¶äº†ESGå› å­åœ¨ä¸­å›½Aè‚¡å¸‚åœºçš„æœ‰æ•ˆæ€§...",
                ["ESG", "å¯æŒç»­æŠ•èµ„", "å› å­ç ”ç©¶"],
                0.82,
                45,
                4,
            ),
        ]
        return mock_docs[:limit]

    def _get_mock_knowledge_graph(self) -> List[KnowledgeNode]:
        """è·å–æ¨¡æ‹ŸçŸ¥è¯†å›¾è°±"""
        return [
            KnowledgeNode("N001", "åŠ¨é‡å› å­", "å› å­", 15, 0.92),
            KnowledgeNode("N002", "ä»·å€¼å› å­", "å› å­", 12, 0.88),
            KnowledgeNode("N003", "è´¨é‡å› å­", "å› å­", 10, 0.85),
            KnowledgeNode("N004", "è¶‹åŠ¿è·Ÿè¸ª", "ç­–ç•¥", 8, 0.78),
            KnowledgeNode("N005", "å‡å€¼å›å½’", "ç­–ç•¥", 7, 0.75),
            KnowledgeNode("N006", "ç§‘æŠ€æ¿å—", "å¸‚åœº", 20, 0.95),
            KnowledgeNode("N007", "æ¶ˆè´¹æ¿å—", "å¸‚åœº", 18, 0.90),
            KnowledgeNode("N008", "æ–°èƒ½æº", "å¸‚åœº", 16, 0.88),
        ]
