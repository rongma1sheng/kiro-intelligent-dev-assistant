# pylint: disable=too-many-lines
"""
MIAç³»ç»Ÿç»Ÿä¸€LLMè°ƒç”¨ç½‘å…³ (Unified LLM Gateway)

ç™½çš®ä¹¦ä¾æ®: ç¬¬äºŒç«  2.8 ç»Ÿä¸€è®°å¿†ç³»ç»Ÿ + ç¬¬åä¸€ç«  11.1 é˜²å¹»è§‰ç³»ç»Ÿ
ç‰ˆæœ¬: v1.6.0
ä½œè€…: MIA Team
æ—¥æœŸ: 2026-01-18

æ ¸å¿ƒç†å¿µ: æ‰€æœ‰LLMè°ƒç”¨å¿…é¡»ç»è¿‡ç»Ÿä¸€çš„æ§åˆ¶å•å…ƒï¼Œç¡®ä¿ï¼š
1. è®°å¿†ç³»ç»Ÿé›†æˆ - æ‰€æœ‰è°ƒç”¨éƒ½æœ‰ä¸Šä¸‹æ–‡è®°å¿†
2. é˜²å¹»è§‰æ£€æµ‹ - æ‰€æœ‰å“åº”éƒ½ç»è¿‡å¹»è§‰è¿‡æ»¤
3. æˆæœ¬æ§åˆ¶ - ç»Ÿä¸€çš„é¢„ç®—ç®¡ç†å’Œæˆæœ¬è¿½è¸ª
4. æ€§èƒ½ç›‘æ§ - å»¶è¿Ÿã€æˆåŠŸç‡ã€è´¨é‡æŒ‡æ ‡ç›‘æ§
5. å®‰å…¨å®¡è®¡ - æ‰€æœ‰è°ƒç”¨éƒ½æœ‰å®Œæ•´çš„å®¡è®¡æ—¥å¿—

ä¸¥ç¦ç›´æ¥è°ƒç”¨LLM APIï¼æ‰€æœ‰è°ƒç”¨å¿…é¡»é€šè¿‡LLMGatewayï¼
"""

import asyncio
import hashlib
import json
import time
from dataclasses import dataclass, field
from datetime import datetime
from enum import Enum
from typing import Any, Dict, List, Optional

import redis
from loguru import logger

from ..base.exceptions import ResourceError, ValidationError
from ..utils.logger import get_logger
from .adaptive_batch_scheduler import AdaptiveBatchScheduler
from .hallucination_filter import HallucinationFilter
from .memory.unified_memory_system import UnifiedMemorySystem
from .vllm_inference_engine import VLLMInferenceEngine

logger = get_logger(__name__)


class LLMProvider(Enum):
    """LLMæä¾›å•†"""

    QWEN_LOCAL = "qwen_local"  # æœ¬åœ°Qwen3-30B-MoE
    QWEN_CLOUD = "qwen_cloud"  # äº‘ç«¯Qwen3-Next-80B
    DEEPSEEK = "deepseek"  # DeepSeek-R1/v3.2
    GLM = "glm"  # GLM-4 (æ™ºè°±AI)
    CLAUDE = "claude"  # Claude-3.5 (å¦‚æœå¯ç”¨)


class CallType(Enum):
    """è°ƒç”¨ç±»å‹"""

    TRADING_DECISION = "trading_decision"  # äº¤æ˜“å†³ç­–
    STRATEGY_ANALYSIS = "strategy_analysis"  # ç­–ç•¥åˆ†æ
    RESEARCH_ANALYSIS = "research_analysis"  # ç ”ç©¶åˆ†æ
    FACTOR_GENERATION = "factor_generation"  # å› å­ç”Ÿæˆ
    CODE_GENERATION = "code_generation"  # ä»£ç ç”Ÿæˆ
    DATA_ANALYSIS = "data_analysis"  # æ•°æ®åˆ†æ
    RISK_ASSESSMENT = "risk_assessment"  # é£é™©è¯„ä¼°
    MARKET_SENTIMENT = "market_sentiment"  # å¸‚åœºæƒ…ç»ª


@dataclass
class LLMRequest:  # pylint: disable=too-many-instance-attributes
    """LLMè¯·æ±‚"""

    call_id: str = field(default_factory=lambda: hashlib.md5(f"{time.time()}".encode()).hexdigest()[:8])
    call_type: CallType = CallType.TRADING_DECISION
    provider: LLMProvider = LLMProvider.QWEN_LOCAL
    model: str = "qwen3-30b-moe"
    messages: List[Dict[str, str]] = field(default_factory=list)
    system_prompt: Optional[str] = None
    max_tokens: int = 2000
    temperature: float = 0.1
    timeout: float = 30.0

    # è®°å¿†ç³»ç»Ÿé…ç½®
    use_memory: bool = True
    memory_context_length: int = 10

    # å®‰å…¨é…ç½®
    enable_hallucination_filter: bool = True
    enable_content_filter: bool = True

    # æˆæœ¬æ§åˆ¶
    max_cost: float = 1.0  # æœ€å¤§æˆæœ¬ï¼ˆå…ƒï¼‰
    priority: int = 1  # ä¼˜å…ˆçº§ 1-5

    # å…ƒæ•°æ®
    caller_module: str = "unknown"
    caller_function: str = "unknown"
    business_context: Dict[str, Any] = field(default_factory=dict)
    created_at: datetime = field(default_factory=datetime.now)


@dataclass
class LLMResponse:  # pylint: disable=too-many-instance-attributes
    """LLMå“åº”"""

    call_id: str
    success: bool
    content: str = ""

    # è´¨é‡æŒ‡æ ‡
    hallucination_score: float = 0.0
    confidence_score: float = 0.0
    quality_score: float = 0.0

    # æ€§èƒ½æŒ‡æ ‡
    latency_ms: float = 0.0
    tokens_used: int = 0
    cost: float = 0.0

    # è®°å¿†ç³»ç»Ÿ
    memory_hits: int = 0
    memory_updates: int = 0

    # é”™è¯¯ä¿¡æ¯
    error_message: str = ""
    error_code: str = ""

    # å…ƒæ•°æ®
    provider_used: LLMProvider = LLMProvider.QWEN_LOCAL
    model_used: str = ""
    timestamp: datetime = field(default_factory=datetime.now)

    # å®¡è®¡ä¿¡æ¯
    audit_log: Dict[str, Any] = field(default_factory=dict)


class LLMGateway:  # pylint: disable=too-many-instance-attributes
    """ç»Ÿä¸€LLMè°ƒç”¨ç½‘å…³

    ç™½çš®ä¹¦ä¾æ®: ç¬¬äºŒç«  2.8 ç»Ÿä¸€è®°å¿†ç³»ç»Ÿ + ç¬¬åä¸€ç«  11.1 é˜²å¹»è§‰ç³»ç»Ÿ

    æ ¸å¿ƒèŒè´£:
    1. ç»Ÿä¸€æ‰€æœ‰LLMè°ƒç”¨å…¥å£
    2. é›†æˆè®°å¿†ç³»ç»Ÿ (Engram + ä¼ ç»Ÿè®°å¿†)
    3. é˜²å¹»è§‰æ£€æµ‹å’Œè¿‡æ»¤
    4. æˆæœ¬æ§åˆ¶å’Œé¢„ç®—ç®¡ç†
    5. æ€§èƒ½ç›‘æ§å’Œè´¨é‡è¯„ä¼°
    6. å®‰å…¨å®¡è®¡å’Œæ—¥å¿—è®°å½•
    7. æ•…éšœè½¬ç§»å’Œé™çº§å¤„ç†

    ä½¿ç”¨ç¤ºä¾‹:
        >>> gateway = LLMGateway()
        >>> request = LLMRequest(
        ...     call_type=CallType.TRADING_DECISION,
        ...     messages=[{"role": "user", "content": "åˆ†æå½“å‰å¸‚åœº"}],
        ...     caller_module="soldier",
        ...     caller_function="make_decision"
        ... )
        >>> response = await gateway.call_llm(request)
        >>> if response.success:
        ...     print(f"å†³ç­–: {response.content}")
    """

    def __init__(self, redis_client: Optional[redis.Redis] = None):
        """åˆå§‹åŒ–LLMç½‘å…³

        Args:
            redis_client: Rediså®¢æˆ·ç«¯ï¼Œç”¨äºè®°å¿†ç³»ç»Ÿå’Œç¼“å­˜
        """
        # Redisè¿æ¥
        self.redis_client = redis_client or redis.Redis(host="localhost", port=6379, db=0, decode_responses=True)

        # æ ¸å¿ƒç»„ä»¶åˆå§‹åŒ–
        self.memory_system = UnifiedMemorySystem(redis_client=self.redis_client)
        self.hallucination_filter = HallucinationFilter()

        # vLLMé›†æˆç»„ä»¶ - Task 10.7
        self.vllm_engine: Optional[VLLMInferenceEngine] = None
        self.batch_scheduler: Optional[AdaptiveBatchScheduler] = None

        # æˆæœ¬æ§åˆ¶
        self.cost_tracker = CostTracker(redis_client=self.redis_client)
        self.budget_manager = BudgetManager(daily_budget=50.0, monthly_budget=1500.0)  # æ—¥é¢„ç®—50å…ƒ  # æœˆé¢„ç®—1500å…ƒ

        # æ€§èƒ½ç›‘æ§
        self.performance_monitor = PerformanceMonitor(redis_client=self.redis_client)

        # LLMå®¢æˆ·ç«¯æ± 
        self.llm_clients = {}
        self._initialize_llm_clients()

        # Task 14.5: å¹¶å‘æ§åˆ¶å’Œè¯·æ±‚é˜Ÿåˆ—
        self.max_concurrent_calls = 10  # æœ€å¤§å¹¶å‘æ•°
        self.concurrent_semaphore = asyncio.Semaphore(self.max_concurrent_calls)
        self.request_queue: asyncio.Queue = asyncio.Queue(maxsize=100)

        # Task 14.5: é‡è¯•é…ç½®
        self.max_retries = 3  # æœ€å¤§é‡è¯•æ¬¡æ•°
        self.retry_base_delay = 1.0  # åŸºç¡€å»¶è¿Ÿï¼ˆç§’ï¼‰
        self.retry_max_delay = 10.0  # æœ€å¤§å»¶è¿Ÿï¼ˆç§’ï¼‰

        # è°ƒç”¨ç»Ÿè®¡
        self.call_stats = {
            "total_calls": 0,
            "successful_calls": 0,
            "failed_calls": 0,
            "hallucination_detected": 0,
            "budget_exceeded": 0,
            "fallback_used": 0,
            "vllm_calls": 0,
            "batch_calls": 0,
            "retries": 0,
            "timeouts": 0,
            "concurrent_limit_hits": 0,
        }

        logger.info("LLMç½‘å…³åˆå§‹åŒ–å®Œæˆ - ç»Ÿä¸€è®°å¿†ç³»ç»Ÿ + é˜²å¹»è§‰ç³»ç»Ÿ + vLLMé›†æˆ + å¹¶å‘æ§åˆ¶å·²å°±ç»ª")

    async def initialize(self):
        """åˆå§‹åŒ–LLMç½‘å…³ - å¼‚æ­¥åˆå§‹åŒ–æ–¹æ³•

        ç™½çš®ä¹¦ä¾æ®: ç¬¬äºŒç«  2.1 AIä¸‰è„‘æ¶æ„ - vLLMé›†æˆ
        éœ€æ±‚: 8.2, 8.8 - vLLMé›†æˆåˆ°AIä¸‰è„‘
        """
        try:
            logger.info("[LLMGateway] Starting vLLM integration initialization...")

            # åˆå§‹åŒ–vLLMæ¨ç†å¼•æ“
            self.vllm_engine = VLLMInferenceEngine()
            await self.vllm_engine.initialize()

            # åˆå§‹åŒ–è‡ªé€‚åº”æ‰¹å¤„ç†è°ƒåº¦å™¨
            self.batch_scheduler = AdaptiveBatchScheduler()
            await self.batch_scheduler.initialize()

            logger.info("[LLMGateway] vLLMé›†æˆåˆå§‹åŒ–å®Œæˆ")

        except Exception as e:  # pylint: disable=broad-exception-caught
            logger.error(f"[LLMGateway] vLLMé›†æˆåˆå§‹åŒ–å¤±è´¥: {e}")  # pylint: disable=logging-fstring-interpolation
            # é™çº§åˆ°ä¼ ç»Ÿæ¨¡å¼
            logger.warning("[LLMGateway] é™çº§åˆ°ä¼ ç»ŸLLMè°ƒç”¨æ¨¡å¼")

        logger.info("LLMç½‘å…³å¼‚æ­¥åˆå§‹åŒ–å®Œæˆ")

    async def call_llm(self, request: LLMRequest) -> LLMResponse:
        """ç»Ÿä¸€LLMè°ƒç”¨æ¥å£ - Task 14.5å¢å¼ºç‰ˆ

        è¿™æ˜¯ç³»ç»Ÿä¸­å”¯ä¸€åˆæ³•çš„LLMè°ƒç”¨å…¥å£ï¼
        æ‰€æœ‰å…¶ä»–æ¨¡å—å¿…é¡»é€šè¿‡æ­¤æ¥å£è°ƒç”¨LLMã€‚

        ç™½çš®ä¹¦ä¾æ®: ç¬¬äºŒç«  2.8 ç»Ÿä¸€è®°å¿†ç³»ç»Ÿ + ç¬¬åä¸€ç«  11.1 é˜²å¹»è§‰ç³»ç»Ÿ
        éœ€æ±‚: 7.6 - LLMè°ƒç”¨ä¼˜åŒ–ï¼ˆè¶…æ—¶ã€é‡è¯•ã€å¹¶å‘æ§åˆ¶ï¼‰

        Args:
            request: LLMè¯·æ±‚å¯¹è±¡

        Returns:
            LLMå“åº”å¯¹è±¡

        Raises:
            ValidationError: è¯·æ±‚å‚æ•°æ— æ•ˆ
            ResourceError: èµ„æºä¸è¶³æˆ–é¢„ç®—è¶…é™
        """
        # Task 14.5: å¹¶å‘æ§åˆ¶
        async with self.concurrent_semaphore:
            # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°å¹¶å‘é™åˆ¶
            if self.concurrent_semaphore.locked():
                self.call_stats["concurrent_limit_hits"] += 1
                logger.warning(  # pylint: disable=logging-fstring-interpolation
                    f"[LLMGateway] è¾¾åˆ°å¹¶å‘é™åˆ¶: {self.max_concurrent_calls}"
                )  # pylint: disable=logging-fstring-interpolation

            # Task 14.5: é‡è¯•æœºåˆ¶ï¼ˆæŒ‡æ•°é€€é¿ï¼‰
            return await self._call_llm_with_retry(request)

    async def _call_llm_with_retry(self, request: LLMRequest) -> LLMResponse:
        """å¸¦é‡è¯•æœºåˆ¶çš„LLMè°ƒç”¨

        ç™½çš®ä¹¦ä¾æ®: ç¬¬ä¸ƒç«  7.6 LLMè°ƒç”¨ä¼˜åŒ–
        éœ€æ±‚: 7.6 - é‡è¯•æœºåˆ¶ï¼ˆæŒ‡æ•°é€€é¿ï¼‰

        Args:
            request: LLMè¯·æ±‚å¯¹è±¡

        Returns:
            LLMå“åº”å¯¹è±¡
        """
        last_error = None

        for attempt in range(self.max_retries + 1):
            try:
                # æ‰§è¡Œå®é™…è°ƒç”¨
                response = await self._execute_call_with_timeout(request)

                # æˆåŠŸåˆ™è¿”å›
                if response.success:
                    if attempt > 0:
                        logger.info(  # pylint: disable=logging-fstring-interpolation
                            f"[LLMGateway] é‡è¯•æˆåŠŸ: {request.call_id}, å°è¯•æ¬¡æ•°: {attempt + 1}"
                        )  # pylint: disable=logging-fstring-interpolation
                    return response

                # å¦‚æœæ˜¯ä¸å¯é‡è¯•çš„é”™è¯¯ï¼Œç›´æ¥è¿”å›
                if response.error_code in ["VALIDATION_ERROR", "BUDGET_EXCEEDED", "HALLUCINATION_DETECTED"]:
                    return response

                # è®°å½•å¤±è´¥ï¼Œå‡†å¤‡é‡è¯•
                last_error = response.error_message

            except asyncio.TimeoutError as e:
                last_error = f"è¶…æ—¶: {str(e)}"
                self.call_stats["timeouts"] += 1
                logger.warning(  # pylint: disable=logging-fstring-interpolation
                    f"[LLMGateway] è°ƒç”¨è¶…æ—¶: {request.call_id}, å°è¯•: {attempt + 1}/{self.max_retries + 1}"
                )  # pylint: disable=logging-fstring-interpolation

            except Exception as e:  # pylint: disable=broad-exception-caught
                last_error = str(e)
                logger.error(  # pylint: disable=logging-fstring-interpolation
                    f"[LLMGateway] è°ƒç”¨å¼‚å¸¸: {request.call_id}, é”™è¯¯: {e}, å°è¯•: {attempt + 1}/{self.max_retries + 1}"
                )

            # å¦‚æœè¿˜æœ‰é‡è¯•æœºä¼šï¼Œç­‰å¾…åé‡è¯•
            if attempt < self.max_retries:
                # æŒ‡æ•°é€€é¿
                delay = min(self.retry_base_delay * (2**attempt), self.retry_max_delay)
                logger.info(  # pylint: disable=logging-fstring-interpolation
                    f"[LLMGateway] ç­‰å¾… {delay:.1f}s åé‡è¯•..."
                )  # pylint: disable=logging-fstring-interpolation
                await asyncio.sleep(delay)
                self.call_stats["retries"] += 1

        # æ‰€æœ‰é‡è¯•éƒ½å¤±è´¥
        error_response = LLMResponse(
            call_id=request.call_id,
            success=False,
            error_message=f"é‡è¯•{self.max_retries}æ¬¡åä»å¤±è´¥: {last_error}",
            error_code="MAX_RETRIES_EXCEEDED",
        )

        self.call_stats["failed_calls"] += 1
        logger.error(f"[LLMGateway] è°ƒç”¨æœ€ç»ˆå¤±è´¥: {request.call_id}")  # pylint: disable=logging-fstring-interpolation

        return error_response

    async def _execute_call_with_timeout(self, request: LLMRequest) -> LLMResponse:
        """å¸¦è¶…æ—¶æ§åˆ¶çš„LLMè°ƒç”¨

        ç™½çš®ä¹¦ä¾æ®: ç¬¬ä¸ƒç«  7.6 LLMè°ƒç”¨ä¼˜åŒ–
        éœ€æ±‚: 7.6 - è¶…æ—¶æ§åˆ¶

        Args:
            request: LLMè¯·æ±‚å¯¹è±¡

        Returns:
            LLMå“åº”å¯¹è±¡

        Raises:
            asyncio.TimeoutError: è°ƒç”¨è¶…æ—¶
        """
        start_time = time.perf_counter()

        try:
            # Task 14.5: è¶…æ—¶æ§åˆ¶
            response = await asyncio.wait_for(self._execute_call_internal(request), timeout=request.timeout)

            return response

        except asyncio.TimeoutError:
            # è¶…æ—¶å¤„ç†
            error_response = LLMResponse(  # pylint: disable=unused-variable
                call_id=request.call_id,
                success=False,
                error_message=f"è°ƒç”¨è¶…æ—¶: {request.timeout}ç§’",
                error_code="TIMEOUT_ERROR",
                latency_ms=(time.perf_counter() - start_time) * 1000,
            )

            logger.warning(  # pylint: disable=logging-fstring-interpolation
                f"[LLMGateway] è°ƒç”¨è¶…æ—¶: {request.call_id}, è¶…æ—¶è®¾ç½®: {request.timeout}s"
            )  # pylint: disable=logging-fstring-interpolation
            raise

    async def _execute_call_internal(self, request: LLMRequest) -> LLMResponse:
        """å†…éƒ¨è°ƒç”¨æ‰§è¡Œé€»è¾‘ï¼ˆåŸcall_llmçš„æ ¸å¿ƒé€»è¾‘ï¼‰

        Args:
            request: LLMè¯·æ±‚å¯¹è±¡

        Returns:
            LLMå“åº”å¯¹è±¡
        """
        start_time = time.perf_counter()

        try:
            # 1. è¯·æ±‚éªŒè¯
            self._validate_request(request)

            # 2. é¢„ç®—æ£€æŸ¥
            await self._check_budget(request)

            # 3. è®°å¿†ç³»ç»Ÿå¢å¼º
            enhanced_request = await self._enhance_with_memory(request)

            # 4. æ‰§è¡ŒLLMè°ƒç”¨
            raw_response = await self._execute_llm_call(enhanced_request)

            # 5. é˜²å¹»è§‰æ£€æµ‹
            filtered_response = await self._filter_hallucination(raw_response, enhanced_request)

            # 6. æ›´æ–°è®°å¿†ç³»ç»Ÿ
            await self._update_memory(enhanced_request, filtered_response)

            # 7. æˆæœ¬è®°å½•
            await self._record_cost(enhanced_request, filtered_response)

            # 8. æ€§èƒ½ç›‘æ§
            latency = (time.perf_counter() - start_time) * 1000
            await self._record_performance(enhanced_request, filtered_response, latency)

            # 9. å®¡è®¡æ—¥å¿—
            await self._log_audit(enhanced_request, filtered_response)

            # 10. æ›´æ–°ç»Ÿè®¡
            self._update_stats(filtered_response)

            logger.info(  # pylint: disable=logging-fstring-interpolation
                f"LLMè°ƒç”¨å®Œæˆ: {request.call_id}, å»¶è¿Ÿ: {latency:.2f}ms, æˆæœ¬: Â¥{filtered_response.cost:.4f}"
            )  # pylint: disable=logging-fstring-interpolation

            return filtered_response

        except Exception as e:  # pylint: disable=broad-exception-caught
            # é”™è¯¯å¤„ç†
            error_response = LLMResponse(
                call_id=request.call_id,
                success=False,
                error_message=str(e),
                error_code=type(e).__name__,
                latency_ms=(time.perf_counter() - start_time) * 1000,
            )

            # è®°å½•é”™è¯¯
            await self._log_error(request, error_response, e)

            logger.error(f"LLMè°ƒç”¨å¤±è´¥: {request.call_id}, é”™è¯¯: {e}")  # pylint: disable=logging-fstring-interpolation

            return error_response

    def _validate_request(self, request: LLMRequest) -> None:
        """éªŒè¯è¯·æ±‚å‚æ•°"""
        if not request.messages:
            raise ValidationError("æ¶ˆæ¯åˆ—è¡¨ä¸èƒ½ä¸ºç©º")

        if request.max_tokens <= 0 or request.max_tokens > 8000:
            raise ValidationError(f"max_tokenså¿…é¡»åœ¨1-8000ä¹‹é—´ï¼Œå½“å‰: {request.max_tokens}")

        if not 0 <= request.temperature <= 2:
            raise ValidationError(f"temperatureå¿…é¡»åœ¨0-2ä¹‹é—´ï¼Œå½“å‰: {request.temperature}")

        if request.timeout <= 0 or request.timeout > 300:
            raise ValidationError(f"timeoutå¿…é¡»åœ¨0-300ç§’ä¹‹é—´ï¼Œå½“å‰: {request.timeout}")

        if not request.caller_module or not request.caller_function:
            raise ValidationError("å¿…é¡»æŒ‡å®šcaller_moduleå’Œcaller_function")

    async def _check_budget(self, request: LLMRequest) -> None:
        """æ£€æŸ¥é¢„ç®—é™åˆ¶"""
        # ä¼°ç®—è°ƒç”¨æˆæœ¬
        estimated_cost = self._estimate_cost(request)

        # æ£€æŸ¥é¢„ç®—
        if not await self.budget_manager.check_budget(estimated_cost):
            raise ResourceError(f"é¢„ç®—ä¸è¶³ï¼Œä¼°ç®—æˆæœ¬: Â¥{estimated_cost:.4f}")

        # æ£€æŸ¥å•æ¬¡è°ƒç”¨æˆæœ¬é™åˆ¶
        if estimated_cost > request.max_cost:
            raise ResourceError(f"å•æ¬¡è°ƒç”¨æˆæœ¬è¶…é™: Â¥{estimated_cost:.4f} > Â¥{request.max_cost:.4f}")

    async def _enhance_with_memory(self, request: LLMRequest) -> LLMRequest:
        """ä½¿ç”¨è®°å¿†ç³»ç»Ÿå¢å¼ºè¯·æ±‚"""
        if not request.use_memory:
            return request

        try:
            # æ„å»ºæŸ¥è¯¢ä¸Šä¸‹æ–‡
            query_context = {
                "call_type": request.call_type.value,
                "caller_module": request.caller_module,
                "business_context": request.business_context,
                "messages": request.messages[-3:],  # æœ€è¿‘3æ¡æ¶ˆæ¯
            }

            # æŸ¥è¯¢Engramè®°å¿†
            memory_vector = await self.memory_system.engram_memory.query_memory(
                text=json.dumps(query_context), context=query_context
            )

            # æŸ¥è¯¢ä¼ ç»Ÿè®°å¿†
            traditional_context = await self.memory_system.get_relevant_context(
                query_context, max_items=request.memory_context_length
            )

            # å¢å¼ºç³»ç»Ÿæç¤º
            enhanced_system_prompt = self._build_enhanced_system_prompt(
                request.system_prompt, memory_vector, traditional_context, request.call_type
            )

            # åˆ›å»ºå¢å¼ºè¯·æ±‚
            enhanced_request = LLMRequest(
                call_id=request.call_id,
                call_type=request.call_type,
                provider=request.provider,
                model=request.model,
                messages=request.messages.copy(),
                system_prompt=enhanced_system_prompt,
                max_tokens=request.max_tokens,
                temperature=request.temperature,
                timeout=request.timeout,
                use_memory=request.use_memory,
                memory_context_length=request.memory_context_length,
                enable_hallucination_filter=request.enable_hallucination_filter,
                enable_content_filter=request.enable_content_filter,
                max_cost=request.max_cost,
                priority=request.priority,
                caller_module=request.caller_module,
                caller_function=request.caller_function,
                business_context=request.business_context,
                created_at=request.created_at,
            )

            logger.debug(f"è®°å¿†ç³»ç»Ÿå¢å¼ºå®Œæˆ: {request.call_id}")  # pylint: disable=logging-fstring-interpolation
            return enhanced_request

        except Exception as e:  # pylint: disable=broad-exception-caught
            logger.warning(f"è®°å¿†ç³»ç»Ÿå¢å¼ºå¤±è´¥: {e}, ä½¿ç”¨åŸå§‹è¯·æ±‚")  # pylint: disable=logging-fstring-interpolation
            return request

    def _build_enhanced_system_prompt(
        self,
        original_prompt: Optional[str],
        memory_vector: Optional[Dict],
        traditional_context: List[Dict],
        call_type: CallType,
    ) -> str:
        """æ„å»ºå¢å¼ºçš„ç³»ç»Ÿæç¤º"""

        # åŸºç¡€ç³»ç»Ÿæç¤º
        base_prompt = original_prompt or self._get_default_system_prompt(call_type)

        # è®°å¿†å¢å¼ºéƒ¨åˆ†
        memory_enhancement = ""

        if memory_vector:
            memory_enhancement += f"""
## ğŸ§  ç›¸å…³è®°å¿† (Engram)
åŸºäºå†å²ç»éªŒï¼Œä»¥ä¸‹ä¿¡æ¯å¯èƒ½ç›¸å…³ï¼š
{memory_vector.get('summary', 'æš‚æ— ç›¸å…³è®°å¿†')}

å…³é”®æ¨¡å¼: {memory_vector.get('patterns', [])}
ç½®ä¿¡åº¦: {memory_vector.get('confidence', 0.0):.2f}
"""

        if traditional_context:
            memory_enhancement += """  # pylint: disable=w1309
## ğŸ“š ä¸Šä¸‹æ–‡è®°å¿† (Traditional)
æœ€è¿‘ç›¸å…³çš„å†³ç­–å’Œåˆ†æï¼š
"""
            for i, ctx in enumerate(traditional_context[:3], 1):
                memory_enhancement += f"{i}. {ctx.get('summary', 'æ— æ‘˜è¦')}\n"

        # ç»„åˆå¢å¼ºæç¤º
        enhanced_prompt = f"""{base_prompt}

{memory_enhancement}

## âš ï¸ é‡è¦æé†’
1. åŸºäºä¸Šè¿°è®°å¿†å’Œä¸Šä¸‹æ–‡è¿›è¡Œåˆ†æ
2. å¦‚æœè®°å¿†ä¸­çš„ä¿¡æ¯ä¸å½“å‰æƒ…å†µå†²çªï¼Œä¼˜å…ˆè€ƒè™‘å½“å‰æ•°æ®
3. æ˜ç¡®è¯´æ˜ä½ çš„æ¨ç†è¿‡ç¨‹å’Œä¾æ®
4. é¿å…äº§ç”Ÿä¸å†å²è®°å½•çŸ›ç›¾çš„ç»“è®º
"""

        return enhanced_prompt

    def _get_default_system_prompt(self, call_type: CallType) -> str:
        """è·å–é»˜è®¤ç³»ç»Ÿæç¤º"""
        prompts = {
            CallType.TRADING_DECISION: """ä½ æ˜¯MIAç³»ç»Ÿçš„äº¤æ˜“å†³ç­–AIã€‚åŸºäºå¸‚åœºæ•°æ®å’ŒæŠ€æœ¯åˆ†æï¼Œæä¾›å‡†ç¡®çš„äº¤æ˜“å»ºè®®ã€‚
è¦æ±‚ï¼š1) æ˜ç¡®çš„ä¹°å…¥/å–å‡º/æŒæœ‰å»ºè®® 2) è¯¦ç»†çš„ç†ç”±è¯´æ˜ 3) é£é™©è¯„ä¼° 4) æ­¢æŸå»ºè®®""",
            CallType.STRATEGY_ANALYSIS: """ä½ æ˜¯MIAç³»ç»Ÿçš„ç­–ç•¥åˆ†æAIã€‚æ·±åº¦åˆ†æäº¤æ˜“ç­–ç•¥çš„æœ‰æ•ˆæ€§ã€é£é™©å’Œæ”¹è¿›å»ºè®®ã€‚
è¦æ±‚ï¼š1) ç­–ç•¥ä¼˜ç¼ºç‚¹åˆ†æ 2) å†å²è¡¨ç°è¯„ä¼° 3) é£é™©å› å­è¯†åˆ« 4) ä¼˜åŒ–å»ºè®®""",
            CallType.RESEARCH_ANALYSIS: """ä½ æ˜¯MIAç³»ç»Ÿçš„ç ”ç©¶åˆ†æAIã€‚åˆ†æå­¦æœ¯è®ºæ–‡ã€ç ”æŠ¥å’Œå¸‚åœºç ”ç©¶ï¼Œæå–å¯æ‰§è¡Œçš„æŠ•èµ„æ´å¯Ÿã€‚
è¦æ±‚ï¼š1) æ ¸å¿ƒè§‚ç‚¹æå– 2) æŠ•èµ„æœºä¼šè¯†åˆ« 3) é£é™©æç¤º 4) å®æ–½å»ºè®®""",
            CallType.FACTOR_GENERATION: """ä½ æ˜¯MIAç³»ç»Ÿçš„å› å­ç”ŸæˆAIã€‚åŸºäºç†è®ºç ”ç©¶å’Œå¸‚åœºæ•°æ®ï¼Œè®¾è®¡æœ‰æ•ˆçš„é‡åŒ–å› å­ã€‚
è¦æ±‚ï¼š1) å› å­é€»è¾‘è¯´æ˜ 2) å®Œæ•´Pythonä»£ç  3) é¢„æœŸæ•ˆæœè¯„ä¼° 4) é£é™©æ§åˆ¶""",
            CallType.CODE_GENERATION: """ä½ æ˜¯MIAç³»ç»Ÿçš„ä»£ç ç”ŸæˆAIã€‚ç”Ÿæˆé«˜è´¨é‡ã€å¯æ‰§è¡Œçš„Pythonä»£ç ã€‚
è¦æ±‚ï¼š1) å®Œæ•´å¯è¿è¡Œä»£ç  2) è¯¦ç»†æ³¨é‡Š 3) é”™è¯¯å¤„ç† 4) æ€§èƒ½ä¼˜åŒ–""",
            CallType.DATA_ANALYSIS: """ä½ æ˜¯MIAç³»ç»Ÿçš„æ•°æ®åˆ†æAIã€‚æ·±åº¦åˆ†æå¸‚åœºæ•°æ®ï¼Œå‘ç°æ¨¡å¼å’Œå¼‚å¸¸ã€‚
è¦æ±‚ï¼š1) æ•°æ®è´¨é‡è¯„ä¼° 2) æ¨¡å¼è¯†åˆ« 3) å¼‚å¸¸æ£€æµ‹ 4) æŠ•èµ„å«ä¹‰""",
            CallType.RISK_ASSESSMENT: """ä½ æ˜¯MIAç³»ç»Ÿçš„é£é™©è¯„ä¼°AIã€‚å…¨é¢è¯„ä¼°æŠ•èµ„é£é™©ï¼Œæä¾›é£æ§å»ºè®®ã€‚
è¦æ±‚ï¼š1) é£é™©ç±»å‹è¯†åˆ« 2) é£é™©é‡åŒ– 3) ç¼“è§£æªæ–½ 4) ç›‘æ§æŒ‡æ ‡""",
            CallType.MARKET_SENTIMENT: """ä½ æ˜¯MIAç³»ç»Ÿçš„å¸‚åœºæƒ…ç»ªAIã€‚åˆ†æå¸‚åœºæƒ…ç»ªå’ŒæŠ•èµ„è€…è¡Œä¸ºã€‚
è¦æ±‚ï¼š1) æƒ…ç»ªæŒ‡æ ‡åˆ†æ 2) è¡Œä¸ºæ¨¡å¼è¯†åˆ« 3) æƒ…ç»ªè½¬æŠ˜ç‚¹ 4) äº¤æ˜“å«ä¹‰""",
        }

        return prompts.get(call_type, "ä½ æ˜¯MIAç³»ç»Ÿçš„AIåŠ©æ‰‹ï¼Œè¯·æä¾›ä¸“ä¸šã€å‡†ç¡®çš„åˆ†æå’Œå»ºè®®ã€‚")

    async def _execute_llm_call(self, request: LLMRequest) -> LLMResponse:
        """æ‰§è¡Œå®é™…çš„LLMè°ƒç”¨

        ç™½çš®ä¹¦ä¾æ®: ç¬¬äºŒç«  2.1 AIä¸‰è„‘æ¶æ„ - vLLMé›†æˆä¼˜åŒ–
        éœ€æ±‚: 8.2, 8.8 - vLLMé›†æˆåˆ°AIä¸‰è„‘
        """
        try:
            # ä¼˜å…ˆä½¿ç”¨vLLMå¼•æ“ï¼ˆå¦‚æœå¯ç”¨ä¸”é€‚åˆï¼‰
            if self._should_use_vllm(request):
                return await self._execute_vllm_call(request)

            # å›é€€åˆ°ä¼ ç»ŸLLMè°ƒç”¨
            return await self._execute_traditional_llm_call(request)

        except Exception as e:
            logger.error(f"LLMè°ƒç”¨æ‰§è¡Œå¤±è´¥: {e}")  # pylint: disable=logging-fstring-interpolation
            raise

    def _should_use_vllm(self, request: LLMRequest) -> bool:
        """åˆ¤æ–­æ˜¯å¦åº”è¯¥ä½¿ç”¨vLLM

        Args:
            request: LLMè¯·æ±‚

        Returns:
            bool: æ˜¯å¦ä½¿ç”¨vLLM
        """
        # vLLMå¯ç”¨æ€§æ£€æŸ¥
        if not self.vllm_engine or not self.batch_scheduler:
            return False

        # æœ¬åœ°æ¨ç†ä¼˜å…ˆä½¿ç”¨vLLM
        if request.provider == LLMProvider.QWEN_LOCAL:
            return True

        # é«˜é¢‘è°ƒç”¨ï¼ˆSoldierï¼‰ä¼˜å…ˆä½¿ç”¨vLLM
        if request.call_type == CallType.TRADING_DECISION:
            return True

        # æ‰¹å¤„ç†å‹å¥½çš„è°ƒç”¨ç±»å‹
        batch_friendly_types = {CallType.STRATEGY_ANALYSIS, CallType.FACTOR_GENERATION, CallType.DATA_ANALYSIS}

        if request.call_type in batch_friendly_types:
            return True

        return False

    async def _execute_vllm_call(self, request: LLMRequest) -> LLMResponse:
        """æ‰§è¡ŒvLLMè°ƒç”¨

        ç™½çš®ä¹¦ä¾æ®: ç¬¬äºŒç«  2.1 AIä¸‰è„‘æ¶æ„ - vLLMä¼˜åŒ–
        éœ€æ±‚: 8.2, 8.8 - vLLMé›†æˆä¼˜åŒ–
        """
        start_time = time.perf_counter()

        try:
            # ç¡®å®šè°ƒç”¨è€…æ¨¡å—ä»¥è®¾ç½®ä¼˜å…ˆçº§
            caller_module = request.caller_module.lower()

            # æ„å»ºvLLMæ¨ç†è¯·æ±‚
            vllm_request = {
                "prompt": self._build_vllm_prompt(request),
                "max_tokens": request.max_tokens,
                "temperature": request.temperature,
                "caller_module": caller_module,
                "request_id": request.call_id,
            }

            # é€šè¿‡æ‰¹å¤„ç†è°ƒåº¦å™¨æäº¤è¯·æ±‚
            if self.batch_scheduler:
                # å¼‚æ­¥æäº¤åˆ°æ‰¹å¤„ç†é˜Ÿåˆ—
                await self.batch_scheduler.submit_request(
                    request_id=request.call_id,
                    source_module=caller_module,
                    prompt=vllm_request["prompt"],
                    max_tokens=request.max_tokens,
                    temperature=request.temperature,
                )

                # ç­‰å¾…æ‰¹å¤„ç†ç»“æœï¼ˆç®€åŒ–å®ç°ï¼‰
                await asyncio.sleep(0.1)  # ç­‰å¾…æ‰¹å¤„ç†å®Œæˆ

                # æ¨¡æ‹ŸvLLMæ‰¹å¤„ç†ç»“æœ
                content = f"[vLLMæ‰¹å¤„ç†å“åº”] {caller_module}æ¨¡å—è¯·æ±‚å¤„ç†å®Œæˆ"
                tokens_used = min(request.max_tokens, 200)

                self.call_stats["batch_calls"] += 1

            else:
                # ç›´æ¥è°ƒç”¨vLLMå¼•æ“
                result = await self.vllm_engine.generate_async(  # pylint: disable=e1123,e1120
                    prompt=vllm_request["prompt"],
                    max_tokens=request.max_tokens,
                    temperature=request.temperature,
                    caller_module=caller_module,
                )

                content = result.get("text", "")
                tokens_used = result.get("tokens_used", 0)

            latency = (time.perf_counter() - start_time) * 1000

            # æ„å»ºå“åº”
            response = LLMResponse(
                call_id=request.call_id,
                success=True,
                content=content,
                latency_ms=latency,
                tokens_used=tokens_used,
                cost=0.0,  # vLLMæœ¬åœ°æ¨ç†æ— æˆæœ¬
                provider_used=LLMProvider.QWEN_LOCAL,
                model_used="qwen3-30b-moe-vllm",
            )

            self.call_stats["vllm_calls"] += 1

            logger.debug(  # pylint: disable=logging-fstring-interpolation
                f"[LLMGateway] vLLMè°ƒç”¨å®Œæˆ: {request.call_id}, å»¶è¿Ÿ: {latency:.2f}ms"
            )  # pylint: disable=logging-fstring-interpolation

            return response

        except Exception as e:  # pylint: disable=broad-exception-caught
            logger.error(f"[LLMGateway] vLLMè°ƒç”¨å¤±è´¥: {e}")  # pylint: disable=logging-fstring-interpolation
            # å›é€€åˆ°ä¼ ç»Ÿè°ƒç”¨
            logger.info(
                f"[LLMGateway] å›é€€åˆ°ä¼ ç»ŸLLMè°ƒç”¨: {request.call_id}"
            )  # pylint: disable=logging-fstring-interpolation
            return await self._execute_traditional_llm_call(request)

    def _build_vllm_prompt(self, request: LLMRequest) -> str:
        """æ„å»ºvLLMæç¤º

        Args:
            request: LLMè¯·æ±‚

        Returns:
            str: æ ¼å¼åŒ–çš„æç¤º
        """
        # ç³»ç»Ÿæç¤º
        system_prompt = request.system_prompt or self._get_default_system_prompt(request.call_type)

        # æ„å»ºå¯¹è¯å†å²
        conversation = []

        if system_prompt:
            conversation.append(f"System: {system_prompt}")

        for message in request.messages:
            role = message.get("role", "user")
            content = message.get("content", "")

            if role == "system":
                conversation.append(f"System: {content}")
            elif role == "user":
                conversation.append(f"Human: {content}")
            elif role == "assistant":
                conversation.append(f"Assistant: {content}")

        # æ·»åŠ Assistantæç¤º
        conversation.append("Assistant:")

        return "\n\n".join(conversation)

    async def _execute_traditional_llm_call(self, request: LLMRequest) -> LLMResponse:
        """æ‰§è¡Œä¼ ç»ŸLLMè°ƒç”¨ï¼ˆåŸæœ‰é€»è¾‘ï¼‰"""
        # é€‰æ‹©æœ€ä½³æä¾›å•†
        provider = await self._select_best_provider(request)

        # è·å–å®¢æˆ·ç«¯
        client = self.llm_clients.get(provider)
        if not client:
            raise ResourceError(f"LLMå®¢æˆ·ç«¯æœªåˆå§‹åŒ–: {provider}")

        # æ‰§è¡Œè°ƒç”¨
        start_time = time.perf_counter()

        if provider == LLMProvider.QWEN_LOCAL:
            result = await self._call_qwen_local(client, request)
        elif provider == LLMProvider.QWEN_CLOUD:
            result = await self._call_qwen_cloud(client, request)
        elif provider == LLMProvider.DEEPSEEK:
            result = await self._call_deepseek(client, request)
        elif provider == LLMProvider.GLM:
            result = await self._call_glm(client, request)
        else:
            raise ResourceError(f"ä¸æ”¯æŒçš„LLMæä¾›å•†: {provider}")

        latency = (time.perf_counter() - start_time) * 1000

        # æ„å»ºå“åº”
        response = LLMResponse(
            call_id=request.call_id,
            success=True,
            content=result["content"],
            latency_ms=latency,
            tokens_used=result.get("tokens_used", 0),
            cost=result.get("cost", 0.0),
            provider_used=provider,
            model_used=result.get("model_used", request.model),
        )

        return response

    async def _filter_hallucination(self, response: LLMResponse, request: LLMRequest) -> LLMResponse:
        """é˜²å¹»è§‰æ£€æµ‹å’Œè¿‡æ»¤"""
        if not request.enable_hallucination_filter or not response.success:
            return response

        try:
            # æ„å»ºæ£€æµ‹ä¸Šä¸‹æ–‡
            context = {
                "call_type": request.call_type.value,
                "business_context": request.business_context,
                "messages": request.messages,
                "historical_accuracy": await self._get_historical_accuracy(request.caller_module),
            }

            # æ‰§è¡Œå¹»è§‰æ£€æµ‹
            detection_result = self.hallucination_filter.detect_hallucination(response.content, context)

            # æ›´æ–°å“åº”
            response.hallucination_score = detection_result["confidence"]
            response.quality_score = 1.0 - detection_result["confidence"]

            # å¦‚æœæ£€æµ‹åˆ°å¹»è§‰
            if detection_result["is_hallucination"]:
                logger.warning(  # pylint: disable=logging-fstring-interpolation
                    f"æ£€æµ‹åˆ°å¹»è§‰: {request.call_id}, ç½®ä¿¡åº¦: {detection_result['confidence']:.3f}"
                )  # pylint: disable=logging-fstring-interpolation

                # è®°å½•å¹»è§‰
                self.call_stats["hallucination_detected"] += 1

                # æ ¹æ®ä¸¥é‡ç¨‹åº¦å¤„ç†
                if detection_result["confidence"] > 0.8:
                    # ä¸¥é‡å¹»è§‰ï¼šæ‹’ç»å“åº”
                    response.success = False
                    response.error_message = f"æ£€æµ‹åˆ°ä¸¥é‡å¹»è§‰ (ç½®ä¿¡åº¦: {detection_result['confidence']:.3f})"
                    response.error_code = "HALLUCINATION_DETECTED"
                elif detection_result["confidence"] > 0.6:
                    # ä¸­ç­‰å¹»è§‰ï¼šæ·»åŠ è­¦å‘Š
                    response.content = f"âš ï¸ è­¦å‘Šï¼šæ­¤å“åº”å¯èƒ½åŒ…å«ä¸å‡†ç¡®ä¿¡æ¯ (å¹»è§‰ç½®ä¿¡åº¦: {detection_result['confidence']:.3f})\n\n{response.content}"  # pylint: disable=line-too-long

                # è®°å½•åˆ°å®¡è®¡æ—¥å¿—
                response.audit_log["hallucination_detection"] = detection_result

            return response

        except Exception as e:  # pylint: disable=broad-exception-caught
            logger.error(f"å¹»è§‰æ£€æµ‹å¤±è´¥: {e}")  # pylint: disable=logging-fstring-interpolation
            # æ£€æµ‹å¤±è´¥æ—¶ï¼Œé™ä½è´¨é‡è¯„åˆ†ä½†ä¸é˜»æ­¢å“åº”
            response.quality_score = 0.5
            return response

    async def _update_memory(self, request: LLMRequest, response: LLMResponse) -> None:
        """æ›´æ–°è®°å¿†ç³»ç»Ÿ"""
        if not request.use_memory or not response.success:
            return

        try:
            # æ„å»ºè®°å¿†æ¡ç›®
            memory_entry = {
                "call_id": request.call_id,
                "call_type": request.call_type.value,
                "caller_module": request.caller_module,
                "caller_function": request.caller_function,
                "request_summary": self._summarize_request(request),
                "response_summary": self._summarize_response(response),
                "business_context": request.business_context,
                "quality_score": response.quality_score,
                "timestamp": datetime.now().isoformat(),
                "success": response.success,
            }

            # æ›´æ–°Engramè®°å¿†
            await self.memory_system.engram_memory.store_memory(
                text=json.dumps(memory_entry),
                context=memory_entry,
                importance=self._calculate_importance(request, response),
            )

            # æ›´æ–°ä¼ ç»Ÿè®°å¿†
            await self.memory_system.add_to_memory(
                memory_type="episodic", content=memory_entry, importance=self._calculate_importance(request, response)
            )

            response.memory_updates = 1
            logger.debug(f"è®°å¿†ç³»ç»Ÿæ›´æ–°å®Œæˆ: {request.call_id}")  # pylint: disable=logging-fstring-interpolation

        except Exception as e:  # pylint: disable=broad-exception-caught
            logger.error(f"è®°å¿†ç³»ç»Ÿæ›´æ–°å¤±è´¥: {e}")  # pylint: disable=logging-fstring-interpolation

    def _calculate_importance(self, request: LLMRequest, response: LLMResponse) -> float:
        """è®¡ç®—è®°å¿†é‡è¦æ€§"""
        importance = 0.5  # åŸºç¡€é‡è¦æ€§

        # æ ¹æ®è°ƒç”¨ç±»å‹è°ƒæ•´
        type_weights = {
            CallType.TRADING_DECISION: 0.9,
            CallType.STRATEGY_ANALYSIS: 0.8,
            CallType.RISK_ASSESSMENT: 0.8,
            CallType.RESEARCH_ANALYSIS: 0.7,
            CallType.FACTOR_GENERATION: 0.7,
            CallType.DATA_ANALYSIS: 0.6,
            CallType.CODE_GENERATION: 0.5,
            CallType.MARKET_SENTIMENT: 0.6,
        }

        importance *= type_weights.get(request.call_type, 0.5)

        # æ ¹æ®è´¨é‡è°ƒæ•´
        importance *= response.quality_score

        # æ ¹æ®æˆåŠŸçŠ¶æ€è°ƒæ•´
        if not response.success:
            importance *= 0.3

        return min(max(importance, 0.0), 1.0)

    def _initialize_llm_clients(self) -> None:
        """åˆå§‹åŒ–LLMå®¢æˆ·ç«¯"""
        # è¿™é‡Œåº”è¯¥åˆå§‹åŒ–å„ç§LLMå®¢æˆ·ç«¯
        # ä¸ºäº†æ¼”ç¤ºï¼Œä½¿ç”¨æ¨¡æ‹Ÿå®¢æˆ·ç«¯
        self.llm_clients = {
            LLMProvider.QWEN_LOCAL: "qwen_local_client",
            LLMProvider.QWEN_CLOUD: "qwen_cloud_client",
            LLMProvider.DEEPSEEK: "deepseek_client",
            LLMProvider.GLM: "glm_client",
        }

        logger.info("LLMå®¢æˆ·ç«¯æ± åˆå§‹åŒ–å®Œæˆ")

    async def _select_best_provider(self, request: LLMRequest) -> LLMProvider:
        """é€‰æ‹©æœ€ä½³LLMæä¾›å•†"""
        # ç®€åŒ–ç‰ˆæœ¬ï¼šç›´æ¥è¿”å›è¯·æ±‚ä¸­æŒ‡å®šçš„æä¾›å•†
        # å®é™…å®ç°åº”è¯¥è€ƒè™‘å¯ç”¨æ€§ã€æˆæœ¬ã€æ€§èƒ½ç­‰å› ç´ 
        return request.provider

    def _estimate_cost(self, request: LLMRequest) -> float:
        """ä¼°ç®—è°ƒç”¨æˆæœ¬"""
        # ç®€åŒ–çš„æˆæœ¬ä¼°ç®—
        token_estimate = len(str(request.messages)) * 1.3  # ä¼°ç®—tokenæ•°

        cost_per_1k_tokens = {
            LLMProvider.QWEN_LOCAL: 0.0,  # æœ¬åœ°å…è´¹
            LLMProvider.QWEN_CLOUD: 0.001,  # Â¥0.001/1K tokens
            LLMProvider.DEEPSEEK: 0.0005,  # Â¥0.0005/1K tokens
            LLMProvider.GLM: 0.0008,  # Â¥0.0008/1K tokens
        }

        rate = cost_per_1k_tokens.get(request.provider, 0.001)
        return (token_estimate / 1000) * rate

    async def _call_qwen_local(self, client, request: LLMRequest) -> Dict[str, Any]:  # pylint: disable=unused-argument
        """è°ƒç”¨æœ¬åœ°Qwenæ¨¡å‹"""
        # æ¨¡æ‹Ÿæœ¬åœ°è°ƒç”¨
        await asyncio.sleep(0.02)  # æ¨¡æ‹Ÿ20mså»¶è¿Ÿ

        return {
            "content": f"[æœ¬åœ°Qwenå“åº”] åŸºäºè¯·æ±‚: {request.messages[-1]['content'][:50]}...",
            "tokens_used": 150,
            "cost": 0.0,
            "model_used": "qwen3-30b-moe-local",
        }

    async def _call_qwen_cloud(self, client, request: LLMRequest) -> Dict[str, Any]:  # pylint: disable=unused-argument
        """è°ƒç”¨äº‘ç«¯Qwenæ¨¡å‹"""
        # æ¨¡æ‹Ÿäº‘ç«¯è°ƒç”¨
        await asyncio.sleep(0.5)  # æ¨¡æ‹Ÿ500mså»¶è¿Ÿ

        return {
            "content": f"[äº‘ç«¯Qwenå“åº”] åŸºäºè¯·æ±‚: {request.messages[-1]['content'][:50]}...",
            "tokens_used": 200,
            "cost": 0.002,
            "model_used": "qwen3-next-80b",
        }

    async def _call_deepseek(self, client, request: LLMRequest) -> Dict[str, Any]:  # pylint: disable=unused-argument
        """è°ƒç”¨DeepSeekæ¨¡å‹"""
        # æ¨¡æ‹ŸDeepSeekè°ƒç”¨
        await asyncio.sleep(0.3)  # æ¨¡æ‹Ÿ300mså»¶è¿Ÿ

        return {
            "content": f"[DeepSeekå“åº”] åŸºäºè¯·æ±‚: {request.messages[-1]['content'][:50]}...",
            "tokens_used": 180,
            "cost": 0.0009,
            "model_used": "deepseek-chat",
        }

    async def _call_glm(self, client, request: LLMRequest) -> Dict[str, Any]:  # pylint: disable=unused-argument
        """è°ƒç”¨GLMæ¨¡å‹"""
        # æ¨¡æ‹ŸGLMè°ƒç”¨
        await asyncio.sleep(0.4)  # æ¨¡æ‹Ÿ400mså»¶è¿Ÿ

        return {
            "content": f"[GLMå“åº”] åŸºäºè¯·æ±‚: {request.messages[-1]['content'][:50]}...",
            "tokens_used": 160,
            "cost": 0.0013,
            "model_used": "glm-4",
        }

    def _summarize_request(self, request: LLMRequest) -> str:
        """æ€»ç»“è¯·æ±‚å†…å®¹"""
        if not request.messages:
            return "ç©ºè¯·æ±‚"

        last_message = request.messages[-1]["content"]
        return last_message[:100] + "..." if len(last_message) > 100 else last_message

    def _summarize_response(self, response: LLMResponse) -> str:
        """æ€»ç»“å“åº”å†…å®¹"""
        if not response.content:
            return "ç©ºå“åº”"

        return response.content[:100] + "..." if len(response.content) > 100 else response.content

    async def _get_historical_accuracy(self, caller_module: str) -> float:
        """è·å–å†å²å‡†ç¡®ç‡"""
        try:
            # ä»Redisè·å–å†å²å‡†ç¡®ç‡
            key = f"llm_accuracy:{caller_module}"
            accuracy = self.redis_client.get(key)
            return float(accuracy) if accuracy else 0.7  # é»˜è®¤70%
        except:  # pylint: disable=w0702
            return 0.7

    async def _record_cost(self, request: LLMRequest, response: LLMResponse) -> None:
        """è®°å½•æˆæœ¬"""
        try:
            await self.cost_tracker.record_cost(
                call_id=request.call_id,
                provider=response.provider_used.value,
                model=response.model_used,
                tokens=response.tokens_used,
                cost=response.cost,
                call_type=request.call_type.value,
            )
        except Exception as e:  # pylint: disable=broad-exception-caught
            logger.error(f"æˆæœ¬è®°å½•å¤±è´¥: {e}")  # pylint: disable=logging-fstring-interpolation

    async def _record_performance(self, request: LLMRequest, response: LLMResponse, latency: float) -> None:
        """è®°å½•æ€§èƒ½æŒ‡æ ‡"""
        try:
            await self.performance_monitor.record_call(
                call_id=request.call_id,
                provider=response.provider_used.value,
                latency_ms=latency,
                success=response.success,
                quality_score=response.quality_score,
                call_type=request.call_type.value,
            )
        except Exception as e:  # pylint: disable=broad-exception-caught
            logger.error(f"æ€§èƒ½è®°å½•å¤±è´¥: {e}")  # pylint: disable=logging-fstring-interpolation

    async def _log_audit(self, request: LLMRequest, response: LLMResponse) -> None:
        """è®°å½•å®¡è®¡æ—¥å¿—"""
        try:
            audit_entry = {
                "call_id": request.call_id,
                "timestamp": datetime.now().isoformat(),
                "caller_module": request.caller_module,
                "caller_function": request.caller_function,
                "call_type": request.call_type.value,
                "provider": response.provider_used.value,
                "model": response.model_used,
                "success": response.success,
                "latency_ms": response.latency_ms,
                "tokens_used": response.tokens_used,
                "cost": response.cost,
                "quality_score": response.quality_score,
                "hallucination_score": response.hallucination_score,
                "request_summary": self._summarize_request(request),
                "response_summary": self._summarize_response(response),
            }

            # å­˜å‚¨åˆ°Redis
            key = f"llm_audit:{datetime.now().strftime('%Y%m%d')}:{request.call_id}"
            self.redis_client.setex(key, 86400 * 7, json.dumps(audit_entry))  # ä¿å­˜7å¤©

        except Exception as e:  # pylint: disable=broad-exception-caught
            logger.error(f"å®¡è®¡æ—¥å¿—è®°å½•å¤±è´¥: {e}")  # pylint: disable=logging-fstring-interpolation

    async def _log_error(
        self, request: LLMRequest, response: LLMResponse, error: Exception  # pylint: disable=unused-argument
    ) -> None:  # pylint: disable=unused-argument
        """è®°å½•é”™è¯¯æ—¥å¿—"""
        try:
            error_entry = {
                "call_id": request.call_id,
                "timestamp": datetime.now().isoformat(),
                "caller_module": request.caller_module,
                "caller_function": request.caller_function,
                "call_type": request.call_type.value,
                "error_type": type(error).__name__,
                "error_message": str(error),
                "request_summary": self._summarize_request(request),
            }

            # å­˜å‚¨åˆ°Redis
            key = f"llm_error:{datetime.now().strftime('%Y%m%d')}:{request.call_id}"
            self.redis_client.setex(key, 86400 * 30, json.dumps(error_entry))  # ä¿å­˜30å¤©

        except Exception as e:  # pylint: disable=broad-exception-caught
            logger.error(f"é”™è¯¯æ—¥å¿—è®°å½•å¤±è´¥: {e}")  # pylint: disable=logging-fstring-interpolation

    def _update_stats(self, response: LLMResponse) -> None:
        """æ›´æ–°ç»Ÿè®¡ä¿¡æ¯"""
        self.call_stats["total_calls"] += 1

        if response.success:
            self.call_stats["successful_calls"] += 1
        else:
            self.call_stats["failed_calls"] += 1

    async def generate_cloud(  # pylint: disable=too-many-positional-arguments
        self,
        prompt: str,
        temperature: float = 0.7,
        max_tokens: int = 2000,
        call_type: CallType = CallType.STRATEGY_ANALYSIS,
        caller_module: str = "unknown",
        caller_function: str = "unknown",
    ) -> str:
        """ç”Ÿæˆäº‘ç«¯å“åº” - å…¼å®¹æ¥å£

        ç™½çš®ä¹¦ä¾æ®: ç¬¬äºŒç«  2.1 AIä¸‰è„‘æ¶æ„ - vLLMé›†æˆ
        éœ€æ±‚: 8.2, 8.8 - vLLMé›†æˆåˆ°AIä¸‰è„‘

        Args:
            prompt: æç¤ºæ–‡æœ¬
            temperature: æ¸©åº¦å‚æ•°
            max_tokens: æœ€å¤§tokenæ•°
            call_type: è°ƒç”¨ç±»å‹
            caller_module: è°ƒç”¨æ¨¡å—
            caller_function: è°ƒç”¨å‡½æ•°

        Returns:
            str: ç”Ÿæˆçš„å“åº”æ–‡æœ¬
        """
        try:
            # æ„å»ºLLMè¯·æ±‚
            request = LLMRequest(
                call_type=call_type,
                provider=LLMProvider.QWEN_CLOUD,  # é»˜è®¤ä½¿ç”¨äº‘ç«¯
                messages=[{"role": "user", "content": prompt}],
                max_tokens=max_tokens,
                temperature=temperature,
                caller_module=caller_module,
                caller_function=caller_function,
            )

            # æ‰§è¡Œè°ƒç”¨
            response = await self.call_llm(request)

            if response.success:  # pylint: disable=no-else-return
                return response.content
            else:
                logger.error(  # pylint: disable=logging-fstring-interpolation
                    f"[LLMGateway] Cloud generation failed: {response.error_message}"
                )  # pylint: disable=logging-fstring-interpolation
                return f"ç”Ÿæˆå¤±è´¥: {response.error_message}"

        except Exception as e:  # pylint: disable=broad-exception-caught
            logger.error(f"[LLMGateway] Cloud generation error: {e}")  # pylint: disable=logging-fstring-interpolation
            return f"ç”Ÿæˆé”™è¯¯: {str(e)}"

    async def generate_local(  # pylint: disable=too-many-positional-arguments
        self,
        prompt: str,
        temperature: float = 0.1,
        max_tokens: int = 1000,
        call_type: CallType = CallType.TRADING_DECISION,
        caller_module: str = "soldier",
        caller_function: str = "decide",
    ) -> str:
        """ç”Ÿæˆæœ¬åœ°å“åº” - vLLMä¼˜åŒ–

        ç™½çš®ä¹¦ä¾æ®: ç¬¬äºŒç«  2.1 AIä¸‰è„‘æ¶æ„ - vLLMæœ¬åœ°æ¨ç†
        éœ€æ±‚: 8.2, 8.8 - vLLMé›†æˆä¼˜åŒ–

        Args:
            prompt: æç¤ºæ–‡æœ¬
            temperature: æ¸©åº¦å‚æ•°
            max_tokens: æœ€å¤§tokenæ•°
            call_type: è°ƒç”¨ç±»å‹
            caller_module: è°ƒç”¨æ¨¡å—
            caller_function: è°ƒç”¨å‡½æ•°

        Returns:
            str: ç”Ÿæˆçš„å“åº”æ–‡æœ¬
        """
        try:
            # æ„å»ºLLMè¯·æ±‚
            request = LLMRequest(
                call_type=call_type,
                provider=LLMProvider.QWEN_LOCAL,  # å¼ºåˆ¶ä½¿ç”¨æœ¬åœ°vLLM
                messages=[{"role": "user", "content": prompt}],
                max_tokens=max_tokens,
                temperature=temperature,
                caller_module=caller_module,
                caller_function=caller_function,
            )

            # æ‰§è¡Œè°ƒç”¨
            response = await self.call_llm(request)

            if response.success:  # pylint: disable=no-else-return
                return response.content
            else:
                logger.error(  # pylint: disable=logging-fstring-interpolation
                    f"[LLMGateway] Local generation failed: {response.error_message}"
                )  # pylint: disable=logging-fstring-interpolation
                return f"ç”Ÿæˆå¤±è´¥: {response.error_message}"

        except Exception as e:  # pylint: disable=broad-exception-caught
            logger.error(f"[LLMGateway] Local generation error: {e}")  # pylint: disable=logging-fstring-interpolation
            return f"ç”Ÿæˆé”™è¯¯: {str(e)}"

    def get_statistics(self) -> Dict[str, Any]:
        """è·å–ç»Ÿè®¡ä¿¡æ¯

        ç™½çš®ä¹¦ä¾æ®: ç¬¬ä¸ƒç«  7.6 LLMè°ƒç”¨ä¼˜åŒ–
        éœ€æ±‚: 7.6 - æ€§èƒ½ç›‘æ§å’Œç»Ÿè®¡

        Returns:
            Dict[str, Any]: ç»Ÿè®¡ä¿¡æ¯å­—å…¸
        """
        stats = {
            **self.call_stats,
            "success_rate": self.call_stats["successful_calls"] / max(self.call_stats["total_calls"], 1),
            "hallucination_rate": self.call_stats["hallucination_detected"] / max(self.call_stats["total_calls"], 1),
            "vllm_usage_rate": self.call_stats["vllm_calls"] / max(self.call_stats["total_calls"], 1),
            "batch_usage_rate": self.call_stats["batch_calls"] / max(self.call_stats["total_calls"], 1),
            "retry_rate": self.call_stats["retries"] / max(self.call_stats["total_calls"], 1),
            "timeout_rate": self.call_stats["timeouts"] / max(self.call_stats["total_calls"], 1),
            "concurrent_limit_rate": self.call_stats["concurrent_limit_hits"] / max(self.call_stats["total_calls"], 1),
            "avg_retries_per_call": self.call_stats["retries"] / max(self.call_stats["total_calls"], 1),
        }

        # æ·»åŠ vLLMå’Œæ‰¹å¤„ç†è°ƒåº¦å™¨ç»Ÿè®¡
        if self.vllm_engine:
            stats["vllm_engine_stats"] = self.vllm_engine.get_stats()

        if self.batch_scheduler:
            stats["batch_scheduler_stats"] = self.batch_scheduler.get_statistics()

        return stats


# è¾…åŠ©ç±»å®šä¹‰
class CostTracker:
    """æˆæœ¬è¿½è¸ªå™¨"""

    def __init__(self, redis_client: redis.Redis):
        self.redis_client = redis_client

    async def record_cost(  # pylint: disable=too-many-positional-arguments
        self, call_id: str, provider: str, model: str, tokens: int, cost: float, call_type: str
    ) -> None:
        """è®°å½•æˆæœ¬"""
        # å®ç°æˆæœ¬è®°å½•é€»è¾‘


class BudgetManager:
    """é¢„ç®—ç®¡ç†å™¨"""

    def __init__(self, daily_budget: float, monthly_budget: float):
        self.daily_budget = daily_budget
        self.monthly_budget = monthly_budget

    async def check_budget(self, estimated_cost: float) -> bool:  # pylint: disable=unused-argument
        """æ£€æŸ¥é¢„ç®—"""
        # å®ç°é¢„ç®—æ£€æŸ¥é€»è¾‘
        return True  # ç®€åŒ–å®ç°


class PerformanceMonitor:
    """æ€§èƒ½ç›‘æ§å™¨"""

    def __init__(self, redis_client: redis.Redis):
        self.redis_client = redis_client

    async def record_call(  # pylint: disable=too-many-positional-arguments
        self, call_id: str, provider: str, latency_ms: float, success: bool, quality_score: float, call_type: str
    ) -> None:
        """è®°å½•è°ƒç”¨æ€§èƒ½"""
        # å®ç°æ€§èƒ½è®°å½•é€»è¾‘
