"""æ€§èƒ½æŠ¥å‘Šç”Ÿæˆå™¨

ç™½çš®ä¹¦ä¾æ®: ç¬¬ä¸€ç« ã€ç¬¬ä¸‰ç«  æ€§èƒ½è¦æ±‚

ç”Ÿæˆç»¼åˆæ€§èƒ½æŠ¥å‘Šï¼ŒåŒ…æ‹¬ï¼š
- æ€§èƒ½åŸºå‡†æµ‹è¯•ç»“æœ
- æ€§èƒ½ç“¶é¢ˆè¯†åˆ«
- æ€§èƒ½ä¼˜åŒ–å»ºè®®
- æ€§èƒ½è¶‹åŠ¿åˆ†æ
"""

import os
import sys
import json
import subprocess
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Any
from loguru import logger


class PerformanceReportGenerator:
    """æ€§èƒ½æŠ¥å‘Šç”Ÿæˆå™¨"""
    
    def __init__(self, output_dir: str = "performance_reports"):
        """åˆå§‹åŒ–æŠ¥å‘Šç”Ÿæˆå™¨
        
        Args:
            output_dir: æŠ¥å‘Šè¾“å‡ºç›®å½•
        """
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(exist_ok=True)
        
        self.timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        self.report_file = self.output_dir / f"performance_report_{self.timestamp}.md"
        
        logger.info(f"Performance report will be saved to: {self.report_file}")
    
    def run_performance_tests(self) -> Dict[str, Any]:
        """è¿è¡Œæ‰€æœ‰æ€§èƒ½æµ‹è¯•
        
        Returns:
            æµ‹è¯•ç»“æœå­—å…¸
        """
        logger.info("Running performance tests...")
        
        results = {}
        
        # è¿è¡Œè°ƒåº¦å™¨æ€§èƒ½æµ‹è¯•
        logger.info("Testing Chronos Scheduler performance...")
        scheduler_result = self._run_pytest("tests/performance/test_scheduler_performance.py")
        results['scheduler'] = scheduler_result
        
        # è¿è¡Œæ•°æ®ç®¡é“æ€§èƒ½æµ‹è¯•
        logger.info("Testing Data Pipeline performance...")
        pipeline_result = self._run_pytest("tests/performance/test_pipeline_performance.py")
        results['pipeline'] = pipeline_result
        
        # è¿è¡ŒSPSCé˜Ÿåˆ—æ€§èƒ½æµ‹è¯•
        logger.info("Testing SPSC Queue performance...")
        spsc_result = self._run_pytest("tests/performance/test_spsc_performance.py")
        results['spsc'] = spsc_result
        
        # è¿è¡Œæ•°æ®æ¸…æ´—å™¨æ€§èƒ½æµ‹è¯•
        logger.info("Testing Data Sanitizer performance...")
        sanitizer_result = self._run_pytest("tests/performance/test_sanitizer_performance.py")
        results['sanitizer'] = sanitizer_result
        
        return results
    
    def _run_pytest(self, test_file: str) -> Dict[str, Any]:
        """è¿è¡Œpytestæµ‹è¯•
        
        Args:
            test_file: æµ‹è¯•æ–‡ä»¶è·¯å¾„
            
        Returns:
            æµ‹è¯•ç»“æœ
        """
        try:
            result = subprocess.run(
                ["pytest", test_file, "-v", "--tb=short"],
                capture_output=True,
                text=True,
                timeout=300
            )
            
            return {
                "success": result.returncode == 0,
                "stdout": result.stdout,
                "stderr": result.stderr,
                "returncode": result.returncode
            }
        except subprocess.TimeoutExpired:
            logger.error(f"Test timeout: {test_file}")
            return {
                "success": False,
                "error": "Test timeout"
            }
        except Exception as e:
            logger.error(f"Test failed: {test_file}, error: {e}")
            return {
                "success": False,
                "error": str(e)
            }
    
    def collect_performance_metrics(self) -> Dict[str, Any]:
        """æ”¶é›†æ€§èƒ½æŒ‡æ ‡
        
        Returns:
            æ€§èƒ½æŒ‡æ ‡å­—å…¸
        """
        metrics = {}
        
        # è¯»å–å„ä¸ªæ¨¡å—çš„æ€§èƒ½æŠ¥å‘Š
        report_files = [
            "performance_report_scheduler.txt",
            "performance_report_pipeline.txt",
            "performance_report_spsc.txt",
            "performance_report_sanitizer.txt"
        ]
        
        for report_file in report_files:
            if os.path.exists(report_file):
                with open(report_file, 'r') as f:
                    content = f.read()
                    module_name = report_file.replace("performance_report_", "").replace(".txt", "")
                    metrics[module_name] = self._parse_report(content)
        
        return metrics
    
    def _parse_report(self, content: str) -> Dict[str, Any]:
        """è§£ææ€§èƒ½æŠ¥å‘Š
        
        Args:
            content: æŠ¥å‘Šå†…å®¹
            
        Returns:
            è§£æåçš„æŒ‡æ ‡
        """
        metrics = {}
        
        lines = content.split('\n')
        current_section = None
        
        for line in lines:
            line = line.strip()
            
            if "Latency Statistics" in line:
                current_section = "latency"
                metrics[current_section] = {}
            elif "Throughput Statistics" in line:
                current_section = "throughput"
                metrics[current_section] = {}
            elif "Memory Usage Statistics" in line:
                current_section = "memory"
                metrics[current_section] = {}
            elif current_section and ":" in line:
                # è§£ææŒ‡æ ‡è¡Œ
                parts = line.split(":")
                if len(parts) == 2:
                    key = parts[0].strip()
                    value_str = parts[1].strip()
                    
                    # æå–æ•°å€¼
                    try:
                        value = float(value_str.split()[0])
                        metrics[current_section][key] = value
                    except (ValueError, IndexError):
                        pass
        
        return metrics
    
    def identify_bottlenecks(self, metrics: Dict[str, Any]) -> List[Dict[str, Any]]:
        """è¯†åˆ«æ€§èƒ½ç“¶é¢ˆ
        
        Args:
            metrics: æ€§èƒ½æŒ‡æ ‡
            
        Returns:
            ç“¶é¢ˆåˆ—è¡¨
        """
        bottlenecks = []
        
        # å®šä¹‰æ€§èƒ½è¦æ±‚
        requirements = {
            "scheduler": {
                "latency_p99": 1.0,  # 1ms
                "throughput_mean": 1000  # 1000 tasks/s
            },
            "pipeline": {
                "latency_p99": 10.0,  # 10ms
                "throughput_mean": 1000000  # 1M records/s
            },
            "spsc": {
                "latency_p99": 100.0,  # 100Î¼s
                "throughput_mean": 10000000  # 10M ops/s
            },
            "sanitizer": {
                "latency_p99": 50.0,  # 50ms for 1000 records
                "throughput_mean": 20000  # 20K records/s
            }
        }
        
        # æ£€æŸ¥æ¯ä¸ªæ¨¡å—
        for module, module_metrics in metrics.items():
            if module not in requirements:
                continue
            
            req = requirements[module]
            
            # æ£€æŸ¥å»¶è¿Ÿ
            if "latency" in module_metrics and "P99" in module_metrics["latency"]:
                p99 = module_metrics["latency"]["P99"]
                if p99 > req["latency_p99"]:
                    bottlenecks.append({
                        "module": module,
                        "metric": "latency_p99",
                        "actual": p99,
                        "required": req["latency_p99"],
                        "severity": "high" if p99 > req["latency_p99"] * 2 else "medium"
                    })
            
            # æ£€æŸ¥ååé‡
            if "throughput" in module_metrics and "Mean" in module_metrics["throughput"]:
                throughput = module_metrics["throughput"]["Mean"]
                if throughput < req["throughput_mean"]:
                    bottlenecks.append({
                        "module": module,
                        "metric": "throughput_mean",
                        "actual": throughput,
                        "required": req["throughput_mean"],
                        "severity": "high" if throughput < req["throughput_mean"] * 0.5 else "medium"
                    })
        
        return bottlenecks
    
    def generate_optimization_suggestions(self, bottlenecks: List[Dict[str, Any]]) -> List[str]:
        """ç”Ÿæˆä¼˜åŒ–å»ºè®®
        
        Args:
            bottlenecks: ç“¶é¢ˆåˆ—è¡¨
            
        Returns:
            ä¼˜åŒ–å»ºè®®åˆ—è¡¨
        """
        suggestions = []
        
        for bottleneck in bottlenecks:
            module = bottleneck["module"]
            metric = bottleneck["metric"]
            
            if module == "scheduler":
                if "latency" in metric:
                    suggestions.append(
                        f"- Scheduler latency optimization:\n"
                        f"  * Reduce lock contention in task queue\n"
                        f"  * Use lock-free data structures\n"
                        f"  * Optimize task sorting algorithm"
                    )
                elif "throughput" in metric:
                    suggestions.append(
                        f"- Scheduler throughput optimization:\n"
                        f"  * Batch task execution\n"
                        f"  * Reduce context switching\n"
                        f"  * Use thread pool for task execution"
                    )
            
            elif module == "pipeline":
                if "latency" in metric:
                    suggestions.append(
                        f"- Pipeline latency optimization:\n"
                        f"  * Reduce data copying\n"
                        f"  * Use zero-copy techniques\n"
                        f"  * Optimize processor chain"
                    )
                elif "throughput" in metric:
                    suggestions.append(
                        f"- Pipeline throughput optimization:\n"
                        f"  * Increase batch size\n"
                        f"  * Use parallel processing\n"
                        f"  * Optimize memory allocation"
                    )
            
            elif module == "spsc":
                if "latency" in metric:
                    suggestions.append(
                        f"- SPSC queue latency optimization:\n"
                        f"  * Use memory barriers correctly\n"
                        f"  * Optimize cache line alignment\n"
                        f"  * Reduce system call overhead"
                    )
                elif "throughput" in metric:
                    suggestions.append(
                        f"- SPSC queue throughput optimization:\n"
                        f"  * Increase queue capacity\n"
                        f"  * Batch read/write operations\n"
                        f"  * Use huge pages for shared memory"
                    )
            
            elif module == "sanitizer":
                if "latency" in metric:
                    suggestions.append(
                        f"- Sanitizer latency optimization:\n"
                        f"  * Vectorize operations with NumPy\n"
                        f"  * Use Pandas optimizations\n"
                        f"  * Cache intermediate results"
                    )
                elif "throughput" in metric:
                    suggestions.append(
                        f"- Sanitizer throughput optimization:\n"
                        f"  * Process data in chunks\n"
                        f"  * Use parallel processing\n"
                        f"  * Optimize layer execution order"
                    )
        
        return suggestions
    
    def generate_report(self) -> None:
        """ç”Ÿæˆç»¼åˆæ€§èƒ½æŠ¥å‘Š"""
        logger.info("Generating comprehensive performance report...")
        
        # è¿è¡Œæ€§èƒ½æµ‹è¯•
        test_results = self.run_performance_tests()
        
        # æ”¶é›†æ€§èƒ½æŒ‡æ ‡
        metrics = self.collect_performance_metrics()
        
        # è¯†åˆ«ç“¶é¢ˆ
        bottlenecks = self.identify_bottlenecks(metrics)
        
        # ç”Ÿæˆä¼˜åŒ–å»ºè®®
        suggestions = self.generate_optimization_suggestions(bottlenecks)
        
        # ç”ŸæˆæŠ¥å‘Š
        report_lines = []
        report_lines.append("# MIA System Performance Report")
        report_lines.append("")
        report_lines.append(f"**Generated**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        report_lines.append(f"**Version**: v1.0")
        report_lines.append("")
        report_lines.append("---")
        report_lines.append("")
        
        # æ‰§è¡Œæ‘˜è¦
        report_lines.append("## Executive Summary")
        report_lines.append("")
        
        total_tests = len(test_results)
        passed_tests = sum(1 for r in test_results.values() if r.get("success", False))
        
        report_lines.append(f"- Total test modules: {total_tests}")
        report_lines.append(f"- Passed modules: {passed_tests}")
        report_lines.append(f"- Failed modules: {total_tests - passed_tests}")
        report_lines.append(f"- Identified bottlenecks: {len(bottlenecks)}")
        report_lines.append("")
        
        # æ€§èƒ½æŒ‡æ ‡
        report_lines.append("## Performance Metrics")
        report_lines.append("")
        
        for module, module_metrics in metrics.items():
            report_lines.append(f"### {module.capitalize()}")
            report_lines.append("")
            
            if "latency" in module_metrics:
                report_lines.append("**Latency:**")
                for key, value in module_metrics["latency"].items():
                    report_lines.append(f"- {key}: {value:.4f}")
                report_lines.append("")
            
            if "throughput" in module_metrics:
                report_lines.append("**Throughput:**")
                for key, value in module_metrics["throughput"].items():
                    report_lines.append(f"- {key}: {value:.2f}")
                report_lines.append("")
            
            if "memory" in module_metrics:
                report_lines.append("**Memory Usage:**")
                for key, value in module_metrics["memory"].items():
                    report_lines.append(f"- {key}: {value:.2f} MB")
                report_lines.append("")
        
        # æ€§èƒ½ç“¶é¢ˆ
        report_lines.append("## Performance Bottlenecks")
        report_lines.append("")
        
        if bottlenecks:
            for bottleneck in bottlenecks:
                severity_emoji = "ğŸ”´" if bottleneck["severity"] == "high" else "ğŸŸ¡"
                report_lines.append(
                    f"{severity_emoji} **{bottleneck['module'].capitalize()}** - {bottleneck['metric']}"
                )
                report_lines.append(f"  - Actual: {bottleneck['actual']:.2f}")
                report_lines.append(f"  - Required: {bottleneck['required']:.2f}")
                report_lines.append(f"  - Severity: {bottleneck['severity']}")
                report_lines.append("")
        else:
            report_lines.append("âœ… No performance bottlenecks detected. All metrics meet requirements.")
            report_lines.append("")
        
        # ä¼˜åŒ–å»ºè®®
        report_lines.append("## Optimization Suggestions")
        report_lines.append("")
        
        if suggestions:
            for suggestion in suggestions:
                report_lines.append(suggestion)
                report_lines.append("")
        else:
            report_lines.append("âœ… No optimization needed. System performance is optimal.")
            report_lines.append("")
        
        # æµ‹è¯•è¯¦æƒ…
        report_lines.append("## Test Details")
        report_lines.append("")
        
        for module, result in test_results.items():
            status = "âœ… PASSED" if result.get("success", False) else "âŒ FAILED"
            report_lines.append(f"### {module.capitalize()} - {status}")
            report_lines.append("")
            
            if not result.get("success", False):
                if "error" in result:
                    report_lines.append(f"**Error**: {result['error']}")
                    report_lines.append("")
        
        # å†™å…¥æŠ¥å‘Š
        with open(self.report_file, 'w', encoding='utf-8') as f:
            f.write('\n'.join(report_lines))
        
        logger.info(f"Performance report generated: {self.report_file}")
        
        # æ‰“å°æ‘˜è¦
        print("\n" + "=" * 60)
        print("Performance Report Summary")
        print("=" * 60)
        print(f"Total test modules: {total_tests}")
        print(f"Passed modules: {passed_tests}")
        print(f"Failed modules: {total_tests - passed_tests}")
        print(f"Identified bottlenecks: {len(bottlenecks)}")
        print(f"\nFull report: {self.report_file}")
        print("=" * 60 + "\n")


def main():
    """ä¸»å‡½æ•°"""
    logger.info("Starting performance report generation...")
    
    generator = PerformanceReportGenerator()
    generator.generate_report()
    
    logger.info("Performance report generation completed")


if __name__ == "__main__":
    main()
