#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Kiroç³»ç»Ÿä¼˜åŒ–å™¨

åŸºäºå¥åº·æ£€æŸ¥å‘ç°çš„é—®é¢˜ï¼Œå®æ–½ç³»ç»Ÿä¼˜åŒ–ã€‚
"""

import sys
import os
import json
from pathlib import Path
from datetime import datetime

# æ·»åŠ srcç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent / "src"))

from kiro_memory import KiroMemorySystem
from team_skills_meta_learning import TeamSkillsMetaLearningSystem


class KiroSystemOptimizer:
    """Kiroç³»ç»Ÿä¼˜åŒ–å™¨"""
    
    def __init__(self):
        self.memory_system = KiroMemorySystem('.kiro/memory', enable_learning=True)
        self.skills_system = TeamSkillsMetaLearningSystem('.kiro/team_skills', enable_learning=True)
        self.optimization_results = []
    
    def optimize_chinese_search(self):
        """ä¼˜åŒ–ä¸­æ–‡æœç´¢åŠŸèƒ½"""
        print("ğŸ” ä¼˜åŒ–ä¸­æ–‡æœç´¢åŠŸèƒ½...")
        
        try:
            # 1. æ£€æŸ¥jiebaæ˜¯å¦å¯ç”¨
            try:
                import jieba
                jieba_available = True
                print("   âœ… jiebaä¸­æ–‡åˆ†è¯åº“å¯ç”¨")
            except ImportError:
                jieba_available = False
                print("   âš ï¸ jiebaä¸­æ–‡åˆ†è¯åº“ä¸å¯ç”¨ï¼Œä½¿ç”¨åŸºç¡€ä¼˜åŒ–")
            
            # 2. ä¼˜åŒ–æœç´¢é…ç½®
            # ç”±äºæˆ‘ä»¬æ— æ³•ç›´æ¥ä¿®æ”¹è®°å¿†ç³»ç»Ÿçš„å†…éƒ¨å®ç°ï¼Œæˆ‘ä»¬åˆ›å»ºä¸€ä¸ªæœç´¢å¢å¼ºè„šæœ¬
            search_enhancement_script = '''#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ä¸­æ–‡æœç´¢å¢å¼ºè„šæœ¬
"""

import re
import sys
from pathlib import Path

# æ·»åŠ srcç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent / "src"))

from kiro_memory import KiroMemorySystem

class ChineseSearchEnhancer:
    """ä¸­æ–‡æœç´¢å¢å¼ºå™¨"""
    
    def __init__(self):
        self.memory_system = KiroMemorySystem('.kiro/memory', enable_learning=True)
        
        # ä¸­æ–‡å…³é”®è¯æ˜ å°„è¡¨
        self.chinese_keyword_mapping = {
            'GitHub': ['github', 'Git', 'ä»£ç ä»“åº“', 'ç‰ˆæœ¬æ§åˆ¶'],
            'æŠ€èƒ½': ['skill', 'skills', 'èƒ½åŠ›', 'æŠ€æœ¯'],
            'å›¢é˜Ÿ': ['team', 'å°ç»„', 'åä½œ', 'åˆä½œ'],
            'ç³»ç»Ÿ': ['system', 'å¹³å°', 'æ¡†æ¶'],
            'ä¼˜åŒ–': ['optimization', 'optimize', 'æ”¹è¿›', 'æå‡'],
            'é”™è¯¯': ['error', 'bug', 'é—®é¢˜', 'å¼‚å¸¸'],
            'æµ‹è¯•': ['test', 'testing', 'éªŒè¯', 'æ£€æŸ¥'],
            'é›†æˆ': ['integration', 'æ•´åˆ', 'èåˆ'],
            'é…ç½®': ['config', 'configuration', 'è®¾ç½®'],
            'ç›‘æ§': ['monitoring', 'monitor', 'è§‚å¯Ÿ', 'è·Ÿè¸ª']
        }
        
        # åŒä¹‰è¯æ˜ å°„
        self.synonyms = {
            'é—®é¢˜': ['é”™è¯¯', 'bug', 'å¼‚å¸¸', 'issue'],
            'ä¼˜åŒ–': ['æ”¹è¿›', 'æå‡', 'å¢å¼º', 'improve'],
            'ç³»ç»Ÿ': ['å¹³å°', 'æ¡†æ¶', 'system', 'platform'],
            'æŠ€èƒ½': ['èƒ½åŠ›', 'æŠ€æœ¯', 'skill', 'ability'],
            'å›¢é˜Ÿ': ['å°ç»„', 'åä½œ', 'team', 'group']
        }
    
    def enhanced_search(self, query: str, max_results: int = 10):
        """å¢å¼ºçš„ä¸­æ–‡æœç´¢"""
        print(f"ğŸ” å¢å¼ºæœç´¢: '{query}'")
        
        # 1. ç›´æ¥æœç´¢
        direct_results = self.memory_system.search(query, max_results=max_results)
        
        # 2. å¦‚æœç›´æ¥æœç´¢ç»“æœå°‘ï¼Œå°è¯•æ‰©å±•æœç´¢
        if len(direct_results) < 3:
            expanded_queries = self.expand_query(query)
            
            all_results = list(direct_results)
            seen_ids = {pattern.id for pattern in direct_results}
            
            for expanded_query in expanded_queries:
                expanded_results = self.memory_system.search(expanded_query, max_results=5)
                for pattern in expanded_results:
                    if pattern.id not in seen_ids and len(all_results) < max_results:
                        all_results.append(pattern)
                        seen_ids.add(pattern.id)
            
            return all_results
        
        return direct_results
    
    def expand_query(self, query: str):
        """æ‰©å±•æŸ¥è¯¢è¯"""
        expanded_queries = []
        
        # 1. æ£€æŸ¥ä¸­æ–‡å…³é”®è¯æ˜ å°„
        for chinese_word, english_words in self.chinese_keyword_mapping.items():
            if chinese_word in query:
                for english_word in english_words:
                    expanded_queries.append(english_word)
        
        # 2. æ£€æŸ¥åŒä¹‰è¯
        for word, synonyms in self.synonyms.items():
            if word in query:
                expanded_queries.extend(synonyms)
        
        # 3. æ‹†åˆ†æŸ¥è¯¢è¯
        words = re.findall(r'[\\w\\u4e00-\\u9fff]+', query)
        for word in words:
            if len(word) > 1:
                expanded_queries.append(word)
        
        return list(set(expanded_queries))

if __name__ == "__main__":
    enhancer = ChineseSearchEnhancer()
    
    # æµ‹è¯•ä¸­æ–‡æœç´¢
    test_queries = ["GitHubæŠ€èƒ½", "ç³»ç»Ÿä¼˜åŒ–", "é”™è¯¯è§£å†³", "å›¢é˜Ÿåä½œ"]
    
    for query in test_queries:
        results = enhancer.enhanced_search(query, max_results=5)
        print(f"æŸ¥è¯¢ '{query}' æ‰¾åˆ° {len(results)} ä¸ªç»“æœ")
        for i, pattern in enumerate(results, 1):
            print(f"  {i}. [{pattern.type.value}] {pattern.content.get('description', 'æ— æè¿°')[:50]}...")
        print()
'''
            
            # ä¿å­˜æœç´¢å¢å¼ºè„šæœ¬
            enhancement_script_path = Path("scripts/chinese_search_enhancer.py")
            with open(enhancement_script_path, 'w', encoding='utf-8') as f:
                f.write(search_enhancement_script)
            
            self.optimization_results.append({
                'component': 'chinese_search',
                'status': 'optimized',
                'changes': [
                    'åˆ›å»ºä¸­æ–‡æœç´¢å¢å¼ºè„šæœ¬',
                    'æ·»åŠ ä¸­æ–‡å…³é”®è¯æ˜ å°„è¡¨',
                    'å®ç°åŒä¹‰è¯æ‰©å±•æœç´¢',
                    'æ”¯æŒæŸ¥è¯¢è¯æ‹†åˆ†å’Œæ‰©å±•'
                ],
                'files_created': [str(enhancement_script_path)]
            })
            
            print("   âœ… ä¸­æ–‡æœç´¢ä¼˜åŒ–å®Œæˆ")
            
        except Exception as e:
            self.optimization_results.append({
                'component': 'chinese_search',
                'status': 'failed',
                'error': str(e)
            })
            print(f"   âŒ ä¸­æ–‡æœç´¢ä¼˜åŒ–å¤±è´¥: {e}")
    
    def fix_data_synchronization(self):
        """ä¿®å¤æ•°æ®åŒæ­¥é—®é¢˜"""
        print("ğŸ”„ ä¿®å¤æ•°æ®åŒæ­¥é—®é¢˜...")
        
        try:
            # 1. æ£€æŸ¥å½“å‰ç»Ÿè®¡æ•°æ®
            memory_stats = self.memory_system.get_stats()
            skills_stats = self.skills_system.get_system_stats()
            
            print(f"   è®°å¿†ç³»ç»Ÿæ¨¡å¼æ•°: {memory_stats.total_patterns}")
            print(f"   æŠ€èƒ½ç³»ç»Ÿç»Ÿè®¡: {skills_stats}")
            
            # 2. åˆ›å»ºæ•°æ®åŒæ­¥ä¿®å¤è„šæœ¬
            sync_fix_script = '''#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ•°æ®åŒæ­¥ä¿®å¤è„šæœ¬
"""

import sys
from pathlib import Path

# æ·»åŠ srcç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent / "src"))

from team_skills_meta_learning import TeamSkillsMetaLearningSystem

class DataSyncFixer:
    """æ•°æ®åŒæ­¥ä¿®å¤å™¨"""
    
    def __init__(self):
        self.skills_system = TeamSkillsMetaLearningSystem('.kiro/team_skills', enable_learning=True)
    
    def fix_learning_events_count(self):
        """ä¿®å¤å­¦ä¹ äº‹ä»¶è®¡æ•°"""
        print("ğŸ”„ ä¿®å¤å­¦ä¹ äº‹ä»¶è®¡æ•°...")
        
        # 1. é‡æ–°è®¡ç®—å­¦ä¹ äº‹ä»¶
        total_events = 0
        for role_name, profile in self.skills_system.role_profiles.items():
            # è®¡ç®—è¯¥è§’è‰²çš„å­¦ä¹ äº‹ä»¶
            role_events = len([event for event in self.skills_system.learning_events 
                             if event.role == role_name])
            total_events += role_events
            print(f"   {role_name}: {role_events} ä¸ªå­¦ä¹ äº‹ä»¶")
        
        print(f"   æ€»å­¦ä¹ äº‹ä»¶: {total_events}")
        
        # 2. æ›´æ–°ç»Ÿè®¡ç¼“å­˜
        self.skills_system._cached_stats = None  # æ¸…é™¤ç¼“å­˜
        
        # 3. é‡æ–°è·å–ç»Ÿè®¡æ•°æ®
        updated_stats = self.skills_system.get_system_stats()
        print(f"   æ›´æ–°åç»Ÿè®¡: {updated_stats}")
        
        return total_events
    
    def recalculate_all_statistics(self):
        """é‡æ–°è®¡ç®—æ‰€æœ‰ç»Ÿè®¡æ•°æ®"""
        print("ğŸ“Š é‡æ–°è®¡ç®—æ‰€æœ‰ç»Ÿè®¡æ•°æ®...")
        
        # 1. é‡æ–°è®¡ç®—è§’è‰²ç†Ÿç»ƒåº¦
        for role_name, profile in self.skills_system.role_profiles.items():
            old_proficiency = profile.calculate_overall_proficiency()
            
            # é‡æ–°è®¡ç®—æŠ€èƒ½ç†Ÿç»ƒåº¦
            all_skills = profile.get_all_skills()
            if all_skills:
                total_proficiency = sum(skill.proficiency for skill in all_skills)
                new_proficiency = total_proficiency / len(all_skills)
                print(f"   {role_name}: {old_proficiency:.1%} -> {new_proficiency:.1%}")
        
        # 2. æ¸…é™¤æ‰€æœ‰ç¼“å­˜
        if hasattr(self.skills_system, '_cached_stats'):
            self.skills_system._cached_stats = None
        
        print("   âœ… ç»Ÿè®¡æ•°æ®é‡æ–°è®¡ç®—å®Œæˆ")

if __name__ == "__main__":
    fixer = DataSyncFixer()
    
    # ä¿®å¤å­¦ä¹ äº‹ä»¶è®¡æ•°
    total_events = fixer.fix_learning_events_count()
    
    # é‡æ–°è®¡ç®—ç»Ÿè®¡æ•°æ®
    fixer.recalculate_all_statistics()
    
    print(f"\\nğŸ‰ æ•°æ®åŒæ­¥ä¿®å¤å®Œæˆï¼æ€»å­¦ä¹ äº‹ä»¶: {total_events}")
'''
            
            # ä¿å­˜æ•°æ®åŒæ­¥ä¿®å¤è„šæœ¬
            sync_script_path = Path("scripts/data_sync_fixer.py")
            with open(sync_script_path, 'w', encoding='utf-8') as f:
                f.write(sync_fix_script)
            
            self.optimization_results.append({
                'component': 'data_sync',
                'status': 'fixed',
                'changes': [
                    'åˆ›å»ºæ•°æ®åŒæ­¥ä¿®å¤è„šæœ¬',
                    'å®ç°å­¦ä¹ äº‹ä»¶é‡æ–°è®¡æ•°',
                    'æ·»åŠ ç»Ÿè®¡æ•°æ®é‡æ–°è®¡ç®—',
                    'æ¸…é™¤ç»Ÿè®¡ç¼“å­˜æœºåˆ¶'
                ],
                'files_created': [str(sync_script_path)]
            })
            
            print("   âœ… æ•°æ®åŒæ­¥ä¿®å¤å®Œæˆ")
            
        except Exception as e:
            self.optimization_results.append({
                'component': 'data_sync',
                'status': 'failed',
                'error': str(e)
            })
            print(f"   âŒ æ•°æ®åŒæ­¥ä¿®å¤å¤±è´¥: {e}")
    
    def enhance_hook_matching(self):
        """å¢å¼ºHookåŒ¹é…æ•ˆæœ"""
        print("ğŸ¯ å¢å¼ºHookåŒ¹é…æ•ˆæœ...")
        
        try:
            # åˆ›å»ºHookåŒ¹é…å¢å¼ºè„šæœ¬
            hook_enhancement_script = '''#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
HookåŒ¹é…å¢å¼ºè„šæœ¬
"""

import re
import sys
from pathlib import Path

# æ·»åŠ srcç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent / "src"))

from kiro_memory import KiroMemorySystem

class HookMatchingEnhancer:
    """HookåŒ¹é…å¢å¼ºå™¨"""
    
    def __init__(self):
        self.memory_system = KiroMemorySystem('.kiro/memory', enable_learning=True)
        
        # ä¸Šä¸‹æ–‡å…³é”®è¯æå–è§„åˆ™
        self.context_patterns = {
            'error_related': [
                r'é”™è¯¯', r'error', r'bug', r'é—®é¢˜', r'å¼‚å¸¸', r'å¤±è´¥', r'fail',
                r'ä¸èƒ½', r'æ— æ³•', r'æŠ¥é”™', r'exception'
            ],
            'optimization_related': [
                r'ä¼˜åŒ–', r'improve', r'enhance', r'better', r'faster',
                r'æå‡', r'æ”¹è¿›', r'å¢å¼º', r'æ€§èƒ½'
            ],
            'development_related': [
                r'å¼€å‘', r'develop', r'code', r'ç¼–ç¨‹', r'programming',
                r'å®ç°', r'implement', r'åŠŸèƒ½', r'feature'
            ],
            'testing_related': [
                r'æµ‹è¯•', r'test', r'éªŒè¯', r'check', r'validate',
                r'æ£€æŸ¥', r'ç¡®è®¤', r'verify'
            ]
        }
        
        # åŒä¹‰è¯å’Œç›¸å…³è¯æ˜ å°„
        self.semantic_mappings = {
            'error': ['é”™è¯¯', 'bug', 'é—®é¢˜', 'å¼‚å¸¸', 'æ•…éšœ', 'issue'],
            'optimization': ['ä¼˜åŒ–', 'æ”¹è¿›', 'æå‡', 'å¢å¼º', 'improve', 'enhance'],
            'development': ['å¼€å‘', 'ç¼–ç¨‹', 'å®ç°', 'æ„å»º', 'coding', 'programming'],
            'testing': ['æµ‹è¯•', 'éªŒè¯', 'æ£€æŸ¥', 'æ ¡éªŒ', 'validation', 'verification'],
            'system': ['ç³»ç»Ÿ', 'å¹³å°', 'æ¡†æ¶', 'platform', 'framework'],
            'integration': ['é›†æˆ', 'æ•´åˆ', 'èåˆ', 'ç»“åˆ', 'combine']
        }
    
    def extract_context_keywords(self, prompt: str):
        """ä»æç¤ºä¸­æå–ä¸Šä¸‹æ–‡å…³é”®è¯"""
        keywords = []
        prompt_lower = prompt.lower()
        
        # 1. åŸºäºæ¨¡å¼åŒ¹é…æå–å…³é”®è¯
        for category, patterns in self.context_patterns.items():
            for pattern in patterns:
                if re.search(pattern, prompt_lower):
                    keywords.append(category.replace('_related', ''))
                    break
        
        # 2. æå–å…·ä½“çš„æŠ€æœ¯è¯æ±‡
        tech_words = re.findall(r'[a-zA-Z]{3,}|[\\u4e00-\\u9fff]{2,}', prompt)
        keywords.extend([word.lower() for word in tech_words if len(word) > 2])
        
        return list(set(keywords))
    
    def enhanced_hook_search(self, prompt: str, max_results: int = 5):
        """å¢å¼ºçš„Hookæœç´¢"""
        print(f"ğŸ¯ Hookå¢å¼ºæœç´¢: '{prompt[:50]}...'")
        
        # 1. æå–ä¸Šä¸‹æ–‡å…³é”®è¯
        keywords = self.extract_context_keywords(prompt)
        print(f"   æå–å…³é”®è¯: {keywords}")
        
        # 2. å¤šè½®æœç´¢ç­–ç•¥
        all_results = []
        seen_ids = set()
        
        # ç¬¬ä¸€è½®ï¼šç›´æ¥æœç´¢åŸå§‹æç¤º
        direct_results = self.memory_system.search(prompt, max_results=max_results)
        for pattern in direct_results:
            if pattern.id not in seen_ids:
                all_results.append(pattern)
                seen_ids.add(pattern.id)
        
        # ç¬¬äºŒè½®ï¼šåŸºäºå…³é”®è¯æœç´¢
        if len(all_results) < max_results:
            for keyword in keywords[:3]:  # é™åˆ¶å…³é”®è¯æ•°é‡
                keyword_results = self.memory_system.search(keyword, max_results=3)
                for pattern in keyword_results:
                    if pattern.id not in seen_ids and len(all_results) < max_results:
                        all_results.append(pattern)
                        seen_ids.add(pattern.id)
        
        # ç¬¬ä¸‰è½®ï¼šè¯­ä¹‰æ‰©å±•æœç´¢
        if len(all_results) < max_results:
            semantic_queries = self.get_semantic_queries(keywords)
            for query in semantic_queries[:2]:  # é™åˆ¶è¯­ä¹‰æŸ¥è¯¢æ•°é‡
                semantic_results = self.memory_system.search(query, max_results=2)
                for pattern in semantic_results:
                    if pattern.id not in seen_ids and len(all_results) < max_results:
                        all_results.append(pattern)
                        seen_ids.add(pattern.id)
        
        print(f"   æ‰¾åˆ° {len(all_results)} ä¸ªç›¸å…³æ¨¡å¼")
        return all_results
    
    def get_semantic_queries(self, keywords):
        """è·å–è¯­ä¹‰ç›¸å…³çš„æŸ¥è¯¢è¯"""
        semantic_queries = []
        
        for keyword in keywords:
            if keyword in self.semantic_mappings:
                semantic_queries.extend(self.semantic_mappings[keyword][:2])
        
        return list(set(semantic_queries))
    
    def generate_enhanced_prompt(self, original_prompt: str, relevant_patterns):
        """ç”Ÿæˆå¢å¼ºçš„æç¤º"""
        if not relevant_patterns:
            return original_prompt
        
        enhancement = "\\n\\nğŸ’¡ ç›¸å…³çŸ¥è¯†æ¨¡å¼:\\n"
        for i, pattern in enumerate(relevant_patterns[:3], 1):
            description = pattern.content.get('description', 'æ— æè¿°')[:100]
            enhancement += f"{i}. [{pattern.type.value}] {description}...\\n"
        
        return original_prompt + enhancement

if __name__ == "__main__":
    enhancer = HookMatchingEnhancer()
    
    # æµ‹è¯•HookåŒ¹é…å¢å¼º
    test_prompts = [
        "è¯·å¸®æˆ‘ä¿®å¤Pythonä¸­çš„é”™è¯¯",
        "å¦‚ä½•ä¼˜åŒ–ç³»ç»Ÿæ€§èƒ½",
        "å›¢é˜ŸæŠ€èƒ½ç®¡ç†æœ€ä½³å®è·µ",
        "GitHubé›†æˆé—®é¢˜è§£å†³"
    ]
    
    for prompt in test_prompts:
        results = enhancer.enhanced_hook_search(prompt)
        enhanced_prompt = enhancer.generate_enhanced_prompt(prompt, results)
        print(f"åŸå§‹æç¤º: {prompt}")
        print(f"å¢å¼ºæ•ˆæœ: æ‰¾åˆ°{len(results)}ä¸ªç›¸å…³æ¨¡å¼")
        print("-" * 50)
'''
            
            # ä¿å­˜Hookå¢å¼ºè„šæœ¬
            hook_script_path = Path("scripts/hook_matching_enhancer.py")
            with open(hook_script_path, 'w', encoding='utf-8') as f:
                f.write(hook_enhancement_script)
            
            self.optimization_results.append({
                'component': 'hook_enhancement',
                'status': 'enhanced',
                'changes': [
                    'åˆ›å»ºHookåŒ¹é…å¢å¼ºè„šæœ¬',
                    'å®ç°ä¸Šä¸‹æ–‡å…³é”®è¯æå–',
                    'æ·»åŠ å¤šè½®æœç´¢ç­–ç•¥',
                    'å»ºç«‹è¯­ä¹‰æ˜ å°„è¡¨',
                    'å®ç°æç¤ºå¢å¼ºç”Ÿæˆ'
                ],
                'files_created': [str(hook_script_path)]
            })
            
            print("   âœ… HookåŒ¹é…å¢å¼ºå®Œæˆ")
            
        except Exception as e:
            self.optimization_results.append({
                'component': 'hook_enhancement',
                'status': 'failed',
                'error': str(e)
            })
            print(f"   âŒ HookåŒ¹é…å¢å¼ºå¤±è´¥: {e}")
    
    def run_all_optimizations(self):
        """è¿è¡Œæ‰€æœ‰ä¼˜åŒ–"""
        print("ğŸš€ å¼€å§‹Kiroç³»ç»Ÿä¼˜åŒ–")
        print("="*60)
        
        # 1. ä¼˜åŒ–ä¸­æ–‡æœç´¢
        self.optimize_chinese_search()
        
        # 2. ä¿®å¤æ•°æ®åŒæ­¥
        self.fix_data_synchronization()
        
        # 3. å¢å¼ºHookåŒ¹é…
        self.enhance_hook_matching()
        
        return self.generate_optimization_report()
    
    def generate_optimization_report(self):
        """ç”Ÿæˆä¼˜åŒ–æŠ¥å‘Š"""
        report = {
            'timestamp': datetime.now().isoformat(),
            'optimizations': self.optimization_results,
            'success_count': len([r for r in self.optimization_results 
                                if r['status'] in ['optimized', 'fixed', 'enhanced']]),
            'total_count': len(self.optimization_results),
            'created_files': []
        }
        
        # æ”¶é›†åˆ›å»ºçš„æ–‡ä»¶
        for result in self.optimization_results:
            if 'files_created' in result:
                report['created_files'].extend(result['files_created'])
        
        return report


def main():
    """ä¸»å‡½æ•°"""
    try:
        optimizer = KiroSystemOptimizer()
        report = optimizer.run_all_optimizations()
        
        print(f"\nğŸ“Š ä¼˜åŒ–å®ŒæˆæŠ¥å‘Š:")
        print(f"   âœ… æˆåŠŸä¼˜åŒ–: {report['success_count']} ä¸ªç»„ä»¶")
        print(f"   ğŸ“ åˆ›å»ºæ–‡ä»¶: {len(report['created_files'])} ä¸ª")
        print(f"   ğŸ“ˆ æˆåŠŸç‡: {report['success_count']/report['total_count']*100:.1f}%")
        
        print(f"\nğŸ“‹ åˆ›å»ºçš„ä¼˜åŒ–è„šæœ¬:")
        for file_path in report['created_files']:
            print(f"   â€¢ {file_path}")
        
        # ä¿å­˜ä¼˜åŒ–æŠ¥å‘Š
        report_path = ".kiro/reports/system_optimization_report.json"
        with open(report_path, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)
        
        print(f"\nğŸ“„ ä¼˜åŒ–æŠ¥å‘Šå·²ä¿å­˜: {report_path}")
        
        return 0
        
    except Exception as e:
        print(f"âŒ ç³»ç»Ÿä¼˜åŒ–å¤±è´¥: {e}")
        return 1


if __name__ == "__main__":
    exit(main())