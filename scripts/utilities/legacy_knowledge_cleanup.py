#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ—§ç‰ˆæœ¬çŸ¥è¯†ç§¯ç´¯å¼•æ“æ¸…ç†è¯„ä¼°
æ™ºèƒ½åˆ†æå’Œæ¸…ç†å†—ä½™çš„çŸ¥è¯†ç§¯ç´¯è„šæœ¬
"""

import json
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Set

class LegacyKnowledgeCleanup:
    def __init__(self):
        self.analysis_date = datetime.now()
        self.scripts_dir = Path("scripts/utilities")
        
    def analyze_legacy_scripts(self) -> Dict:
        """åˆ†ææ—§ç‰ˆæœ¬çŸ¥è¯†ç§¯ç´¯è„šæœ¬"""
        
        # è¯†åˆ«çŸ¥è¯†ç§¯ç´¯ç›¸å…³è„šæœ¬
        knowledge_scripts = []
        for script_file in self.scripts_dir.glob("*.py"):
            if self._is_knowledge_script(script_file):
                analysis = self._analyze_script(script_file)
                knowledge_scripts.append(analysis)
        
        # åˆ†ç±»è„šæœ¬
        categorized_scripts = self._categorize_scripts(knowledge_scripts)
        
        # ç”Ÿæˆæ¸…ç†å»ºè®®
        cleanup_recommendations = self._generate_cleanup_recommendations(categorized_scripts)
        
        return {
            "analysis_metadata": {
                "analysis_date": self.analysis_date.isoformat(),
                "total_scripts_analyzed": len(knowledge_scripts),
                "scripts_directory": str(self.scripts_dir)
            },
            "script_categories": categorized_scripts,
            "cleanup_recommendations": cleanup_recommendations,
            "new_system_status": {
                "background_accumulator": "å·²åˆ›å»º - ç°ä»£åŒ–åå°å¼•æ“",
                "integrated_support": "å·²åˆ›å»º - å®Œæ•´é›†æˆç³»ç»Ÿ",
                "quick_extract": "å·²åˆ›å»º - é«˜æ•ˆçŸ¥è¯†æå–"
            }
        }
    
    def _is_knowledge_script(self, script_file: Path) -> bool:
        """åˆ¤æ–­æ˜¯å¦ä¸ºçŸ¥è¯†ç§¯ç´¯ç›¸å…³è„šæœ¬"""
        knowledge_keywords = [
            "extract", "knowledge", "accumulation", "finalize",
            "bilingual", "task_execution", "comprehensive"
        ]
        
        filename = script_file.name.lower()
        return any(keyword in filename for keyword in knowledge_keywords)
    
    def _analyze_script(self, script_file: Path) -> Dict:
        """åˆ†æå•ä¸ªè„šæœ¬"""
        try:
            content = script_file.read_text(encoding='utf-8')
            
            return {
                "filename": script_file.name,
                "path": str(script_file),
                "size_kb": script_file.stat().st_size / 1024,
                "line_count": len(content.splitlines()),
                "last_modified": datetime.fromtimestamp(script_file.stat().st_mtime).isoformat(),
                "has_main_function": "def main(" in content,
                "has_mcp_integration": "mcp_memory" in content,
                "complexity_score": self._calculate_complexity(content)
            }
        except Exception as e:
            return {
                "filename": script_file.name,
                "path": str(script_file),
                "error": str(e),
                "analysis_failed": True
            }
    
    def _calculate_complexity(self, content: str) -> int:
        """è®¡ç®—è„šæœ¬å¤æ‚åº¦"""
        complexity = 0
        complexity += content.count("def ") * 2  # å‡½æ•°å®šä¹‰
        complexity += content.count("class ") * 5  # ç±»å®šä¹‰
        complexity += content.count("if ") * 1  # æ¡ä»¶è¯­å¥
        complexity += content.count("for ") * 2  # å¾ªç¯
        complexity += content.count("try:") * 3  # å¼‚å¸¸å¤„ç†
        return complexity
    
    def _categorize_scripts(self, scripts: List[Dict]) -> Dict:
        """åˆ†ç±»è„šæœ¬"""
        categories = {
            "legacy_extractors": [],  # æ—§ç‰ˆæœ¬æå–å™¨
            "finalization_scripts": [],  # æœ€ç»ˆåŒ–è„šæœ¬
            "bilingual_processors": [],  # åŒè¯­å¤„ç†è„šæœ¬
            "task_specific_extractors": [],  # ç‰¹å®šä»»åŠ¡æå–å™¨
            "utility_scripts": []  # å·¥å…·è„šæœ¬
        }
        
        for script in scripts:
            filename = script["filename"].lower()
            
            if "finalize" in filename:
                categories["finalization_scripts"].append(script)
            elif "bilingual" in filename:
                categories["bilingual_processors"].append(script)
            elif "extract" in filename and any(word in filename for word in ["task", "comprehensive", "git", "cross"]):
                categories["task_specific_extractors"].append(script)
            elif "extract" in filename:
                categories["legacy_extractors"].append(script)
            else:
                categories["utility_scripts"].append(script)
        
        return categories
    
    def _generate_cleanup_recommendations(self, categorized_scripts: Dict) -> Dict:
        """ç”Ÿæˆæ¸…ç†å»ºè®®"""
        
        recommendations = {
            "immediate_cleanup": [],  # ç«‹å³æ¸…ç†
            "conditional_cleanup": [],  # æ¡ä»¶æ¸…ç†
            "keep_for_reference": [],  # ä¿ç•™å‚è€ƒ
            "archive_candidates": []  # å½’æ¡£å€™é€‰
        }
        
        # åˆ†ææ¯ä¸ªç±»åˆ«
        for category, scripts in categorized_scripts.items():
            if category == "finalization_scripts":
                # æœ€ç»ˆåŒ–è„šæœ¬é€šå¸¸å¯ä»¥æ¸…ç†
                for script in scripts:
                    recommendations["immediate_cleanup"].append({
                        "script": script["filename"],
                        "reason": "æœ€ç»ˆåŒ–è„šæœ¬ï¼ŒåŠŸèƒ½å·²è¢«æ–°ç³»ç»Ÿæ›¿ä»£",
                        "action": "åˆ é™¤"
                    })
            
            elif category == "legacy_extractors":
                # æ—§ç‰ˆæœ¬æå–å™¨éœ€è¦è¯„ä¼°
                for script in scripts:
                    if script.get("complexity_score", 0) > 50:
                        recommendations["archive_candidates"].append({
                            "script": script["filename"],
                            "reason": "å¤æ‚åº¦è¾ƒé«˜ï¼Œå¯èƒ½åŒ…å«æœ‰ä»·å€¼é€»è¾‘",
                            "action": "å½’æ¡£åˆ°archiveç›®å½•"
                        })
                    else:
                        recommendations["immediate_cleanup"].append({
                            "script": script["filename"],
                            "reason": "ç®€å•æå–å™¨ï¼Œå·²è¢«æ–°ç³»ç»Ÿæ›¿ä»£",
                            "action": "åˆ é™¤"
                        })
            
            elif category == "task_specific_extractors":
                # ç‰¹å®šä»»åŠ¡æå–å™¨æ¡ä»¶æ¸…ç†
                for script in scripts:
                    recommendations["conditional_cleanup"].append({
                        "script": script["filename"],
                        "reason": "ç‰¹å®šä»»åŠ¡æå–å™¨ï¼Œå¦‚æœä»»åŠ¡å·²å®Œæˆå¯æ¸…ç†",
                        "action": "æ£€æŸ¥ä»»åŠ¡çŠ¶æ€åå†³å®š"
                    })
            
            elif category == "bilingual_processors":
                # åŒè¯­å¤„ç†è„šæœ¬ä¿ç•™å‚è€ƒ
                for script in scripts:
                    recommendations["keep_for_reference"].append({
                        "script": script["filename"],
                        "reason": "åŒè¯­å¤„ç†é€»è¾‘å¯èƒ½åœ¨æœªæ¥æœ‰ç”¨",
                        "action": "ä¿ç•™ä½†ç§»è‡³å‚è€ƒç›®å½•"
                    })
        
        return recommendations
    
    def execute_cleanup(self, recommendations: Dict, dry_run: bool = True) -> Dict:
        """æ‰§è¡Œæ¸…ç†æ“ä½œ"""
        
        cleanup_results = {
            "dry_run": dry_run,
            "execution_date": datetime.now().isoformat(),
            "actions_taken": [],
            "space_saved_kb": 0,
            "files_processed": 0
        }
        
        # åˆ›å»ºå½’æ¡£ç›®å½•
        archive_dir = Path("archive/legacy_knowledge_scripts")
        if not dry_run:
            archive_dir.mkdir(parents=True, exist_ok=True)
        
        # æ‰§è¡Œç«‹å³æ¸…ç†
        for item in recommendations["immediate_cleanup"]:
            script_path = self.scripts_dir / item["script"]
            if script_path.exists():
                size_kb = script_path.stat().st_size / 1024
                
                if not dry_run:
                    script_path.unlink()
                
                cleanup_results["actions_taken"].append({
                    "action": "åˆ é™¤",
                    "file": item["script"],
                    "reason": item["reason"],
                    "size_kb": size_kb
                })
                cleanup_results["space_saved_kb"] += size_kb
                cleanup_results["files_processed"] += 1
        
        # æ‰§è¡Œå½’æ¡£
        for item in recommendations["archive_candidates"]:
            script_path = self.scripts_dir / item["script"]
            if script_path.exists():
                archive_path = archive_dir / item["script"]
                size_kb = script_path.stat().st_size / 1024
                
                if not dry_run:
                    script_path.rename(archive_path)
                
                cleanup_results["actions_taken"].append({
                    "action": "å½’æ¡£",
                    "file": item["script"],
                    "reason": item["reason"],
                    "archive_location": str(archive_path),
                    "size_kb": size_kb
                })
                cleanup_results["files_processed"] += 1
        
        return cleanup_results
    
    def generate_cleanup_report(self, analysis: Dict, cleanup_results: Dict = None) -> Dict:
        """ç”Ÿæˆæ¸…ç†æŠ¥å‘Š"""
        
        report = {
            "report_metadata": {
                "report_type": "æ—§ç‰ˆæœ¬çŸ¥è¯†ç§¯ç´¯å¼•æ“æ¸…ç†æŠ¥å‘Š",
                "generated_by": "ğŸ§  Knowledge Engineer - ç³»ç»Ÿæ¸…ç†ä¸“å®¶",
                "generation_date": self.analysis_date.isoformat(),
                "analysis_scope": "scripts/utilitiesç›®å½•ä¸‹çš„çŸ¥è¯†ç§¯ç´¯ç›¸å…³è„šæœ¬"
            },
            "analysis_summary": {
                "total_scripts_found": analysis["analysis_metadata"]["total_scripts_analyzed"],
                "categories_identified": len(analysis["script_categories"]),
                "cleanup_recommendations": len(analysis["cleanup_recommendations"]["immediate_cleanup"]),
                "archive_candidates": len(analysis["cleanup_recommendations"]["archive_candidates"])
            },
            "new_system_advantages": {
                "background_accumulator": "é›¶å¹²æ‰°çš„åå°çŸ¥è¯†ç§¯ç´¯",
                "integrated_support": "å®Œæ•´çš„å¼€å‘æ”¯æŒç”Ÿæ€ç³»ç»Ÿ",
                "quick_extract": "é«˜æ•ˆçš„å¿«é€ŸçŸ¥è¯†æå–",
                "system_integration": "ä¸MCPè®°å¿†ç³»ç»Ÿæ·±åº¦é›†æˆ"
            },
            "cleanup_benefits": {
                "code_maintainability": "å‡å°‘ä»£ç åº“å¤æ‚åº¦",
                "performance_improvement": "é™ä½ç³»ç»Ÿèµ„æºå ç”¨",
                "developer_experience": "ç®€åŒ–å¼€å‘ç¯å¢ƒ",
                "focus_enhancement": "ä¸“æ³¨äºæ ¸å¿ƒåŠŸèƒ½"
            },
            "recommendations": analysis["cleanup_recommendations"]
        }
        
        if cleanup_results:
            report["cleanup_execution"] = cleanup_results
        
        return report

def main():
    """ä¸»å‡½æ•°"""
    print("å¼€å§‹æ—§ç‰ˆæœ¬çŸ¥è¯†ç§¯ç´¯å¼•æ“æ¸…ç†è¯„ä¼°...")
    
    cleanup_analyzer = LegacyKnowledgeCleanup()
    
    # åˆ†ææ—§ç‰ˆæœ¬è„šæœ¬
    analysis = cleanup_analyzer.analyze_legacy_scripts()
    
    # æ‰§è¡Œæ¸…ç†ï¼ˆå¹²è¿è¡Œï¼‰
    cleanup_results = cleanup_analyzer.execute_cleanup(
        analysis["cleanup_recommendations"], 
        dry_run=True
    )
    
    # ç”ŸæˆæŠ¥å‘Š
    report = cleanup_analyzer.generate_cleanup_report(analysis, cleanup_results)
    
    # ä¿å­˜æŠ¥å‘Š
    report_path = Path(".kiro/reports/legacy_knowledge_cleanup_report.json")
    report_path.parent.mkdir(parents=True, exist_ok=True)
    
    with open(report_path, 'w', encoding='utf-8') as f:
        json.dump(report, f, ensure_ascii=False, indent=2)
    
    # è¾“å‡ºæ‘˜è¦
    print(f"åˆ†æå®Œæˆï¼å‘ç° {analysis['analysis_metadata']['total_scripts_analyzed']} ä¸ªçŸ¥è¯†ç§¯ç´¯ç›¸å…³è„šæœ¬")
    print(f"å»ºè®®ç«‹å³æ¸…ç†: {len(analysis['cleanup_recommendations']['immediate_cleanup'])} ä¸ª")
    print(f"å»ºè®®å½’æ¡£: {len(analysis['cleanup_recommendations']['archive_candidates'])} ä¸ª")
    print(f"é¢„è®¡èŠ‚çœç©ºé—´: {cleanup_results['space_saved_kb']:.1f} KB")
    print(f"è¯¦ç»†æŠ¥å‘Š: {report_path}")
    
    return {
        "analysis": analysis,
        "cleanup_results": cleanup_results,
        "report": report,
        "report_path": str(report_path)
    }

if __name__ == "__main__":
    result = main()
    print("æ¸…ç†è¯„ä¼°å®Œæˆï¼")