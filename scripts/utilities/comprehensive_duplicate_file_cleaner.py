#!/usr/bin/env python3
"""
å…¨é¢é‡å¤æ–‡ä»¶æ¸…ç†å™¨

ä½œä¸ºğŸ” Code Review Specialistï¼Œæˆ‘è´Ÿè´£å¯¹æ•´ä¸ªä»£ç åº“è¿›è¡Œå…¨é¢æ£€æŸ¥ï¼Œ
è¯†åˆ«å¹¶åˆ é™¤é‡å¤æ–‡ä»¶ï¼Œä¼˜åŒ–åº“ç»“æ„ï¼Œæå‡ä»£ç è´¨é‡ã€‚
"""

import os
import hashlib
import json
import shutil
from datetime import datetime
from pathlib import Path
from collections import defaultdict
from typing import Dict, List, Set, Tuple

class ComprehensiveDuplicateCleaner:
    """å…¨é¢é‡å¤æ–‡ä»¶æ¸…ç†å™¨"""
    
    def __init__(self):
        self.base_path = Path(".")
        self.file_hashes = defaultdict(list)
        self.duplicate_groups = []
        self.cleanup_log = []
        self.stats = {
            "total_files_scanned": 0,
            "duplicate_files_found": 0,
            "files_deleted": 0,
            "space_saved": 0
        }
        
        # æ’é™¤çš„ç›®å½•å’Œæ–‡ä»¶æ¨¡å¼
        self.exclude_dirs = {
            ".git", "__pycache__", ".pytest_cache", "node_modules",
            ".hypothesis", "htmlcov", "htmlcov_ai_brain_coordinator", 
            "htmlcov_single_test", ".kiro/memory", "backup_20260203_135753"
        }
        
        self.exclude_patterns = {
            ".pyc", ".pyo", ".pyd", ".so", ".dll", ".dylib",
            ".DS_Store", "Thumbs.db", ".gitkeep"
        }
        
    def calculate_file_hash(self, file_path: Path) -> str:
        """è®¡ç®—æ–‡ä»¶çš„MD5å“ˆå¸Œå€¼"""
        try:
            hash_md5 = hashlib.md5()
            with open(file_path, "rb") as f:
                for chunk in iter(lambda: f.read(4096), b""):
                    hash_md5.update(chunk)
            return hash_md5.hexdigest()
        except (IOError, OSError) as e:
            print(f"âš ï¸ æ— æ³•è¯»å–æ–‡ä»¶ {file_path}: {e}")
            return ""
    
    def should_exclude_file(self, file_path: Path) -> bool:
        """åˆ¤æ–­æ˜¯å¦åº”è¯¥æ’é™¤æ–‡ä»¶"""
        # æ£€æŸ¥ç›®å½•æ’é™¤
        for part in file_path.parts:
            if part in self.exclude_dirs:
                return True
        
        # æ£€æŸ¥æ–‡ä»¶æ‰©å±•åæ’é™¤
        if file_path.suffix in self.exclude_patterns:
            return True
            
        # æ£€æŸ¥æ–‡ä»¶åæ’é™¤
        if file_path.name in self.exclude_patterns:
            return True
            
        return False
    
    def scan_for_duplicates(self):
        """æ‰«æé‡å¤æ–‡ä»¶"""
        print("ğŸ” å¼€å§‹æ‰«æé‡å¤æ–‡ä»¶...")
        
        for file_path in self.base_path.rglob("*"):
            if not file_path.is_file():
                continue
                
            if self.should_exclude_file(file_path):
                continue
            
            self.stats["total_files_scanned"] += 1
            
            # è®¡ç®—æ–‡ä»¶å“ˆå¸Œ
            file_hash = self.calculate_file_hash(file_path)
            if file_hash:
                self.file_hashes[file_hash].append(file_path)
        
        # è¯†åˆ«é‡å¤æ–‡ä»¶ç»„
        for file_hash, file_list in self.file_hashes.items():
            if len(file_list) > 1:
                self.duplicate_groups.append({
                    "hash": file_hash,
                    "files": file_list,
                    "size": file_list[0].stat().st_size if file_list[0].exists() else 0
                })
                self.stats["duplicate_files_found"] += len(file_list) - 1
        
        print(f"ğŸ“Š æ‰«æå®Œæˆ: {self.stats['total_files_scanned']} ä¸ªæ–‡ä»¶")
        print(f"ğŸ” å‘ç° {len(self.duplicate_groups)} ç»„é‡å¤æ–‡ä»¶")
        print(f"ğŸ“ é‡å¤æ–‡ä»¶æ€»æ•°: {self.stats['duplicate_files_found']} ä¸ª")
    
    def analyze_duplicates(self):
        """åˆ†æé‡å¤æ–‡ä»¶"""
        print("ğŸ“Š åˆ†æé‡å¤æ–‡ä»¶...")
        
        analysis = {
            "by_extension": defaultdict(int),
            "by_size": defaultdict(int),
            "by_directory": defaultdict(int),
            "large_duplicates": [],
            "critical_duplicates": []
        }
        
        for group in self.duplicate_groups:
            files = group["files"]
            size = group["size"]
            
            # æŒ‰æ‰©å±•ååˆ†æ
            for file_path in files:
                ext = file_path.suffix or "æ— æ‰©å±•å"
                analysis["by_extension"][ext] += 1
            
            # æŒ‰å¤§å°åˆ†æ
            if size > 1024 * 1024:  # > 1MB
                analysis["large_duplicates"].append(group)
            
            # æŒ‰ç›®å½•åˆ†æ
            for file_path in files:
                dir_name = str(file_path.parent)
                analysis["by_directory"][dir_name] += 1
            
            # è¯†åˆ«å…³é”®é‡å¤æ–‡ä»¶
            critical_extensions = {".py", ".js", ".ts", ".json", ".yaml", ".yml"}
            if any(f.suffix in critical_extensions for f in files):
                analysis["critical_duplicates"].append(group)
        
        return analysis
    
    def prioritize_file_for_keeping(self, files: List[Path]) -> Path:
        """ä¼˜å…ˆé€‰æ‹©è¦ä¿ç•™çš„æ–‡ä»¶"""
        # ä¼˜å…ˆçº§è§„åˆ™ï¼š
        # 1. åœ¨ä¸»è¦ç›®å½•ä¸­çš„æ–‡ä»¶ï¼ˆsrc/, scripts/, tests/ï¼‰
        # 2. è·¯å¾„è¾ƒçŸ­çš„æ–‡ä»¶
        # 3. æœ€è¿‘ä¿®æ”¹çš„æ–‡ä»¶
        
        priority_dirs = ["src", "scripts", "tests", "docs", "config"]
        
        def get_priority_score(file_path: Path) -> Tuple[int, int, float]:
            # ç›®å½•ä¼˜å…ˆçº§åˆ†æ•°
            dir_score = 0
            for i, priority_dir in enumerate(priority_dirs):
                if priority_dir in str(file_path):
                    dir_score = len(priority_dirs) - i
                    break
            
            # è·¯å¾„é•¿åº¦åˆ†æ•°ï¼ˆè¶ŠçŸ­è¶Šå¥½ï¼‰
            path_score = -len(str(file_path))
            
            # ä¿®æ”¹æ—¶é—´åˆ†æ•°
            try:
                mtime_score = file_path.stat().st_mtime
            except OSError:
                mtime_score = 0
            
            return (dir_score, path_score, mtime_score)
        
        # æŒ‰ä¼˜å…ˆçº§æ’åºï¼Œè¿”å›æœ€é«˜ä¼˜å…ˆçº§çš„æ–‡ä»¶
        return max(files, key=get_priority_score)
    
    def create_backup_before_deletion(self, files_to_delete: List[Path]):
        """åˆ é™¤å‰åˆ›å»ºå¤‡ä»½"""
        if not files_to_delete:
            return
        
        backup_dir = Path(".kiro/backups/duplicate_cleanup")
        backup_dir.mkdir(parents=True, exist_ok=True)
        
        backup_info = {
            "backup_date": datetime.now().isoformat(),
            "files": []
        }
        
        for file_path in files_to_delete:
            if file_path.exists():
                # åˆ›å»ºç›¸å¯¹è·¯å¾„çš„å¤‡ä»½
                relative_path = file_path.relative_to(self.base_path)
                backup_path = backup_dir / relative_path
                backup_path.parent.mkdir(parents=True, exist_ok=True)
                
                try:
                    shutil.copy2(file_path, backup_path)
                    backup_info["files"].append({
                        "original": str(file_path),
                        "backup": str(backup_path),
                        "size": file_path.stat().st_size
                    })
                except Exception as e:
                    print(f"âš ï¸ å¤‡ä»½æ–‡ä»¶å¤±è´¥ {file_path}: {e}")
        
        # ä¿å­˜å¤‡ä»½ä¿¡æ¯
        backup_info_path = backup_dir / "backup_info.json"
        with open(backup_info_path, "w", encoding="utf-8") as f:
            json.dump(backup_info, f, indent=2, ensure_ascii=False)
        
        print(f"ğŸ’¾ å·²å¤‡ä»½ {len(backup_info['files'])} ä¸ªæ–‡ä»¶åˆ° {backup_dir}")
    
    def clean_duplicates(self, interactive: bool = True):
        """æ¸…ç†é‡å¤æ–‡ä»¶"""
        print("ğŸ§¹ å¼€å§‹æ¸…ç†é‡å¤æ–‡ä»¶...")
        
        files_to_delete = []
        
        for i, group in enumerate(self.duplicate_groups):
            files = group["files"]
            size = group["size"]
            
            print(f"\nğŸ“ é‡å¤ç»„ {i+1}/{len(self.duplicate_groups)}:")
            print(f"   æ–‡ä»¶å¤§å°: {size:,} å­—èŠ‚")
            
            # æ˜¾ç¤ºæ‰€æœ‰é‡å¤æ–‡ä»¶
            for j, file_path in enumerate(files):
                print(f"   {j+1}. {file_path}")
            
            # é€‰æ‹©è¦ä¿ç•™çš„æ–‡ä»¶
            keep_file = self.prioritize_file_for_keeping(files)
            files_to_remove = [f for f in files if f != keep_file]
            
            print(f"   âœ… ä¿ç•™: {keep_file}")
            print(f"   ğŸ—‘ï¸ åˆ é™¤: {len(files_to_remove)} ä¸ªæ–‡ä»¶")
            
            if interactive:
                response = input("   ç¡®è®¤åˆ é™¤è¿™äº›é‡å¤æ–‡ä»¶? (y/N): ").strip().lower()
                if response != 'y':
                    print("   â­ï¸ è·³è¿‡æ­¤ç»„")
                    continue
            
            files_to_delete.extend(files_to_remove)
            
            # è®°å½•æ“ä½œ
            self.cleanup_log.append({
                "group_id": i + 1,
                "kept_file": str(keep_file),
                "deleted_files": [str(f) for f in files_to_remove],
                "size_saved": size * len(files_to_remove)
            })
        
        # åˆ›å»ºå¤‡ä»½
        if files_to_delete:
            self.create_backup_before_deletion(files_to_delete)
        
        # æ‰§è¡Œåˆ é™¤
        deleted_count = 0
        space_saved = 0
        
        for file_path in files_to_delete:
            try:
                if file_path.exists():
                    file_size = file_path.stat().st_size
                    file_path.unlink()
                    deleted_count += 1
                    space_saved += file_size
                    print(f"ğŸ—‘ï¸ å·²åˆ é™¤: {file_path}")
            except Exception as e:
                print(f"âŒ åˆ é™¤å¤±è´¥ {file_path}: {e}")
        
        self.stats["files_deleted"] = deleted_count
        self.stats["space_saved"] = space_saved
        
        print(f"\nâœ… æ¸…ç†å®Œæˆ:")
        print(f"   åˆ é™¤æ–‡ä»¶: {deleted_count} ä¸ª")
        print(f"   èŠ‚çœç©ºé—´: {space_saved:,} å­—èŠ‚ ({space_saved/1024/1024:.2f} MB)")
    
    def identify_other_cleanup_opportunities(self):
        """è¯†åˆ«å…¶ä»–æ¸…ç†æœºä¼š"""
        print("ğŸ” è¯†åˆ«å…¶ä»–æ¸…ç†æœºä¼š...")
        
        opportunities = {
            "empty_directories": [],
            "large_files": [],
            "old_backup_files": [],
            "temporary_files": [],
            "unused_test_files": []
        }
        
        for file_path in self.base_path.rglob("*"):
            if self.should_exclude_file(file_path):
                continue
            
            if file_path.is_dir():
                # æ£€æŸ¥ç©ºç›®å½•
                try:
                    if not any(file_path.iterdir()):
                        opportunities["empty_directories"].append(file_path)
                except OSError:
                    pass
            elif file_path.is_file():
                try:
                    stat = file_path.stat()
                    
                    # æ£€æŸ¥å¤§æ–‡ä»¶ (>10MB)
                    if stat.st_size > 10 * 1024 * 1024:
                        opportunities["large_files"].append({
                            "path": file_path,
                            "size": stat.st_size
                        })
                    
                    # æ£€æŸ¥å¤‡ä»½æ–‡ä»¶
                    if any(pattern in file_path.name.lower() 
                           for pattern in ["backup", "bak", ".old", ".orig", "~"]):
                        opportunities["old_backup_files"].append(file_path)
                    
                    # æ£€æŸ¥ä¸´æ—¶æ–‡ä»¶
                    if any(pattern in file_path.name.lower() 
                           for pattern in ["temp", "tmp", ".cache"]):
                        opportunities["temporary_files"].append(file_path)
                    
                    # æ£€æŸ¥å¯èƒ½æœªä½¿ç”¨çš„æµ‹è¯•æ–‡ä»¶
                    if (file_path.name.startswith("test_") and 
                        file_path.suffix == ".py" and
                        stat.st_size < 1024):  # å°äº1KBçš„æµ‹è¯•æ–‡ä»¶å¯èƒ½æ˜¯ç©ºçš„
                        opportunities["unused_test_files"].append(file_path)
                        
                except OSError:
                    pass
        
        return opportunities
    
    def generate_cleanup_report(self, analysis, opportunities):
        """ç”Ÿæˆæ¸…ç†æŠ¥å‘Š"""
        print("ğŸ“Š ç”Ÿæˆæ¸…ç†æŠ¥å‘Š...")
        
        report = {
            "metadata": {
                "cleanup_date": datetime.now().isoformat(),
                "cleaner": "ğŸ” Code Review Specialist",
                "total_files_scanned": self.stats["total_files_scanned"]
            },
            "duplicate_analysis": {
                "duplicate_groups_found": len(self.duplicate_groups),
                "duplicate_files_found": self.stats["duplicate_files_found"],
                "files_deleted": self.stats["files_deleted"],
                "space_saved_bytes": self.stats["space_saved"],
                "space_saved_mb": round(self.stats["space_saved"] / 1024 / 1024, 2)
            },
            "duplicate_breakdown": {
                "by_extension": dict(analysis["by_extension"]),
                "by_directory": dict(analysis["by_directory"]),
                "large_duplicates_count": len(analysis["large_duplicates"]),
                "critical_duplicates_count": len(analysis["critical_duplicates"])
            },
            "cleanup_opportunities": {
                "empty_directories": len(opportunities["empty_directories"]),
                "large_files": len(opportunities["large_files"]),
                "old_backup_files": len(opportunities["old_backup_files"]),
                "temporary_files": len(opportunities["temporary_files"]),
                "unused_test_files": len(opportunities["unused_test_files"])
            },
            "cleanup_log": self.cleanup_log,
            "recommendations": [
                "å®šæœŸè¿è¡Œé‡å¤æ–‡ä»¶æ£€æŸ¥",
                "å»ºç«‹æ–‡ä»¶å‘½åè§„èŒƒé¿å…é‡å¤",
                "ä½¿ç”¨ç‰ˆæœ¬æ§åˆ¶ç³»ç»Ÿç®¡ç†æ–‡ä»¶å†å²",
                "å®šæœŸæ¸…ç†ä¸´æ—¶æ–‡ä»¶å’Œå¤‡ä»½æ–‡ä»¶",
                "ç›‘æ§å¤§æ–‡ä»¶çš„å¢é•¿"
            ],
            "quality_improvements": {
                "code_organization": "åˆ é™¤é‡å¤æ–‡ä»¶æå‡äº†ä»£ç ç»„ç»‡æ€§",
                "storage_efficiency": f"èŠ‚çœäº† {self.stats['space_saved']} å­—èŠ‚å­˜å‚¨ç©ºé—´",
                "maintenance_burden": "å‡å°‘äº†ç»´æŠ¤è´Ÿæ‹…å’Œæ··æ·†é£é™©",
                "build_performance": "å¯èƒ½æå‡æ„å»ºå’Œæœç´¢æ€§èƒ½"
            }
        }
        
        # ä¿å­˜æŠ¥å‘Š
        report_path = Path(".kiro/reports/comprehensive_duplicate_cleanup_report.json")
        report_path.parent.mkdir(exist_ok=True)
        
        with open(report_path, "w", encoding="utf-8") as f:
            json.dump(report, f, indent=2, ensure_ascii=False)
        
        print(f"âœ… æ¸…ç†æŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_path}")
        return report
    
    def execute_comprehensive_cleanup(self, interactive: bool = False):
        """æ‰§è¡Œå…¨é¢æ¸…ç†"""
        print("ğŸ§¹ å¼€å§‹æ‰§è¡Œå…¨é¢é‡å¤æ–‡ä»¶æ¸…ç†...")
        print("=" * 60)
        
        try:
            # 1. æ‰«æé‡å¤æ–‡ä»¶
            self.scan_for_duplicates()
            
            # 2. åˆ†æé‡å¤æ–‡ä»¶
            analysis = self.analyze_duplicates()
            
            # 3. æ¸…ç†é‡å¤æ–‡ä»¶
            if self.duplicate_groups:
                self.clean_duplicates(interactive=interactive)
            else:
                print("âœ… æœªå‘ç°é‡å¤æ–‡ä»¶")
            
            # 4. è¯†åˆ«å…¶ä»–æ¸…ç†æœºä¼š
            opportunities = self.identify_other_cleanup_opportunities()
            
            # 5. ç”ŸæˆæŠ¥å‘Š
            report = self.generate_cleanup_report(analysis, opportunities)
            
            print("=" * 60)
            print("ğŸ‰ å…¨é¢æ¸…ç†å®Œæˆ!")
            print(f"ğŸ“Š æ‰«ææ–‡ä»¶: {self.stats['total_files_scanned']} ä¸ª")
            print(f"ğŸ” é‡å¤æ–‡ä»¶ç»„: {len(self.duplicate_groups)} ç»„")
            print(f"ğŸ—‘ï¸ åˆ é™¤æ–‡ä»¶: {self.stats['files_deleted']} ä¸ª")
            print(f"ğŸ’¾ èŠ‚çœç©ºé—´: {self.stats['space_saved']:,} å­—èŠ‚")
            
            # æ˜¾ç¤ºå…¶ä»–æ¸…ç†æœºä¼š
            if any(opportunities.values()):
                print("\nğŸ” å‘ç°å…¶ä»–æ¸…ç†æœºä¼š:")
                for category, items in opportunities.items():
                    if items:
                        print(f"   {category}: {len(items)} ä¸ª")
            
            return True
            
        except Exception as e:
            print(f"âŒ æ¸…ç†è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {str(e)}")
            return False

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ” å…¨é¢é‡å¤æ–‡ä»¶æ¸…ç†å™¨")
    print("ä½œä¸ºCode Review Specialistï¼Œæˆ‘å°†å¯¹æ•´ä¸ªä»£ç åº“è¿›è¡Œæ¸…ç†")
    print()
    
    cleaner = ComprehensiveDuplicateCleaner()
    
    # è¯¢é—®æ˜¯å¦äº¤äº’æ¨¡å¼
    interactive = input("æ˜¯å¦ä½¿ç”¨äº¤äº’æ¨¡å¼ç¡®è®¤æ¯ä¸ªåˆ é™¤æ“ä½œ? (y/N): ").strip().lower() == 'y'
    
    success = cleaner.execute_comprehensive_cleanup(interactive=interactive)
    
    if success:
        print("\nğŸ¯ æ¸…ç†æˆåŠŸå®Œæˆ!")
        print("ğŸ“š ä»£ç åº“ç°åœ¨æ›´åŠ æ•´æ´å’Œé«˜æ•ˆ")
        print("ğŸ’¡ å»ºè®®å®šæœŸè¿è¡Œæ­¤æ¸…ç†å·¥å…·")
    else:
        print("\nâš ï¸ æ¸…ç†è¿‡ç¨‹ä¸­é‡åˆ°é—®é¢˜")

if __name__ == "__main__":
    main()