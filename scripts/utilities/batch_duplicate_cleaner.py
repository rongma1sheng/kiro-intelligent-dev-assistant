#!/usr/bin/env python3
"""
æ‰¹é‡é‡å¤æ–‡ä»¶æ¸…ç†å™¨ - éäº¤äº’æ¨¡å¼

ä½œä¸ºğŸ” Code Review Specialistï¼Œæˆ‘å°†è‡ªåŠ¨åˆ é™¤æ‰€æœ‰é‡å¤æ–‡ä»¶ï¼Œ
æ— éœ€ç”¨æˆ·é€ä¸ªç¡®è®¤ï¼Œç„¶åéªŒè¯3.0ç‰ˆæœ¬çš„å®Œæ•´æ€§ã€‚
"""

import os
import hashlib
import json
import shutil
from datetime import datetime
from pathlib import Path
from collections import defaultdict
from typing import Dict, List, Set, Tuple

class BatchDuplicateCleaner:
    """æ‰¹é‡é‡å¤æ–‡ä»¶æ¸…ç†å™¨"""
    
    def __init__(self):
        self.base_path = Path(".")
        self.file_hashes = defaultdict(list)
        self.duplicate_groups = []
        self.cleanup_log = []
        self.stats = {
            "total_files_scanned": 0,
            "duplicate_files_found": 0,
            "files_deleted": 0,
            "space_saved": 0
        }
        
        # æ’é™¤çš„ç›®å½•å’Œæ–‡ä»¶æ¨¡å¼
        self.exclude_dirs = {
            ".git", "__pycache__", ".pytest_cache", "node_modules",
            ".hypothesis", "htmlcov", "htmlcov_ai_brain_coordinator", 
            "htmlcov_single_test", ".kiro/memory"
        }
        
        self.exclude_patterns = {
            ".pyc", ".pyo", ".pyd", ".so", ".dll", ".dylib",
            ".DS_Store", "Thumbs.db", ".gitkeep"
        }
        
    def calculate_file_hash(self, file_path: Path) -> str:
        """è®¡ç®—æ–‡ä»¶çš„MD5å“ˆå¸Œå€¼"""
        try:
            hash_md5 = hashlib.md5()
            with open(file_path, "rb") as f:
                for chunk in iter(lambda: f.read(4096), b""):
                    hash_md5.update(chunk)
            return hash_md5.hexdigest()
        except (IOError, OSError):
            return ""
    
    def should_exclude_file(self, file_path: Path) -> bool:
        """åˆ¤æ–­æ˜¯å¦åº”è¯¥æ’é™¤æ–‡ä»¶"""
        # æ£€æŸ¥ç›®å½•æ’é™¤
        for part in file_path.parts:
            if part in self.exclude_dirs:
                return True
        
        # æ£€æŸ¥æ–‡ä»¶æ‰©å±•åæ’é™¤
        if file_path.suffix in self.exclude_patterns:
            return True
            
        # æ£€æŸ¥æ–‡ä»¶åæ’é™¤
        if file_path.name in self.exclude_patterns:
            return True
            
        return False
    
    def scan_for_duplicates(self):
        """æ‰«æé‡å¤æ–‡ä»¶"""
        print("ğŸ” æ‰«æé‡å¤æ–‡ä»¶...")
        
        for file_path in self.base_path.rglob("*"):
            if not file_path.is_file():
                continue
                
            if self.should_exclude_file(file_path):
                continue
            
            self.stats["total_files_scanned"] += 1
            
            # è®¡ç®—æ–‡ä»¶å“ˆå¸Œ
            file_hash = self.calculate_file_hash(file_path)
            if file_hash:
                self.file_hashes[file_hash].append(file_path)
        
        # è¯†åˆ«é‡å¤æ–‡ä»¶ç»„
        for file_hash, file_list in self.file_hashes.items():
            if len(file_list) > 1:
                self.duplicate_groups.append({
                    "hash": file_hash,
                    "files": file_list,
                    "size": file_list[0].stat().st_size if file_list[0].exists() else 0
                })
                self.stats["duplicate_files_found"] += len(file_list) - 1
        
        print(f"ğŸ“Š æ‰«æå®Œæˆ: {self.stats['total_files_scanned']} ä¸ªæ–‡ä»¶")
        print(f"ğŸ” å‘ç° {len(self.duplicate_groups)} ç»„é‡å¤æ–‡ä»¶")
        print(f"ğŸ“ é‡å¤æ–‡ä»¶æ€»æ•°: {self.stats['duplicate_files_found']} ä¸ª")
    
    def prioritize_file_for_keeping(self, files: List[Path]) -> Path:
        """ä¼˜å…ˆé€‰æ‹©è¦ä¿ç•™çš„æ–‡ä»¶"""
        # ä¼˜å…ˆçº§è§„åˆ™ï¼š
        # 1. åœ¨ä¸»è¦ç›®å½•ä¸­çš„æ–‡ä»¶ï¼ˆsrc/, scripts/, tests/, 3.0/ï¼‰
        # 2. è·¯å¾„è¾ƒçŸ­çš„æ–‡ä»¶
        # 3. æœ€è¿‘ä¿®æ”¹çš„æ–‡ä»¶
        
        priority_dirs = ["3.0", "src", "scripts", "tests", "docs", "config"]
        
        def get_priority_score(file_path: Path) -> Tuple[int, int, float]:
            # ç›®å½•ä¼˜å…ˆçº§åˆ†æ•°
            dir_score = 0
            for i, priority_dir in enumerate(priority_dirs):
                if priority_dir in str(file_path):
                    dir_score = len(priority_dirs) - i
                    break
            
            # è·¯å¾„é•¿åº¦åˆ†æ•°ï¼ˆè¶ŠçŸ­è¶Šå¥½ï¼‰
            path_score = -len(str(file_path))
            
            # ä¿®æ”¹æ—¶é—´åˆ†æ•°
            try:
                mtime_score = file_path.stat().st_mtime
            except OSError:
                mtime_score = 0
            
            return (dir_score, path_score, mtime_score)
        
        # æŒ‰ä¼˜å…ˆçº§æ’åºï¼Œè¿”å›æœ€é«˜ä¼˜å…ˆçº§çš„æ–‡ä»¶
        return max(files, key=get_priority_score)
    
    def batch_clean_duplicates(self):
        """æ‰¹é‡æ¸…ç†é‡å¤æ–‡ä»¶"""
        print("ğŸ§¹ æ‰¹é‡æ¸…ç†é‡å¤æ–‡ä»¶...")
        
        files_to_delete = []
        total_space_to_save = 0
        
        # æ”¶é›†æ‰€æœ‰è¦åˆ é™¤çš„æ–‡ä»¶
        for i, group in enumerate(self.duplicate_groups):
            files = group["files"]
            size = group["size"]
            
            # é€‰æ‹©è¦ä¿ç•™çš„æ–‡ä»¶
            keep_file = self.prioritize_file_for_keeping(files)
            files_to_remove = [f for f in files if f != keep_file]
            
            files_to_delete.extend(files_to_remove)
            total_space_to_save += size * len(files_to_remove)
            
            # è®°å½•æ“ä½œ
            self.cleanup_log.append({
                "group_id": i + 1,
                "kept_file": str(keep_file),
                "deleted_files": [str(f) for f in files_to_remove],
                "size_saved": size * len(files_to_remove)
            })
        
        print(f"ğŸ“‹ å‡†å¤‡åˆ é™¤ {len(files_to_delete)} ä¸ªé‡å¤æ–‡ä»¶")
        print(f"ğŸ’¾ é¢„è®¡èŠ‚çœç©ºé—´: {total_space_to_save:,} å­—èŠ‚ ({total_space_to_save/1024/1024:.2f} MB)")
        
        # åˆ›å»ºå¤‡ä»½ç›®å½•
        backup_dir = Path(".kiro/backups/duplicate_cleanup")
        backup_dir.mkdir(parents=True, exist_ok=True)
        
        # æ‰§è¡Œåˆ é™¤
        deleted_count = 0
        space_saved = 0
        
        for file_path in files_to_delete:
            try:
                if file_path.exists():
                    file_size = file_path.stat().st_size
                    file_path.unlink()
                    deleted_count += 1
                    space_saved += file_size
                    if deleted_count % 50 == 0:  # æ¯50ä¸ªæ–‡ä»¶æ˜¾ç¤ºä¸€æ¬¡è¿›åº¦
                        print(f"ğŸ—‘ï¸ å·²åˆ é™¤ {deleted_count} ä¸ªæ–‡ä»¶...")
            except Exception as e:
                print(f"âŒ åˆ é™¤å¤±è´¥ {file_path}: {e}")
        
        self.stats["files_deleted"] = deleted_count
        self.stats["space_saved"] = space_saved
        
        print(f"âœ… æ‰¹é‡æ¸…ç†å®Œæˆ:")
        print(f"   åˆ é™¤æ–‡ä»¶: {deleted_count} ä¸ª")
        print(f"   èŠ‚çœç©ºé—´: {space_saved:,} å­—èŠ‚ ({space_saved/1024/1024:.2f} MB)")
    
    def verify_version_3_integrity(self):
        """éªŒè¯3.0ç‰ˆæœ¬çš„å®Œæ•´æ€§"""
        print("ğŸ” éªŒè¯3.0ç‰ˆæœ¬å®Œæ•´æ€§...")
        
        version_3_path = Path("3.0")
        if not version_3_path.exists():
            print("âŒ 3.0ç›®å½•ä¸å­˜åœ¨!")
            return False
        
        # æ£€æŸ¥å¿…éœ€çš„å¹³å°ç›®å½•å’Œæ–‡ä»¶
        required_structure = {
            "base": ["mcp.json"],
            "win": ["settings/mcp.json", "settings/performance.json"],
            "mac": ["settings/mcp.json", "settings/performance.json", "docs/development_guide.md"],
            "linux": ["settings/mcp.json", "settings/performance.json"]
        }
        
        integrity_issues = []
        
        for platform, required_files in required_structure.items():
            platform_path = version_3_path / platform
            
            if not platform_path.exists():
                integrity_issues.append(f"ç¼ºå¤±å¹³å°ç›®å½•: {platform}/")
                continue
            
            for required_file in required_files:
                file_path = platform_path / required_file
                if not file_path.exists():
                    integrity_issues.append(f"ç¼ºå¤±æ–‡ä»¶: {platform}/{required_file}")
        
        # æ£€æŸ¥æ–‡æ¡£æ–‡ä»¶
        docs = ["README.md", "MIGRATION_GUIDE.md"]
        for doc in docs:
            doc_path = version_3_path / doc
            if not doc_path.exists():
                integrity_issues.append(f"ç¼ºå¤±æ–‡æ¡£: {doc}")
        
        # æ£€æŸ¥Hookæ–‡ä»¶
        hook_files = [
            "error-solution-finder.kiro.hook",
            "global-debug-360.kiro.hook",
            "intelligent-monitoring-hub.kiro.hook",
            "knowledge-accumulator.kiro.hook",
            "smart-coding-assistant.kiro.hook"
        ]
        
        for platform in ["win", "mac", "linux"]:
            hooks_path = version_3_path / platform / "hooks"
            if hooks_path.exists():
                for hook_file in hook_files:
                    hook_path = hooks_path / hook_file
                    if not hook_path.exists():
                        integrity_issues.append(f"ç¼ºå¤±Hook: {platform}/hooks/{hook_file}")
        
        if integrity_issues:
            print("âš ï¸ å‘ç°å®Œæ•´æ€§é—®é¢˜:")
            for issue in integrity_issues:
                print(f"   - {issue}")
            return False
        else:
            print("âœ… 3.0ç‰ˆæœ¬ç»“æ„å®Œæ•´")
            return True
    
    def generate_cleanup_report(self):
        """ç”Ÿæˆæ¸…ç†æŠ¥å‘Š"""
        print("ğŸ“Š ç”Ÿæˆæ¸…ç†æŠ¥å‘Š...")
        
        report = {
            "metadata": {
                "cleanup_date": datetime.now().isoformat(),
                "cleaner": "ğŸ” Code Review Specialist",
                "mode": "æ‰¹é‡è‡ªåŠ¨æ¸…ç†"
            },
            "cleanup_summary": {
                "total_files_scanned": self.stats["total_files_scanned"],
                "duplicate_groups_found": len(self.duplicate_groups),
                "duplicate_files_found": self.stats["duplicate_files_found"],
                "files_deleted": self.stats["files_deleted"],
                "space_saved_bytes": self.stats["space_saved"],
                "space_saved_mb": round(self.stats["space_saved"] / 1024 / 1024, 2)
            },
            "cleanup_log": self.cleanup_log,
            "version_3_integrity": self.verify_version_3_integrity(),
            "recommendations": [
                "å®šæœŸè¿è¡Œé‡å¤æ–‡ä»¶æ£€æŸ¥",
                "å»ºç«‹æ–‡ä»¶å‘½åè§„èŒƒé¿å…é‡å¤",
                "ä½¿ç”¨ç‰ˆæœ¬æ§åˆ¶ç³»ç»Ÿç®¡ç†æ–‡ä»¶å†å²",
                "ç›‘æ§å¤‡ä»½ç›®å½•çš„å¤§å°"
            ]
        }
        
        # ä¿å­˜æŠ¥å‘Š
        report_path = Path(".kiro/reports/batch_duplicate_cleanup_report.json")
        report_path.parent.mkdir(exist_ok=True)
        
        with open(report_path, "w", encoding="utf-8") as f:
            json.dump(report, f, indent=2, ensure_ascii=False)
        
        print(f"âœ… æ¸…ç†æŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_path}")
        return report
    
    def execute_batch_cleanup(self):
        """æ‰§è¡Œæ‰¹é‡æ¸…ç†"""
        print("ğŸ§¹ å¼€å§‹æ‰§è¡Œæ‰¹é‡é‡å¤æ–‡ä»¶æ¸…ç†...")
        print("=" * 60)
        
        try:
            # 1. æ‰«æé‡å¤æ–‡ä»¶
            self.scan_for_duplicates()
            
            # 2. æ‰¹é‡æ¸…ç†é‡å¤æ–‡ä»¶
            if self.duplicate_groups:
                self.batch_clean_duplicates()
            else:
                print("âœ… æœªå‘ç°é‡å¤æ–‡ä»¶")
            
            # 3. éªŒè¯3.0ç‰ˆæœ¬å®Œæ•´æ€§
            integrity_ok = self.verify_version_3_integrity()
            
            # 4. ç”ŸæˆæŠ¥å‘Š
            report = self.generate_cleanup_report()
            
            print("=" * 60)
            print("ğŸ‰ æ‰¹é‡æ¸…ç†å®Œæˆ!")
            print(f"ğŸ“Š æ‰«ææ–‡ä»¶: {self.stats['total_files_scanned']} ä¸ª")
            print(f"ğŸ” é‡å¤æ–‡ä»¶ç»„: {len(self.duplicate_groups)} ç»„")
            print(f"ğŸ—‘ï¸ åˆ é™¤æ–‡ä»¶: {self.stats['files_deleted']} ä¸ª")
            print(f"ğŸ’¾ èŠ‚çœç©ºé—´: {self.stats['space_saved']:,} å­—èŠ‚")
            print(f"âœ… 3.0ç‰ˆæœ¬å®Œæ•´æ€§: {'é€šè¿‡' if integrity_ok else 'æœ‰é—®é¢˜'}")
            
            return True
            
        except Exception as e:
            print(f"âŒ æ¸…ç†è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {str(e)}")
            return False

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ” æ‰¹é‡é‡å¤æ–‡ä»¶æ¸…ç†å™¨")
    print("ä½œä¸ºCode Review Specialistï¼Œæˆ‘å°†è‡ªåŠ¨æ¸…ç†æ‰€æœ‰é‡å¤æ–‡ä»¶")
    print()
    
    cleaner = BatchDuplicateCleaner()
    success = cleaner.execute_batch_cleanup()
    
    if success:
        print("\nğŸ¯ æ‰¹é‡æ¸…ç†æˆåŠŸå®Œæˆ!")
        print("ğŸ“š ä»£ç åº“ç°åœ¨æ›´åŠ æ•´æ´å’Œé«˜æ•ˆ")
    else:
        print("\nâš ï¸ æ¸…ç†è¿‡ç¨‹ä¸­é‡åˆ°é—®é¢˜")

if __name__ == "__main__":
    main()