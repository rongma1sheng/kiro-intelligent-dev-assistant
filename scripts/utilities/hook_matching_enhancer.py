#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
HookåŒ¹é…å¢å¼ºè„šæœ¬
"""

import re
import sys
from pathlib import Path

# æ·»åŠ srcç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent / "src"))

from kiro_memory import KiroMemorySystem

class HookMatchingEnhancer:
    """HookåŒ¹é…å¢å¼ºå™¨"""
    
    def __init__(self):
        self.memory_system = KiroMemorySystem('.kiro/memory', enable_learning=True)
        
        # ä¸Šä¸‹æ–‡å…³é”®è¯æå–è§„åˆ™
        self.context_patterns = {
            'error_related': [
                r'é”™è¯¯', r'error', r'bug', r'é—®é¢˜', r'å¼‚å¸¸', r'å¤±è´¥', r'fail',
                r'ä¸èƒ½', r'æ— æ³•', r'æŠ¥é”™', r'exception'
            ],
            'optimization_related': [
                r'ä¼˜åŒ–', r'improve', r'enhance', r'better', r'faster',
                r'æå‡', r'æ”¹è¿›', r'å¢å¼º', r'æ€§èƒ½'
            ],
            'development_related': [
                r'å¼€å‘', r'develop', r'code', r'ç¼–ç¨‹', r'programming',
                r'å®ç°', r'implement', r'åŠŸèƒ½', r'feature'
            ],
            'testing_related': [
                r'æµ‹è¯•', r'test', r'éªŒè¯', r'check', r'validate',
                r'æ£€æŸ¥', r'ç¡®è®¤', r'verify'
            ]
        }
        
        # ä¸­æ–‡åˆ°è‹±æ–‡ç¿»è¯‘æ˜ å°„
        self.chinese_to_english = {
            'é”™è¯¯': 'error', 'é—®é¢˜': 'issue', 'ä¿®å¤': 'fix', 'è§£å†³': 'solve',
            'ä¼˜åŒ–': 'optimization', 'æ”¹è¿›': 'improve', 'æå‡': 'enhance',
            'å¼€å‘': 'development', 'ç¼–ç¨‹': 'programming', 'ä»£ç ': 'code',
            'æµ‹è¯•': 'test', 'éªŒè¯': 'verify', 'æ£€æŸ¥': 'check',
            'ç³»ç»Ÿ': 'system', 'å¹³å°': 'platform', 'æ¡†æ¶': 'framework',
            'å›¢é˜Ÿ': 'team', 'åä½œ': 'collaboration', 'ç®¡ç†': 'management',
            'æŠ€èƒ½': 'skill', 'èƒ½åŠ›': 'ability', 'å­¦ä¹ ': 'learning',
            'é›†æˆ': 'integration', 'æ•´åˆ': 'integrate',
            'é…ç½®': 'configuration', 'è®¾ç½®': 'setting',
            'æ€§èƒ½': 'performance', 'æ•ˆç‡': 'efficiency',
            'è´¨é‡': 'quality', 'æ ‡å‡†': 'standard',
            'æµç¨‹': 'process', 'æ–¹æ³•': 'method',
            'å·¥å…·': 'tool', 'è„šæœ¬': 'script',
            'æ•°æ®': 'data', 'ä¿¡æ¯': 'information',
            'ç½‘ç»œ': 'network', 'æœåŠ¡': 'service',
            'å®‰å…¨': 'security', 'æƒé™': 'permission',
            'ç›‘æ§': 'monitoring', 'æ—¥å¿—': 'log',
            'éƒ¨ç½²': 'deployment', 'å‘å¸ƒ': 'release',
            'ç‰ˆæœ¬': 'version', 'æ›´æ–°': 'update',
            'æ–‡æ¡£': 'documentation', 'è¯´æ˜': 'instruction',
            'æ¥å£': 'interface', 'API': 'api',
            'æ•°æ®åº“': 'database', 'å­˜å‚¨': 'storage',
            'ç®—æ³•': 'algorithm', 'æ¨¡å‹': 'model',
            'æ¶æ„': 'architecture', 'è®¾è®¡': 'design',
            'æ¨¡å—': 'module', 'ç»„ä»¶': 'component',
            'åŠŸèƒ½': 'feature', 'ç‰¹æ€§': 'feature',
            'å®ç°': 'implementation', 'æ‰§è¡Œ': 'execution',
            'åˆ†æ': 'analysis', 'è¯„ä¼°': 'evaluation',
            'æŠ¥å‘Š': 'report', 'ç»Ÿè®¡': 'statistics',
            'æŒ‡æ ‡': 'metrics', 'åŸºå‡†': 'benchmark'
        }
        
        # åŒä¹‰è¯å’Œç›¸å…³è¯æ˜ å°„
        self.semantic_mappings = {
            'error': ['é”™è¯¯', 'bug', 'é—®é¢˜', 'å¼‚å¸¸', 'æ•…éšœ', 'issue'],
            'optimization': ['ä¼˜åŒ–', 'æ”¹è¿›', 'æå‡', 'å¢å¼º', 'improve', 'enhance'],
            'development': ['å¼€å‘', 'ç¼–ç¨‹', 'å®ç°', 'æ„å»º', 'coding', 'programming'],
            'testing': ['æµ‹è¯•', 'éªŒè¯', 'æ£€æŸ¥', 'æ ¡éªŒ', 'validation', 'verification'],
            'system': ['ç³»ç»Ÿ', 'å¹³å°', 'æ¡†æ¶', 'platform', 'framework'],
            'integration': ['é›†æˆ', 'æ•´åˆ', 'èåˆ', 'ç»“åˆ', 'combine'],
            'team': ['å›¢é˜Ÿ', 'å°ç»„', 'åä½œ', 'åˆä½œ', 'collaboration'],
            'skill': ['æŠ€èƒ½', 'èƒ½åŠ›', 'æŠ€æœ¯', 'ability', 'capability'],
            'performance': ['æ€§èƒ½', 'æ•ˆç‡', 'é€Ÿåº¦', 'efficiency', 'speed'],
            'quality': ['è´¨é‡', 'æ ‡å‡†', 'è§„èŒƒ', 'standard', 'specification']
        }
    
    def extract_context_keywords(self, prompt: str):
        """ä»æç¤ºä¸­æå–ä¸Šä¸‹æ–‡å…³é”®è¯"""
        keywords = []
        prompt_lower = prompt.lower()
        
        # 1. åŸºäºæ¨¡å¼åŒ¹é…æå–å…³é”®è¯
        for category, patterns in self.context_patterns.items():
            for pattern in patterns:
                if re.search(pattern, prompt_lower):
                    keywords.append(category.replace('_related', ''))
                    break
        
        # 2. æå–å…·ä½“çš„æŠ€æœ¯è¯æ±‡
        tech_words = re.findall(r'[a-zA-Z]{3,}|[\u4e00-\u9fff]{2,}', prompt)
        keywords.extend([word.lower() for word in tech_words if len(word) > 2])
        
        return list(set(keywords))
    
    def enhanced_hook_search(self, prompt: str, max_results: int = 5):
        """å¢å¼ºçš„Hookæœç´¢"""
        print(f"ğŸ¯ Hookå¢å¼ºæœç´¢: '{prompt[:50]}...'")
        
        # 1. æå–ä¸Šä¸‹æ–‡å…³é”®è¯
        keywords = self.extract_context_keywords(prompt)
        print(f"   æå–å…³é”®è¯: {keywords}")
        
        # 2. ç¿»è¯‘ä¸­æ–‡å…³é”®è¯
        translated_keywords = self.translate_chinese_keywords(keywords)
        all_keywords = keywords + translated_keywords
        print(f"   ç¿»è¯‘åå…³é”®è¯: {all_keywords}")
        
        # 3. å¤šè½®æœç´¢ç­–ç•¥
        all_results = []
        seen_ids = set()
        
        # ç¬¬ä¸€è½®ï¼šç›´æ¥æœç´¢åŸå§‹æç¤º
        direct_results = self.memory_system.search(prompt, max_results=max_results)
        for pattern in direct_results:
            if pattern.id not in seen_ids:
                all_results.append(pattern)
                seen_ids.add(pattern.id)
        
        # ç¬¬äºŒè½®ï¼šç¿»è¯‘åçš„æç¤ºæœç´¢
        translated_prompt = self.translate_prompt(prompt)
        if translated_prompt != prompt:
            translated_results = self.memory_system.search(translated_prompt, max_results=3)
            for pattern in translated_results:
                if pattern.id not in seen_ids and len(all_results) < max_results:
                    all_results.append(pattern)
                    seen_ids.add(pattern.id)
        
        # ç¬¬ä¸‰è½®ï¼šåŸºäºå…³é”®è¯æœç´¢
        if len(all_results) < max_results:
            for keyword in all_keywords[:5]:  # é™åˆ¶å…³é”®è¯æ•°é‡
                keyword_results = self.memory_system.search(keyword, max_results=2)
                for pattern in keyword_results:
                    if pattern.id not in seen_ids and len(all_results) < max_results:
                        all_results.append(pattern)
                        seen_ids.add(pattern.id)
        
        # ç¬¬å››è½®ï¼šè¯­ä¹‰æ‰©å±•æœç´¢
        if len(all_results) < max_results:
            semantic_queries = self.get_semantic_queries(all_keywords)
            for query in semantic_queries[:3]:  # é™åˆ¶è¯­ä¹‰æŸ¥è¯¢æ•°é‡
                semantic_results = self.memory_system.search(query, max_results=2)
                for pattern in semantic_results:
                    if pattern.id not in seen_ids and len(all_results) < max_results:
                        all_results.append(pattern)
                        seen_ids.add(pattern.id)
        
        print(f"   æ‰¾åˆ° {len(all_results)} ä¸ªç›¸å…³æ¨¡å¼")
        return all_results
    
    def translate_chinese_keywords(self, keywords):
        """ç¿»è¯‘ä¸­æ–‡å…³é”®è¯ä¸ºè‹±æ–‡"""
        translated = []
        for keyword in keywords:
            if keyword in self.chinese_to_english:
                translated.append(self.chinese_to_english[keyword])
        return translated
    
    def translate_prompt(self, prompt: str):
        """ç¿»è¯‘æç¤ºä¸­çš„ä¸­æ–‡è¯æ±‡"""
        translated_prompt = prompt
        for chinese, english in self.chinese_to_english.items():
            if chinese in translated_prompt:
                translated_prompt = translated_prompt.replace(chinese, english)
        return translated_prompt
    
    def get_semantic_queries(self, keywords):
        """è·å–è¯­ä¹‰ç›¸å…³çš„æŸ¥è¯¢è¯"""
        semantic_queries = []
        
        for keyword in keywords:
            if keyword in self.semantic_mappings:
                semantic_queries.extend(self.semantic_mappings[keyword][:2])
        
        return list(set(semantic_queries))
    
    def generate_enhanced_prompt(self, original_prompt: str, relevant_patterns):
        """ç”Ÿæˆå¢å¼ºçš„æç¤º"""
        if not relevant_patterns:
            return original_prompt
        
        enhancement = "\n\nğŸ’¡ ç›¸å…³çŸ¥è¯†æ¨¡å¼:\n"
        for i, pattern in enumerate(relevant_patterns[:3], 1):
            description = pattern.content.get('description', 'æ— æè¿°')[:100]
            enhancement += f"{i}. [{pattern.type.value}] {description}...\n"
        
        return original_prompt + enhancement

if __name__ == "__main__":
    enhancer = HookMatchingEnhancer()
    
    # æµ‹è¯•HookåŒ¹é…å¢å¼º
    test_prompts = [
        "è¯·å¸®æˆ‘ä¿®å¤Pythonä¸­çš„é”™è¯¯",
        "å¦‚ä½•ä¼˜åŒ–ç³»ç»Ÿæ€§èƒ½",
        "å›¢é˜ŸæŠ€èƒ½ç®¡ç†æœ€ä½³å®è·µ",
        "GitHubé›†æˆé—®é¢˜è§£å†³"
    ]
    
    for prompt in test_prompts:
        results = enhancer.enhanced_hook_search(prompt)
        enhanced_prompt = enhancer.generate_enhanced_prompt(prompt, results)
        print(f"åŸå§‹æç¤º: {prompt}")
        print(f"å¢å¼ºæ•ˆæœ: æ‰¾åˆ°{len(results)}ä¸ªç›¸å…³æ¨¡å¼")
        print("-" * 50)
